# Model evaluation {#sec-modelvalidation}

So far we have discussed how to build a model and use it to predict the distribution of a species. However, it is equally important to understand how to interpret and evaluate the results of the model. A key step in this process is assessing the accuracy of the model's predictions - a process known as *model validation*.

In @sec-4evalstats, we introduced the AUC-ROC, a statistic that provides a first measure of model performance. However, it's important to note that evaluating a model on the same data it was trained on can lead to overly optimistic assessments, as models tend to perform worse on new, unseen data. To truly assess a model's predictive power, it must be tested on a separate dataset that wasn't used during training (paragraph [-@sec-traintestpoints] and [-@sec-crossvalidation]).

Another important consideration is the context in which predictions are made. For example, making predictions under climate conditions that deviate strongly from those represented in the training data (novel climates) introduces uncertainty. In such cases, the model's assumptions and extrapolations may no longer hold, which can compromise the reliability of its predictions (paragraph [-@sec-novelclimates]).

::: callout-note
## Work in progress

This chapter is work in progress, and will be updated with new examples as time allows, so stay tuned!
:::

## Training and testing Points {#sec-traintestpoints}

The easiest way to do this is to split the presence data into two parts, and use one part to train the model, and another part to test the model's performance. But before we do that, we first create a new mapset and a folder to store the model results. Of course, you are free to organize your model results differently. However, it's important to note that while GRASS GIS prevents accidental overwriting of data, MaxEnt does not. If you use the same output folder and the files in that folder have the same names, MaxEnt will overwrite them without warning.

::: {#exm-v4swwqZVAv .hiddendiv}
:::

::: {.panel-tabset group="interface"}
## {{< fa solid terminal >}}

``` bash
# Folders to store data
mkdir model_02

# Create a new mapset and switch to it
g.mapset -c mapset=model_02

# Define the region and set the MASK
g.region raster=bio_1@climate_current
```

## {{< fa brands python >}}

``` python
# Set working directory and create a new folder in the working directory
os.chdir("replace-for-path-to-working-directory")
os.makedirs("model_02", exist_ok=True)

# Create a new mapset and switch to it
gs.run_command("g.mapset", flags="c", mapset="model_02")

# Set the region and create a MASK
gs.run_command("g.region", raster="bio_1@climate_current")  
```

## {{< fa regular window-restore >}}

Create the folder [model_02]{.style-db} in your working directory using your favorite file browser. Next, create a new mapset and switch to this mapset using the Data panel. Alternatively, open the [g.mapsets]{.style-function} dialog and run it with the following parameter settings:

| Parameter                             | Value    |
|---------------------------------------|----------|
| Name of mapset (mapset)               | model_02 |
| Create mapset if it doesn't exist (c) | ✅       |

: {tbl-colwidths="\[50,50\]"}

<br>Next, use the [g.region]{.style-function} module to set the computational region style parameter, based on the [bio_1]{.style-data} raster layer in the [climate_current]{.style-db} mapset.

| Parameter | Value                  |
|-----------|------------------------|
| raster    | bio_1\@climate_current |

: {tbl-colwidths="\[50,50\]"}
:::

### Model training

We train the model as we did in @sec-modeltraining, but this time we ask Maxent to use 80% of the presence points to train the model, and set aside 20% of the presence points to test the model's performance. We can do this using the [randomtestpoints]{.style-parameter} parameter of the [r.maxent.train]{.style-function} module.

::: {#exm-kjKw0Mrq26 .hiddendiv}
:::

::: {.panel-tabset group="interface"}
## {{< fa solid terminal >}}

``` bash
r.maxent.train \
samplesfile=dataset01/species.swd \
environmentallayersfile=dataset01/background_points.swd \
outputdirectory=model_02 \
randomtestpoints=20 \
threads=4 memory=1000 \
-ybg
```

## {{< fa brands python >}}

``` python
gs.run_command(
    "r.maxent.train",
    samplesfile="dataset01/species.swd",  
    environmentallayersfile="dataset01/background_points.swd",
    outputdirectory="model_02", 
    randomtestpoints=20,
    threads=4,
    memory=1000,
    flags="ybg", 
)  
```

## {{< fa regular window-restore >}}

Open the [r.maxent.train]{.style-function} dialog and run the module with the following parameter settings:

| Parameter | Value |
|----|----|
| samplesfile | dataset01/species.swd |
| environmentallayersfile | dataset01/background.swd |
| outputdirectory | model_02 |
| threads | 4 |
| memory | 1000 |
| Create a vector point layer from the sample predictions (y) | ✅ |
| Create a vector point layer with predictions at backgr. points (b) | ✅ |
| Create response curves (g) | ✅ |

: {tbl-colwidths="\[63,37\]"}
:::

:::: {.panel-tabset .exercise}
## {{< fa regular circle-question >}}

::: {#exr-3_1}
You might have noticed that when training the model, we omitted a few parameters compared to @exm-g8jUY2JKvW. Which parameters did we leave out, and what does this mean for our outcomes?
:::

## {{< fa regular comment >}}

**projectionlayers**: We omitted the [projectionlayers]{.style-paramter} parameter. As explained in the [help page](https://grass.osgeo.org/grass-stable/manuals/addons/r.maxent.train.html), this parameter allows you to specify the location of a set of rasters representing the same environmental variables used to build the Maxent model. When provided, Maxent generates a prediction raster layer based on these rasters. Since we didn’t use this parameter, no prediction raster layer was created in this run. Skipping this step saves time, as generating these layers can be time-consuming.

**samplepredictions**: We included the [-y]{.style-parameter}, instructing Maxent to create vector point layers with predictions at presence points. However, we did not set the [samplepredictions]{.style-parameter} parameters so Maxent assigned a default name to the output. This default name combines the species name with the suffix \*\_obs_samplePredictions\*. In this case, the point layer is named [Erebia_alberganus_obs_samplePredictions]{.style-data}

**backgroundpredictions**: Similarly, we used the [-b]{.style-parameter} parameter to create vector point layers with predictions at background points. Since we did not specify the [backgroundpredictions]{.style-parameter} parameter, Maxent again used a default naming convention: the species name followed by the suffix \*\_obs_backgroundPredictions\*. Thus, the file is named [Erebia_alberganus_obs_backgroundPredictions]{.style-data}.

To reiterate, none of these are model parameters, i.e., leaving them out does not change the model itself.
::::

### Validation

The [r.maxent.train]{.style-function} module output on the console (@fig-modeltrainconsulemessages02) shows that compared to the first model, we have fewer training samples. Twenty percent less to be exactly, which are of course the training samples that we set aside as test points.

This time, two AUC statistics are reported: the training AUC and the test AUC. The training AUC reflects how well the model fits the training data, while the test AUC reflects how well the model generalizes to new data. The training AUC is 0.8844, while the test AUC is slightly higher at 0.886, with a standard deviation of 0.004.

::: {.panel-tabset .exercise}
## Output messages

![Messages of the r.maxent.train module in the console, showing the number of training points, and the training and test AUC.](images/modeltrainconsulemessages02.png){#fig-modeltrainconsulemessages02 fig-align="left" width=""}

## ROC

![ROC curve and the area under the curve statistics of model_02, based on the training data (red curve) and the test data (blue curve).](images/Erebia_alberganus_obs_roc_model02.png){#fig-auc_model_02 fig-align="left"}
:::

To view the corresponding ROC curve (@fig-auc_model_02), open the file [Erebia_alberganus_obs.html]{.style-file} located in the results folder. The red line represents the ROC curve based on the training data, while the blue line represents the ROC curve based on the test data. The curves are very similar. This is to be expected. The density of sample points is high, which means that training and test points are usually close together. That is, the environmental conditions in the test points are very similar to those in the training point locations.

:::: {.panel-tabset .exercise}
## {{< fa regular circle-question >}}

::: {#exr-SErwbZwytm}
It is somewhat surprising that the AUC based on the test data is higher than that based on the training data. What could be a possible explanation?
:::

## {{< fa regular comment >}}

A possible explanation is that the test data represents a much smaller set of presence points, which results in a much smaller prevalence — the ratio of presence points to background points — in the test set compared to the training dataset. Background points are typically easier to classify as absence, especially if they are situated in environmentally distinct areas that are far from the species' known occurrences. This can lead to a higher true negative rate (specificity) because the model more confidently identifies absence areas. Since AUC (area under the ROC curve) reflects both sensitivity (true positive rate) and specificity (true negative rate), an improvement in specificity is likely to result in a higher AUC.
::::

As we did not change any of the model parameters, the outcomes should otherwise be the same as in [paragraph -@sec-4trainthemodel], with some minor differences due to small differences in input data.

### Using the model

The process of setting aside occurrence data for testing is essential to evaluate the predictive power of a model under new conditions or in untested areas. This validation step ensures that the model can generalize beyond the data it was trained on. Typically, you will develop and compare multiple models, each trained using different parameter settings or algorithms. Through this process, the best-performing model is identified based on validation results.

Once the best model is selected, the next step is to rebuild it using all available occurrence data. By incorporating the full dataset, the final model benefits from the maximum amount of information, improving its robustness for future predictions and analyses. This model is the one typically used for practical applications, such as predicting species distributions across a broader landscape or under different environmental scenarios. In other words, model prediction as we have done in @sec-modelprediction.

## Cross-validation {#sec-crossvalidation}

### Description

One limitation of the validation method described in the previous paragraph is that the results can vary depending on which 20% of the points are selected for testing. A more robust and commonly used technique to estimate the accuracy of a predictive model is [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)). There are different types of cross-validation, but we'll focus on k-fold cross-validation here. In k-fold cross-validation, the original sample is randomly partitioned into **k** subsamples with an approximately equal number of records, as further explained in @fig-vkgSx6qmBy.

![Schematic explanation of a 4-fold cross-validation. The presence data is randomly partitioned into 4 groups with an approximately equal number of records. Of these groups, a single group is set apart as the validation data for testing the model, and the remaining subsamples are combined to be used as training data to create the model. Next, the model is used to predict the presence of the locations of the validation subset. The results of the prediction are then compared with the known presence and background designations in the validation subset. The cross-validation process is repeated as many times as there are subsamples, whereby each of the subsamples is used exactly once as the validation data.](images/4-fold_cross-validation.svg){#fig-vkgSx6qmBy fig-align="left" width="600"}

### Model training

For this tutorial, we carry out a 4-fold cross-validation. To perform the cross-validation, we run [r.maxent.train]{.style-function} with the [replicatetype]{.style-parameter} and the [replicates]{.style-parameter} parameter to tell Maxent to run a 4-fold cross-validation. This means that Maxent will process trains four models in the background, calculates validation statistics for each model, and computes average statistics across the four sub-models.

::: {.callout-note appearance="simple"}
We will henceforth refer to these as the "four models". However, these are not distinct, separate models but represent the four iterations of a 4-fold cross-validation process. Each iteration corresponds to training the model on a unique combination of three subsets of the data and testing it on the remaining subset. Thus, while the results may vary slightly across the iterations due to differences in the training and testing subsets, they are essentially iterations of the same underlying model (same parameter settings), evaluated under different data splits.
:::

If both the option to create a sample prediction ([-y]{.style-parameter} flag) and k-fold cross-validation are selected, the sample prediction point layer's attribute table will include for each point the average predicted probabilities and the range of predicted probabilities across the four models. Note that the the [v.db.pyupdate](https://grass.osgeo.org/grass-stable/manuals/v.db.update.html) addon needs to be installed to do this.

We furthermore specify the location of the folder with the input environmental raster layers using the [projectionlayers]{.style-parameter} parameter. This instructs Maxent to generate a raster prediction layer. When the k-fold cross-validation option is enabled, this raster shows the average predicted probability of species presence, calculated across all four model iterations. Additionally, three extra layers will be produced, showing the minimum, maximum, and standard deviation of the predictions across the four models.

::: {#exm-fb4bZ2MIs0 .hiddendiv}
:::

::: {.panel-tabset group="interface"}
## {{< fa solid terminal >}}

``` bash
# Install addon
g.extension extension=v.db.pyupdate

# Setup
mkdir model_03
g.mapset -c mapset=model_03
g.region raster=bio_1@climate_current

# Train
r.maxent.train \
samplesfile=dataset01/species.swd \
environmentallayersfile=dataset01/background_points.swd \
projectionlayers=dataset01/envdat \
outputdirectory=model_03 \
replicatetype=crossvalidate \
replicates=4 threads=4 memory=1000 -yg
```

## {{< fa brands python >}}

``` python
# Install addon
gs.run_command("g.extension", extension="v.db.pyupdate")

# Setup
os.chdir("replace-for-path-to-working-directory")
os.makedirs("model_03", exist_ok=True)
gs.run_command("g.mapset", flags="c", mapset="model_03")
gs.run_command("g.region", raster="bio_1@climate_current")  

# Train
gs.run_command(
    "r.maxent.train",
    samplesfile="dataset01/species.swd",  
    environmentallayersfile="dataset01/background_points.swd",
    projectionlayers="dataset01/envdat",
    outputdirectory="model_03", 
    replicatetype="crossvalidate",
    replicates=4,
    threads=4,
    memory=1000,
    flags="yg", 
)  
```

## {{< fa regular window-restore >}}

To install the addon, go to the menu \[settings → addon extensions → install extensions from addons\], look for *v.db.pyupdate* and install it.

Create the folder [model_03]{.style-db} in your working directory using your favorite file browser. Next, create a new mapset and switch to this mapset using the Data panel. Alternatively, open the [g.mapsets]{.style-function} dialog and run it with the following parameter settings:

| Parameter                             | Value    |
|---------------------------------------|----------|
| Name of mapset (mapset)               | model_03 |
| Create mapset if it doesn't exist (c) | ✅       |

: {tbl-colwidths="\[60,40\]"}

<br>Next, use the [g.region]{.style-function} module to set the computational region style parameter, based on the [bio_1]{.style-data} raster layer in the [climate_current]{.style-db} mapset.

| Parameter | Value                  |
|-----------|------------------------|
| raster    | bio_1\@climate_current |

: {tbl-colwidths="\[60,40\]"}

<br>Open the [r.maxent.train]{.style-function} dialog and run the module with the following parameter settings:

| Parameter | Value |
|----|----|
| samplesfile | dataset01/species.swd |
| environmentallayersfile | dataset01/background.swd |
| projectionlayers | dataset01/envdat |
| outputdirectory | model_03 |
| replicatetype | crossvalidate |
| replicates | 4 |
| threads | 4 |
| memory | 1000 |
| Create a vector point layer from the sample predictions (y) | ✅ |
| Create response curves (g) | ✅ |

: {tbl-colwidths="\[60,40\]"}
:::

### Evaluation statistics

To examine the model statistics, open the HTML file [Erebia_alberganus_obs.html]{.style-file} located in the [model_03]{.style-db} output folder. The page provides a summary of the results from the 4-fold cross-validation and includes links to the results of the individual models in the top right corner. The average test AUC across the replicate runs is 0.889, with a standard deviation of 0.003.

::: {.panel-tabset .exercise}
## AUC-ROC

Figure [-@fig-roc-auc_model3] shows the receiver operating characteristic (ROC) curve, averaged over the replicate runs (red line). The standard deviation is represented by the blue band around the red line.

![ROC curve and the area under the curve statistics for model_03.](share/model_03/plots/Erebia_alberganus_obs_roc.png){#fig-roc-auc_model3 fig-align="left" group="corcom"}

## Omission graph

Figure [-@fig-omission_model3] shows the test omission rate (green line) and predicted area (red line) as a function of the cumulative threshold, averaged over the replicate runs. The standard deviation of the omission rate and predicted area are illustrated by respectively the yellow and blue bands.

![The omission/commission graph for model_03.](share/model_03/plots/Erebia_alberganus_obs_omission.png){#fig-omission_model3 fig-align="left" group="corcom"}
:::

The validation diagnostics from each group help indicate how the model will perform when estimating presence in unknown locations. If the model performs well for some groups, but poorly for others, we should be careful when interpreting the model outcomes. In this case, differences are fairly small (resulting in a small standard deviation).

### Probability maps

The sample prediction layer and the various raster prediction layers generated by Maxent offer further understanding of the spatial patterns of agreement and disagreement across the cross-validation iterations.

The default color scheme of the [Erebia_alberganus_obs_samplePredictions]{.style-data} represents the average predicted probabilities, based on the four models (Figure @fig-samlepred_model_03a). To visualizing the variability in predictions across the four models, we use the values in the [Cloglog_range]{.style-data} column of the attribute table. These values represent the range (difference between the maximum and minimum predicted probabilities) across the four models (@fig-samlepred_model_03b).

To create the new color table, we use the [r.colors](https://grass.osgeo.org/grass-stable/manuals/r.colors.html) function.

::: {#exm-varKii8QsL .hiddendiv}
:::

::: {.panel-tabset group="interface"}
## {{< fa solid terminal >}}

``` bash
v.colors map=Erebia_alberganus_obs_samplePredictions \
use=attr \
column=Cloglog_range \
color=bcyr
```

## {{< fa brands python >}}

``` python
gs.run_command(
    "v.colors",
    map="Erebia_alberganus_obs_samplePredictions",
    use="attr",
    column="Cloglog_range",
    color="bcyr",
)
```

## {{< fa regular window-restore >}}

Open the [v.colors]() dialog and run it with:

| Parameter | Value                                   |
|-----------|-----------------------------------------|
| map       | Erebia_alberganus_obs_samplePredictions |
| use       | attr                                    |
| column    | Cloglog_range                           |
| color     | bcyr                                    |

: {tbl-colwidths="\[50,50\]"}
:::

Now, go to the [data]{.style-menu} panel, and and open the various layers in the [Map display]{.style-menu} panel. Pay particular attention to the areas of disagreement. These highlight regions where the model predictions are less consistent, signaling the need for cautious interpretation of the results in these areas.

::: {.panel-tabset .exercise}
## Average sample predictions

![The [Erebia_alberganus_obs_samplePredictions]{.style-data} vector layer with the GBIF occurrences. The colors represent the predicted probability that the species occurs at these locations, averaged over the four models.](images/Erebia_alberganus_obs_samplePredictions1.png){#fig-samlepred_model_03a group="ytHc07Sg8P"}

## Range sample predictions

![The [Erebia_alberganus_obs_samplePredictions]{.style-data} vector layer with GBIF occurrence data. The colors represent the range of predicted probabilities (the difference between the maximum and minimum values) across the four models.](images/Erebia_alberganus_obs_samplePredictions2.png){#fig-samlepred_model_03b group="ytHc07Sg8P"}

## Mean vs range

![Density plot, with the redicted mean probabilities plotted against the range of predicted probabilities across the four iterations of model 3. The grey lines show the median mean and range values. See @exm-972VUKNJIx below for the code used to create this plot.](images/scatterplot_mean_range.png){#fig-meanvsrange fig-align="left"}
:::

Ideally, the four model iterations should result in high average scores for occurrence locations, indicating consistency and accuracy, and low range values, reflecting strong agreement between different model iterations. To identify locations with high average scores and low range values, or vice versa, we can create a scatterplot using the [v.scatterplot](https://grass.osgeo.org/grass-stable/manuals/addons/v.scatterplot.html) add-on.

::: {#exm-972VUKNJIx .hiddendiv}
:::

::: {.panel-tabset group="interface"}
## {{< fa solid terminal >}}

``` bash
# Install v.scatterplot
g.extension extension=v.scatterplot

# Create a scatter/density plot
v.scatterplot map=Erebia_alberganus_obs_samplePredictions \
x=Cloglog_mean y=Cloglog_range type=density \
quadrants=median
```

## {{< fa brands python >}}

``` python
# Install v.scatterplot
gs.run_command("g.extension", extension="v.scatterplot")

# Create a scatter/density plot
gs.run_command(
    "v.scatterplot",
    map="Erebia_alberganus_obs_samplePredictions",
    x="Cloglog_mean",
    y="Cloglog_range",
    type="density",
    quadrants="median",
)
```

## {{< fa regular window-restore >}}

Install the [v.scatterplot]{.style-function} addon, and run it with the following parameters:

| Parameter | Value                                   |
|-----------|-----------------------------------------|
| map       | Erebia_alberganus_obs_samplePredictions |
| x         | Cloglog_mean                            |
| y         | Cloglog_range                           |
| type      | density                                 |
| quadrants | median                                  |

: {tbl-colwidths="\[40,60\]"}
:::

The resulting scatterplot (@fig-meanvsrange) shows that there are quite a few occurrence locations where the model iterations fairly consistently predict a low probability (false negatives), while there are a few locations where predictions vary more across the different iterations (less robust predictions). You can extract and plot outliers on the map with the [v.extract](https://grass.osgeo.org/grass-stable/manuals/v.extract.html) module (result not shown here). Alternatively, you can select and highlight them using the [SQL Query Builder]{.style-menu} of the [Attribute Table Manager]{.style-menu}.

::: {#exm-NlC6SPkAy7 .hiddendiv}
:::

::: {.panel-tabset group="interface"}
## {{< fa solid terminal >}}

``` bash
# Extract outliers to new map
v.extract input=Erebia_alberganus_obs_samplePredictions \
where='"Cloglog_mean" < 0.3 AND "Cloglog_range" >= 0.09\' \
output=outliers
```

## {{< fa brands python >}}

``` python
# Extract outliers to new map
gs.run_command(
    "v.extract",
    input="Erebia_alberganus_obs_samplePredictions",
    where='"Cloglog_mean" < 0.3 AND "Cloglog_range" >= 0.09',
    output="outliers",
)
```

## {{< fa regular window-restore >}}

Extract outliers to new map using [v.extract](.style-function) with the following parameters:

| Parameter | Value                                               |
|-----------|-----------------------------------------------------|
| input     | Erebia_alberganus_obs_samplePredictions             |
| where     | `'"Cloglog_mean" < 0.3 AND "Cloglog_range" > 0.09'` |
| output    | outliers                                            |

: {tbl-colwidths="\[40,60\]"}

Alternatively, use the [Attribute Table Manager]{.style-menu}:

{{< video "https://ecodiv.earth/share/sdm_in_grassgis/select_outliers_in_attributetable2.mp4" >}}
:::

To get a better idea about the spatial patterns of agreement and disagreement among the models outside the areas where the species was observed, we can examine the prediction raster layers with the average and standard deviation of the values generated by the four models.

::: {.panel-tabset .exercise}
## Average predictions

![The [Erebia_alberganus_obs_envdat_avg]{.style-data} raster layer. The colors represent the predicted probability, averaged over the four models.](images/Erebia_alberganus_obs_envdat_avg.png){#fig-predlay_model_03a group="ytHc07Sg8P"}

## Standard deviation of predictions

![The [Erebia_alberganus_obs_envdata_stddev]{.style-data} raster layer. The colors represent the standard deviation of predicted probabilities across the four models.](images/Erebia_alberganus_obs_envdat_stddev.png){#fig-predlay_model_03b group="ytHc07Sg8P"}
:::

### Response curves {#sec-4responsecurves_model3}

The response curves in the HTML file [Erebia_alberganus_obs.html]{.style-file} show the mean response of the 4 replicate Maxent runs (red) and and the mean +/- one standard deviation (blue, two shades for categorical variables).

::::: {.panel-tabset .exercise}
## Marginal response curves

::: {#fig-responsecurves1 layout-ncol="4"}
![](share/model_03/plots/Erebia_alberganus_obs_bio_1.png){group="4foldresponse"}

![](share/model_03/plots/Erebia_alberganus_obs_bio_2.png){group="4foldresponse"}

![](share/model_03/plots/Erebia_alberganus_obs_bio_4.png){group="4foldresponse"}

![](share/model_03/plots/Erebia_alberganus_obs_bio_8.png){group="4foldresponse"}

![](share/model_03/plots/Erebia_alberganus_obs_bio_9.png){group="4foldresponse"}

![](share/model_03/plots/Erebia_alberganus_obs_bio_13.png){group="4foldresponse"}

![](share/model_03/plots/Erebia_alberganus_obs_bio_14.png){group="4foldresponse"}

![](share/model_03/plots/Erebia_alberganus_obs_bio_15.png){group="4foldresponse"}

Response curves created by varying the specific variable, while keeping all other variables fixed at their average sample value
:::

## single-variable response curves

::: {#fig-responsecurves2 layout-ncol="4"}
![](share/model_03/plots/Erebia_alberganus_obs_bio_1_only.png){group="4foldresponse2"}

![](share/model_03/plots/Erebia_alberganus_obs_bio_2_only.png){group="4foldresponse2"}

![](share/model_03/plots/Erebia_alberganus_obs_bio_4_only.png){group="4foldresponse2"}

![](share/model_03/plots/Erebia_alberganus_obs_bio_8_only.png){group="4foldresponse2"}

![](share/model_03/plots/Erebia_alberganus_obs_bio_9_only.png){group="4foldresponse2"}

![](share/model_03/plots/Erebia_alberganus_obs_bio_13_only.png){group="4foldresponse2"}

![](share/model_03/plots/Erebia_alberganus_obs_bio_14_only.png){group="4foldresponse2"}

![](share/model_03/plots/Erebia_alberganus_obs_bio_15_only.png){group="4foldresponse2"}

Response curves created by running a model based on only the specific variable as explanatory variable.
:::
:::::

### Using the model

We used cross-validation to evaluate the predictive power of the model (as defined by the selected parameter settings) under new conditions or in untested areas. This process produced four model variants, each trained on slightly different subsets of the data due to the cross-validation procedure. These models are described by the lambdas files in the output folder. For each of these model variants, MaxEnt generated a raster layer representing the predicted probability of occurrence. These layers were then summarized by calculating their average, median, minimum, maximum, and standard deviation. The resulting summary layers are the ones currently available in our mapset.

But what if we want to use the selected parameter settings to predict species distributions under different environmental scenarios and compare the result predicted potential distribution with the current potential distribution (as we have done in @sec-modelprediction)? One option is to select one of the four models or compute the average probability values across all four, as done during training. However, the standard approach is to rebuild the model using all available occurrence data, ensuring that the model benefits from the full dataset. This model can then be used to make predictions under different environmental conditions or in new geographic areas.

## Novel climates {#sec-novelclimates}

When using a species distribution model to make predictions under conditions that are outside the range of conditions observed during model training (novel conditions), predictions may be less reliable. This is because the model is based on relationships learned from the training data. And these relationships may be very different in other areas or under future conditions.

### MES statistics

The Multivariate Environmental Similarity (MES), as described by Elith et al. [@elith2010], can be used to evaluate if and where future conditions are outside the range of conditions observed during model training, and thus where model predictions might be less reliable. In GRASS, we can use the [r.mess](https://grass.osgeo.org/grass-stable/manuals/addons/r.mess.html) addon to calculate the MESS.

We use the [r.mess]{.style-function} module to examine how different the climatic conditions in 2081 are to present conditions. For the present climate conditions, we use the same data and bioclimatic variables as we used to create our first model in [paragraph -@sec-4trainthemodel]. We only consider the conditions in the presence and background locations, as this is what we used to train the model. To do so, we combine and convert presence and background points into a new raster layer [referencelayer]{.style-data}, and use this as input for the [ref_rast]{.style-parameter} parameter.

::: {#exm-dCCiaw6IUX .hiddendiv}
:::

::: {.panel-tabset group="interface"}
## {{< fa solid terminal >}}

``` bash
# Install the addon
g.extension extension=r.mess

# Go to mapset 'model_01' and set the region 
g.mapset mapset=model_01 
g.region raster=bio_1@climate_current # <1>

# Combine the occurrence and background points
v.patch input=E_alberganus_samplepred,E_alberganus_bgrdpred \
output=referencepoints

# Convert the point layer to a raster layer # <2>
v.to.rast input=referencepoints output=referencepoints use=value
```

1.  The region should already be defined correctly, but just to be sure.
2.  In the next step, we use the [ref_rast]{.style-parameter} parameter to set the reference raster layer, which we create here, as the reference layer. Alternatively, we could use the [ref_vect]{.style-parameter} parameter with the [referencepoints]{.style-data} vector layer. However, the vector layer contains multiple point within some raster cells. The [r.mess]{.style-function} module will ignore all but one with a warning message. To avoid such message, we convert the point layer to a raster, effectively resolving the issue by eliminating duplicate points.

## {{< fa brands python >}}

``` python
# Install the addon
gs.run_command("g.extension", extension="r.mess")

# Go to mapset 'model_01' and set the region 
gs.run_command("g.mapset", mapset="model_01") 
gs.run_command("g.region", raster="bio_1@climate_current") # <1>

# Combine the occurrence and background points
gs.run_command(
    "v.patch",
    input=["E_alberganus_samplepred", "E_alberganus_bgrdpred"],
    output="referencepoints",
)

# Convert the point layer to a raster layer # <2>
gs.run_command(
    "v.to.rast", input="referencepoints", output="referencepoints", use="value"
)
```

1.  The region should already be defined correctly, but just to be sure.
2.  In the next step, we use the [ref_rast]{.style-parameter} parameter to set the reference raster layer, which we create here, as the reference layer. Alternatively, we could use the [ref_vect]{.style-parameter} parameter with the [referencepoints]{.style-data} vector layer. However, the vector layer contains multiple point within some raster cells. The [r.mess]{.style-function} module will ignore all but one with a warning message. To avoid such message, we convert the point layer to a raster, effectively resolving the issue by eliminating duplicate points.

## {{< fa regular window-restore >}}

Open the [g.extension]{.style-function} dialog and run it with:

| Parameter | Value  |
|-----------|--------|
| extension | r.mess |

: {tbl-colwidths="\[40,60\]"}

Open the [g.mapset]{.style-function} dialog and run it with:

| Parameter | Value    |
|-----------|----------|
| mapset    | model_01 |

: {tbl-colwidths="\[40,60\]"}

Open the [g.region]{.style-function} dialog and run it with:

| Parameter | Value                  |
|-----------|------------------------|
| raster    | bio_1\@climate_current |

: {tbl-colwidths="\[40,60\]"}

To combine the occurrence and background points, open the [v.patch]{.style-function} dialog, and run it with:

| Parameter | Value                                         |
|-----------|-----------------------------------------------|
| input     | E_alberganus_samplepred,E_alberganus_bgrdpred |
| output    | referencepoints                               |

: {tbl-colwidths="\[40,60\]"}

And convert the point layer to a raster with the [v.to.rast]{.style-function} module. Open it and run it with the following parameters:

| Parameter | Value           |
|-----------|-----------------|
| input     | referencepoints |
| output    | referencepoints |

: {tbl-colwidths="\[40,60\]"}

In the next step, we use the [ref_rast]{.style-parameter} parameter to set the reference raster layer, which we create here, as the reference layer. Alternatively, we could use the [ref_vect]{.style-parameter} parameter with the [referencepoints]{.style-data} vector layer. However, the vector layer contains multiple point within some raster cells. The [r.mess]{.style-function} module will ignore all but one with a warning message. To avoid such message, we convert the point layer to a raster, effectively resolving the issue by eliminating duplicate points.
:::

To assess if and where future conditions are outside the range of conditions observed during model training, we use the same future climate data that we used as input to our model prediction in [paragraph -@sec-e8039LmOdu]. These are the projected bioclimatic data for the period 2081-2100, based on the SSP585 climate scenario from the EC-Earth3-Veg general circulation model.

::: {#exm-lHWXJikKCV .hiddendiv}
:::

::: {.panel-tabset group="interface"}
## {{< fa solid terminal >}}

``` bash
# Compute mess
r.mess flags=mnkci \# <1>
ref_env=bio_1,bio_2,bio_4,bio_8,bio_9,bio_13,bio_14,bio_15,bio_19 \
proj_env=bio.1,bio.2,bio.4,bio.8,bio.9,bio.13,bio.14,bio.15,bio.19 \
ref_rast=referencepoints output=mess nprocs=2 memory=1000
```

1.  Check the [manual page](https://grass.osgeo.org/grass-stable/manuals/addons/r.mess.html) about the meaning of these flags.

## {{< fa brands python >}}

``` python
gs.run_command("r.mess", flags="mnkci", # <1>
    ref_env=["bio_1", "bio_2", "bio_4", "bio_8", "bio_9",
             "bio_13", "bio_14", "bio_15", "bio_19"],
    proj_env=["bio.1", "bio.2", "bio.4", "bio.8","bio.9",
              "bio.13", "bio.14", "bio.15", "bio.19"],
    ref_rast="referencepoints", output="mess",
    nprocs=2, memory=1000,
)
```

1.  Check the [manual page](https://grass.osgeo.org/grass-stable/manuals/addons/r.mess.html) about the meaning of these flags.

## {{< fa regular window-restore >}}

Open the [r.mess]{.style-function} dialog and run it with:

| Parameter | Value |
|----|----|
| ref_env | bio_1,bio_2,bio_4,bio_8,bio_9,bio_13,bio_14,bio_15,bio_19 |
| proj_env | bio.1,bio.2,bio.4,bio.8,bio.9,bio.13,bio.14,bio.15,bio.19 |
| proj_env | attr |
| ref_rast | referencepoints |
| nprocs | 2 |
| memory | 1000 |
| Most dissimilar variable (MoD) (m) | ✅ |
| Area with negative MESS (n) | ✅ |
| sum(IES), where IES \< 0 (k) | ✅ |
| Number of IES layers with values \< 0 (c) | ✅ |
| Remove ind. env. similarity layers (IES) (i) | ✅ |

: {tbl-colwidths="\[42,57\]"}
:::

We have now five new layers that provide more insight in where future conditions are outside the range of conditions observed during model training. We can compare these to the predicted probability distribution of our species

### Results

The main MESS map (@fig-messmap) shows the degree of environmental similarity between current conditions and projected conditions in 2081 at each location. Negative values (indicated in red on the map) means there are one or more environmental variables outside the range present in the training data, so predictions in those areas should be treated with strong caution.

::: {.panel-tabset .exercise}
## MESS

![The MESS value at a location is determined by identifying the environmental variable that deviates the most in 2081 compared to its current range. <u>Negative MESS</u> values (indicated in red on the map) highlight areas with novel climate conditions, meaning that the projected 2081 climate variables fall outside the range observed under current conditions. This is expressed as a fraction of the variable’s current range (e.g., how far outside the known range the projected value lies). <u>Positive MESS</u> values (shown in blue) indicate areas where future conditions are not novel. A MESS value of 100 represents locations where the projected conditions in 2081 are identical to the median values of the corresponding variables in the current data. Lower positive values (closer to 0) are further from the median of the current conditions, but still within the observed range.](images/mess_MES.png){#fig-messmap fig-align="left" group="mcLze6OQc6"}

## MOB

![The analysis identifies the most novel variable at each point, revealing that bio_8 (mean temperature of the wettest quarter) and bio_9 (mean temperature of the driest quarter) are the most frequently novel variables overall. However, within the regions where the species has been observed, the results are more divers. In these areas, a variety of variables are projected to be the most novel for 2081 in small, localized regions, which probably reflects the highly topographically heterogeneous conditions.](images/mess_MoD.png){#fig-modmap fig-align="left" group="mcLze6OQc6"}

## Count Negative

![The number of variables with a negative MESS values, i.e., for each location, it shows the number of variables that are projected to have a value outside the range of values in the training data.](images/mess_CountNeg.png){#fig-countneg fig-align="left" group="mcLze6OQc6"}
:::

You can compare the MESS map with the occurrence point map (@fig-samlepred_model_01), the predicted probability map under future climate conditions (@fig-futdistr_01) and the presence-absence map (@fig-r_cross_01) to assess the reliability of the predictions. If you zoom in, you'll notice that for most locations where the species has been observed, conditions in 2081 are projected to remain within the range of observed values under current conditions.

Unfortunately, for our species, in some areas where the model predicts the species to be absent under current conditions but present under future conditions, the environmental conditions supporting such predictions fall outside the range of observed values. These predictions of increased suitability in these regions should therefore be interpreted with caution.
