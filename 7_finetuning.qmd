# Model fine-tuning {#sec-modelfinetuning}

## Introduction

The first part of this tutorial introduced the basic steps of species distribution modeling. And in @sec-modelvalidation, we learned how to use cross-validation to evaluate a model. In this section, we'll build on that foundation by examining how changes in parameters can influence model outcomes, using cross-validation to evaluate the results. Questions we’ll touch on here are e.g., what parameters can we adjust, and how do these changes impact the results? Which model should we ultimately select? Or could we even combine multiple models?

By now, you should be familiar with the basic steps of modeling. Since testing different parameter combinations, validating outcomes, and selecting the best-performing model can involve many steps that are easy to lose track of, it is recommended to use Bash or Python scripts during this phase. For this reason, the examples in this section will focus on these tools.

That said, if you prefer to use the graphical user interface (GUI), you can still follow along and manually adjust parameters. However, keep in mind that scripting offers greater efficiency and reproducibility for complex workflows.

## Get started

Maxent offers a number of options to fine-tune the model (@fig-UOU9jFNrtR). These can be found under the [Parameters]{.style-menu} and [Advanced]{.style-menu} tabs. We'll explore two of these options. For other parameters and a more detailed explanation of the options, see the tutorial and other materials on the [Maxent website](https://biodiversityinformatics.amnh.org/open_source/maxent/).

:::::: grid
::: {.g-col-md-7 .g-col-12}
![](images/r_maxent_train_parameteroptions.png)
:::

:::: {.g-col-md-5 .g-col-12}
::: removespace
![Under the tabs [Parameters]{.style-menu} and [Advanced]{.style-menu}, you'll find the various options to fine-tune the model. As a reminder, in each function dialog, the parameter (or flag) name is visible on the right side of each input field. This makes it simple to understand how the module dialog corresponds to the command representation. The other way around, each parameter setting you change is reflected in the corresponding bash code at the bottom of the dialog. As mentioned above, we'll show the Python and bash code, but it still can be useful to explore the different parameter options in the dialog.](images/r_maxent_train_parameteroptions_blank.png){#fig-UOU9jFNrtR}
:::
::::
::::::

To test these options, we follow an iterative process - each time you change some options, you have to rerun the model and evaluate the outcomes, like we did in @sec-4examinetheresults.

As we did previously, we we'll first create a separate sub-folder in our working directory for the new models we are going to create. Similarly, we will create a new mapset in our GRASS database for these models.

::: {#exm-DXBuP8nUXR .hiddendiv}
:::

::: {.panel-tabset group="bashpython"}
## {{< fa solid terminal >}}

``` bash
# Folders to store data
mkdir model_04

# Create a new mapset and switch to it
g.mapset -c mapset=model_04

# Define the region and set the MASK
g.region raster=bio_1@climate_current 
```

## {{< fa brands python >}}

``` python
# Set working directory
os.chdir("replace-for-path-to-working-directory")

# Create a new mapset and switch to it
gs.run_command("g.mapset", flags="c", mapset="model_04")

# Set the region and create a MASK
gs.run_command("g.region", raster="bio_1@climate_current") 
```
:::

In the next section, we'll experiment with some parameter settings. We'll do this in the same mapset we just created. To keep track of the different model outcomes, we make sure to give the output layers and files a different (base) name each time we train a new model.

## Regularization

### Background

In MaxEnt, the regularization multiplier ([betamultiplier]{.style-parameter}) parameter controls the degree of regularization applied to the model. Regularization is a technique used to prevent [overfitting](https://en.wikipedia.org/wiki/Overfitting) by penalizing overly complex models.

A *lower regularization multiplier* (e.g., less than 1) allows the model to fit more complex relationships in the data, which can improve predictive performance on training data but increases the risk of overfitting and poor generalization to new data.

A *higher regularization multiplier* (e.g., 2 or higher) enforces stronger regularization, resulting in simpler models with fewer features. This reduces the risk of overfitting, especially when working with small datasets or datasets with noisy predictors. However, it may also reduce model accuracy.

![In panel A, underfitting occurs when the model is too simple to capture the underlying relationship between the predictor variable and the predicted value. This leads to high errors during training and is also likely to lead to poor performance on test data as the model fails to capture the underlying trends in the data. Overfitting, on the other hand, occurs when the model is overly complex and fits the training data too closely. This typically results in very low training error, but is likely to lead to poor generalisation and higher error on test data. The middle plot shows an optimal fit, where the model captures the essential patterns in the data without overfitting, achieving a balance that is likely to result in low errors on both training and test data. Panel B applies the same principles to a classification model. The underfitted model uses a linear boundary that fails to separate the two classes effectively. The overfitted model creates a highly complex decision boundary that closely matches every detail of the training data, but potentially performs poorly on unseen data. The optimal model strikes a balance, using a decision boundary that is likely to generalise well for both training and test data.](images/overfitting.png){#fig-overfitting fig-align="left"}

### Training

The default beta multiplier value is set to 1. In this example, we'll create two models using different regularization multiplier values: one with a value of 0.01 and another with a value of 3. Most settings will remain the same as described in @exm-fb4bZ2MIs0, including the use of 4-fold cross-validation. Two differences are that we will not create the prediction raster layers, and we will customize the output layer names instead of using the default names. Can you see how these output layer names are explicitly specified?

::: {#exm-xp61oQN3Ho .hiddendiv}
:::

::: {.panel-tabset group="bashpython"}
## {{< fa solid terminal >}}

First run using [betamultiplier]{.style-parameter} of 0.01:

``` bash
# Create output folder
mkdir model_04a

# Train model
r.maxent.train \
samplesfile=dataset01/species.swd \
environmentallayersfile=dataset01/background_points.swd \
outputdirectory=model_04a \
samplepredictions=Erealb_sp \
backgroundpredictions=Erealb_bg \
projectionlayers=dataset01/envdat \
predictionlayer=Erealb \
suffix=_m4a \
replicatetype=crossvalidate \
betamultiplier=0.01 \
replicates=4 threads=4 memory=1000 -ygb
```

Second run using [betamultiplier]{.style-parameter} of 3:

``` bash
# Create output folder
mkdir model_04b

# Train model
r.maxent.train \
samplesfile=dataset01/species.swd \
environmentallayersfile=dataset01/background_points.swd \
outputdirectory=model_04b \
samplepredictions=Erealb_sp \
backgroundpredictions=Erealb_bg \
projectionlayers=dataset01/envdat \
predictionlayer=Erealb \
suffix=_m4b \
replicatetype=crossvalidate \
betamultiplier=3 \
replicates=4 threads=4 memory=1000 -ygb
```

## {{< fa brands python >}}

First run using [betamultiplier]{.style-parameter} of 0.01:

``` python
# Create output folder
os.makedirs("model_04a", exist_ok=True)

# Train model
gs.run_command(
    "r.maxent.train",
    samplesfile="dataset01/species.swd",  
    environmentallayersfile="dataset01/background_points.swd",
    outputdirectory="model_04a", 
    samplepredictions="Erealb_sp",
    backgroundpredictions="Erealb_bg",
    projectionlayers="dataset01/envdat",
    predictionlayer="Erealb",
    suffix="_m4a", 
    replicatetype="crossvalidate",
    betamultiplier=0.01,
    replicates=4,
    threads=4,
    memory=1000,
    flags="ygb", 
)  
```

Second run using [betamultiplier]{.style-parameter} of 3:

``` python
# Create output folder
os.makedirs("model_04b", exist_ok=True)

# Train model
gs.run_command(
    "r.maxent.train",
    samplesfile="dataset01/species.swd",  
    environmentallayersfile="dataset01/background_points.swd",
    outputdirectory="model_04b",
    samplepredictions="Erealb_sp",
    backgroundpredictions="Erealb_bg",
    projectionlayers="dataset01/envdat",
    predictionlayer="Erealb",
    suffix="_m4b", 
    replicatetype="crossvalidate",
    betamultiplier=3,
    replicates=4,
    threads=4,
    memory=1000,
    flags="ygb", 
)  
```
:::

### Evaluation

We can now compare the test AUCs to determine which model performs better. These AUC values are printed to the console and can also be found in the [Erebia_alberganus_obs.html]{.style-file} files in the output folders of the models, together with other model statistics. If you followed the instructions, these are the folders [model_04a]{.style-folder} and [model_04b]{.style-folder}.

The results show that with a test AUC (area under the ROC) of 0.890 ± 0.003, [model 4a]{.style-output} appears to slightly outperform [model 4b]{.style-output}, which has a test AUC of 0.887 ± 0.003. See also the corresponding receiver operating characteristic (ROC) curves in @fig-rocmodel4a and [-@fig-rocmodel4b].

::: {.panel-tabset .exercise}
## ROC for model 4a

![The receiver operating characteristic (ROC) curve, again averaged over the 4 sub-models (replicate runs) of model 4a, using a regularization multiplier of 0.01. The average test AUC (area under the curve) for the replicate runs is 890, and the standard deviation is 0.003.](share/model_04a/plots/Erebia_alberganus_obs_roc.png){#fig-rocmodel4a fig-align="left"}

## ROC for model 4b

![The receiver operating characteristic (ROC) curve, again averaged over the 4 sub-models (replicate runs) of model 4b, using a regularization multiplier of 3. The average test AUC (area under the curve) for the replicate runs is 0.887, and the standard deviation is 0.003.](share/model_04b/plots/Erebia_alberganus_obs_roc.png){#fig-rocmodel4b fig-align="left"}
:::

We can also compare the variable importance tables and response curves of both models in the aforementioned [Erebia_alberganus_obs.html]{.style-file} files. The differences in variable importance and response curves are more pronounced. The response curves of [model 4a]{.style-output} have a larger standard deviation, indicating that the estimated relationships between environmental variables and species presence are more sensitive to the specific training data. This sensitivity reduces the model's generalizability. In contrast, the smoother response curves of [model 4b]{.style-output} suggest that it captures more general patterns, although it may miss some of the finer, true patterns present in the data. The response curves of model 3 (which we created in @sec-611) shows an intermediate between the two.

:::::: {.panel-tabset .exercise}
## Model 4a

::: {#fig-rcmodel4a layout-ncol="3"}
![](share/model_04a/plots/Erebia_alberganus_obs_bio_1.png){group="ADmw0wYYhy"}

![](share/model_04a/plots/Erebia_alberganus_obs_bio_8.png){group="ADmw0wYYhy"}

![](share/model_04a/plots/Erebia_alberganus_obs_bio_9.png){group="ADmw0wYYhy"}

Response curves created by varying the specific variable, while keeping all other variables fixed at their average sample value.
:::

## Model 3

::: {#fig-rcmodel4a layout-ncol="3"}
![](share/model_03/plots/Erebia_alberganus_obs_bio_1.png){group="Cl8jg8d8vi"}

![](share/model_03/plots/Erebia_alberganus_obs_bio_8.png){group="Cl8jg8d8vi"}

![](share/model_03/plots/Erebia_alberganus_obs_bio_9.png){group="Cl8jg8d8vi"}

Response curves created by varying the specific variable, while keeping all other variables fixed at their average sample value.
:::

## Model 4b

::: {#fig-rcmodel4b layout-ncol="3"}
![](share/model_04b/plots/Erebia_alberganus_obs_bio_1.png){group="tisHr12i1g"}

![](share/model_04b/plots/Erebia_alberganus_obs_bio_8.png){group="tisHr12i1g"}

![](share/model_04b/plots/Erebia_alberganus_obs_bio_9.png){group="tisHr12i1g"}

Response curves created by varying the specific variable, while keeping all other variables fixed at their average sample value.
:::
::::::

The maps in @fig-Erealbavm3 (created in @sec-611), @fig-Erealbavm4a, and @fig-Erealbavm4b show the predicted probabilities distribution based on model_03, model_04a, and model_04b, respectively. The different distribution patterns underscore the impact of regularization on prediction outcomes.

Model 4a, with a low regularization multiplier (0.01), captures more specific, localized patterns, which may be useful for identifying fine-scale details. However, this comes at the cost of reduced generalizability. Model 4b, with a higher regularization multiplier (3), emphasizes broader, general patterns. This results in smoother predictions that are less sensitive to the training data but may reduce detail, lead to slightly lower AUC values, and overestimate the distribution range. Model 3 represents a middle ground, balancing the capture of detail with maintaining generalizability.

::: {.panel-tabset .exercise}
## Model 4a

![Predicted probability distribution based on model 4a, using a regularization multiplier of 0.01. The average test AUC was 890 ± 0.003](images/Erealb_avg_m4a.png){#fig-Erealbavm4a group="jerZqYsEkn"}

## Model 3

![Predicted probability distribution based on model 3 (created in @sec-611), using a regularization multiplier of 1. The test AUC was 0.889 ± 0.003.](images/Erebia_alberganus_obs_envdat_avg.png){#fig-Erealbavm3 group="jerZqYsEkn"}

## Model 4b

![Predicted probability distribution based on model 4b, using a regularization multiplier of 3. The average test AUC was 887 ± 0.003](images/Erealb_avg_m4b.png){#fig-Erealbavm4b group="jerZqYsEkn"}
:::

When the goal of species distribution modeling is to predict future potential distribution patterns under changing climate conditions, assessing model robustness becomes essential. A robust model produces consistent and reliable predictions, even when input data vary. This is reflected in minimal variation in predictions across iterations of a cross-validation. In contrast, a large range or high standard deviation in predicted values suggests that the model's predictions are sensitive to the input data. Maps with the suffix [\_stddev]{.style-data} illustrate the standard deviation of predicted values across k iterations of a k-fold cross-validation. They are shown below in @fig-stddevmodel4a, @fig-stddevmodel3, and @fig-stddevmodel4b. See line 8-26 in the code block of [Appendix -@sec-ws4FDpBcUr] how to assign the same color table to the three maps.

::: {.panel-tabset .exercise}
## Model 4a

![Map showing the standard deviation of predicted values across the four iterations of the 4-fold cross-validation for Model 4a (low regularization multiplier, 0.01). The legend is truncated for readability, showing only the lower part of the standard deviation range."](images/stddev_model_4a.png){#fig-stddevmodel4a group="rf8GNKvMdo"}

## Model 3

![Map showing the standard deviation of predicted values across the four iterations of the 4-fold cross-validation for Model 3 (moderate regularization multiplier, 1).](images/stddev_model_3.png){#fig-stddevmodel3 group="rf8GNKvMdo"}

## Model 4b

![Map showing the standard deviation of predicted values across the four iterations of the 4-fold cross-validation for Model 4b (high regularization multiplier, 3).](images/stddev_model_4b.png){#fig-stddevmodel4b group="rf8GNKvMdo"}
:::

can be found in the column [Cloglog_range]{.style-data} of the attribute table of the vector layers [Erealb_bg_m4a]{.style-data} and [Erealb_bg_m4b]{.style-data}. Follow the same steps as in @sec-6probabilitymaps to assign a color scheme to these layers that represent the variability in predictions across the four models[^7_finetuning-1]. Use the same color scheme to

[^7_finetuning-1]: Not sure if it is a bug, but for the colors to show properly, you need to be in the mapset of the layer that you want to display.

Robust models show minimal variation in predictions across iterations, indicating stability and insensitivity to differences in input data. Conversely, a large range in predicted values indicates that the model's predictions are sensitive to the input data, raising concerns about its reliability under future conditions.

::: {.panel-tabset .exercise}
## Comparing averages

![Boxplots comparing the average predicted suitability values for occurrences and background points across the 4-fold cross-validation iterations for Models 4a, 3, and 4b.](images/boxplots_bgpredictions.png){#fig-stddevmodel4a fig-align="left" width="550" group="PGzAcbwwfw"}

## Comparing ranges

![Boxplots comparing difference between maximum and minimum predicted values (range) for occurrences and background points across the 4-fold cross-validation iterations for Models 4a, 3, and 4b. Note that for readability, the figure leaves out the more extreme outliers.](images/boxplots_rangepredictions.png){#fig-stddevmodel3 fig-align="left" width="550" group="PGzAcbwwfw"}
:::

The vector point layer containing the occurrence locations, created by setting the [-y]{.style-parameter} flag, includes a column that captures the range of predicted probability values across the four model iterations (e.g., @fig-predlaymodel03b). Similarly, the vector point layer for background locations, generated by setting the [-b]{.style-parameter} flag, also includes this column (see the examples below). Additionally, when we specify a folder with environmental layers using the [projectionlayers]{.style-parameter} parameter in combination with the cross-validation option, one of the outputs is a raster layer that represents the standard deviation of predicted probabilities across the four model iterations (e.g., @fig-samlepredmodel03b).

The maps with range values producted by [r.maxent.train]{.style-function} allow for a spatial comparison of prediction stability. To quickly compare how well the different models perform in this aspect, it is also helpful to summarize the range values using box plots. adsfasdf images/boxplots_bgpredictions.svg

images/boxplots_rangepredictions.png

The figure shows the variability in the range of predicted values across four cross-validation iterations for three models with different regularization settings. Model 4a, with the lowest regularization (betamultiplier = 0.01), exhibits the highest variability in predictions, particularly for occurrence points. This is evident from its larger median and wider spread compared to the other models. The increased variability suggests that Model 4a is highly sensitive to differences in the input data, reflecting a tendency to overfit, as it assigns very high predicted values to areas near occurrence points while keeping predictions for background points more restricted.

In contrast, Model 4b, with the highest regularization (betamultiplier = 3), shows a much narrower range of variability, indicating more generalized predictions. The relatively low median and tighter spread for occurrence points suggest that this model is less responsive to input data differences but may underestimate the specificity of species distribution. For background points, Model 4b has the most consistent predictions, further highlighting its regularization's smoothing effect.

Model 3, with an intermediate regularization (betamultiplier = 1), balances these tendencies. Its predictions for occurrence points show moderate variability, suggesting it generalizes better than Model 4a while retaining more flexibility than Model 4b. However, the spread for background points is still relatively high, indicating some sensitivity to variation in input data.

Overall, Model 4a risks overfitting by being too sensitive to input data, whereas Model 4b leans toward underfitting by overly generalizing. Model 3 strikes a middle ground but may still slightly overestimate the areas suitable for the species.

These differences highlight the importance of balancing regularization to achieve predictions that are both accurate and generalizable. The choice between models depends on the research question and the intended application. If the focus is on fine-scale habitat suitability, model 4a may be more appropriate. However, for broader-scale analyses where general trends are more important, model 3 or 4b might the better option.
