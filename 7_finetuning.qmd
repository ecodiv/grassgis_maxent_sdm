# Model fine-tuning {#sec-modelfinetuning}

The first part of this tutorial introduced the basic steps of species distribution modeling. And in @sec-modelvalidation, we learned how to use cross-validation to evaluate a model. In this section, we'll build on that foundation by examining how changes in parameters can influence model outcomes, using cross-validation to evaluate the results. Questions we’ll touch on here are e.g., what parameters can we adjust, and how do these changes impact the results? Which model should we ultimately select? Or could we even combine multiple models?

By now, you should be familiar with the basic steps of modeling. Since testing different parameter combinations, validating outcomes, and selecting the best-performing model can involve many steps that are easy to lose track of, it is recommended to use Bash or Python scripts during this phase. For this reason, the examples in this section will focus on these tools.

That said, if you prefer to use the graphical user interface (GUI), you can still follow along and manually adjust parameters. However, keep in mind that scripting offers greater efficiency and reproducibility for complex workflows.

## Organize outputs.

As we did previously, we we'll first create a separate sub-folder in our working directory for the new models we are going to create. Similarly, we will create a new mapset in our GRASS database for these models.

::: {#exm-DXBuP8nUXR .hiddendiv}
:::

::: {.panel-tabset group="interface"}
## {{< fa solid terminal >}}

``` bash
# Folders to store data
mkdir model_04

# Create a new mapset and switch to it
g.mapset -c mapset=model_04

# Define the region and set the MASK
g.region raster=bio_1@climate_current 
```

## {{< fa brands python >}}

``` python
# Set working directory and create a new folder in the working directory
os.chdir("replace-for-path-to-working-directory")
os.makedirs("model_04", exist_ok=True)

# Create a new mapset and switch to it
gs.run_command("g.mapset", flags="c", mapset="model_04")

# Set the region and create a MASK
gs.run_command("g.region", raster="bio_1@climate_current") 
```
:::

In the next section, we'll experiment with some parameter settings. We'll do this in the same mapset we just created. To keep track of the different model outcomes, we make sure to give the output layers and files a different (base) name each time we train a new model.

## Model training {#sec-modeltraining2}

Maxent offers a number of options to fine-tune the model (@fig-UOU9jFNrtR). To test these options, we follow an iterative process - each time you change some options, you have to rerun the model and evaluate the outcomes, like we did in @sec-4examinetheresults.

:::::: grid
::: {.g-col-md-7 .g-col-12}
![](images/r_maxent_train_parameteroptions.png)
:::

:::: {.g-col-md-5 .g-col-12}
::: removespace
![Under the tabs [Parameters]{.style-menu} and [Advanced]{.style-menu}, you'll find the various options to fine-tune the model. As a reminder, in each function dialog, the parameter (or flag) name is visible on the right side of each input field. This makes it simple to understand how the module dialog corresponds to the command representation. The other way around, each parameter setting you change is reflected in the corresponding bash code at the bottom of the dialog. As mentioned above, we'll show the Python and bash code, but it still can be useful to explore the different parameter options in the dialog.](images/r_maxent_train_parameteroptions_blank.png){#fig-UOU9jFNrtR}
:::
::::
::::::

Under the [Parameters]{.style-menu} and [Advanced]{.style-menu} tabs, you’ll find a comprehensive list of parameters that can be used to fine-tune the model. We'll explore two of these options. Each time you change some options, you have to rerun the model and evaluate the outcomes using the steps explained in the previous sections. See the original tutorial and the materials on the [Maxent website](https://biodiversityinformatics.amnh.org/open_source/maxent/) for more examples and explanations.

### Regularization

In MaxEnt, the regularization multiplier ([betamultiplier]{.style-parameter}) parameter controls the degree of regularization applied to the model. Regularization is a technique used to prevent [overfitting](https://en.wikipedia.org/wiki/Overfitting) by penalizing overly complex models.

A *higher regularization multiplier* (e.g., 2 or higher) enforces stronger regularization, resulting in simpler models with fewer features. This reduces the risk of overfitting, especially when working with small datasets or datasets with noisy predictors. However, it may also reduce model accuracy.

A *lower regularization multiplier* (e.g., less than 1) allows the model to fit more complex relationships in the data, which can improve predictive performance on training data but increases the risk of overfitting and poor generalization to new data.

![Example of under- and overfitting a regression model (A) and a classification model (B). Underfitting results in both a high training and test error. Overfitting will generally result in an low training error and high test error.](images/overfitting.png){#fig-overfitting fig-align="left"}

The default beta mulitplier value is 1. Let's try to create two models, one using a beta multiplier of 0,01 and another using a betamultiplier of 3.
