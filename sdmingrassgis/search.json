[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Species distribution modelling",
    "section": "",
    "text": "Preface\nSpecies distribution modeling (SDM), also known as climate envelope modeling or niche modeling, are numerical methods that combine observations of species occurrence or abundance with environmental estimates. It is used to gain ecological and evolutionary insights and to predict distributions across landscapes [1], and have become an important tool in the toolbox of ecologists and professionals involved in e.g, conservation planning and management, biodiversity monitoring and environmental impact assessment.\n\n\n\n\n\n\n\n\nFigure 1: The Almond-eyed Ringlet (Erebia alberganus). Source: Wikimedia, license CC BY-SA 3.0\n\n\n\nMaxent [3] is one of the more popular algorithms for species distribution modeling. Unlike many other algorithms, it focuses specifically on presence-only data, which is typically the type of data available. Maxent is, among others, available as a stand-alone program [4] and as R package [5]. This tutorial focuses on the use of Maxent addons for GRASS GIS. As example, we will use SDM to map the potential distribution of species using the example of the Almond-eyed Ringlet (Erebia alberganus), a butterfly found in parts of Austria, Bulgaria, France, Italy, Serbia and Switzerland [6].\n\n\n\n\n\n\n Objective\n\n\n\nThis tutorial aims to introduce the Maxent tool set in GRASS [7], and to illustrate how to use it with other GIS modules to create an effective and flexible analytical workflow.\n\n\nYou are assumed to be familiar with the fundamentals of species distribution modelling (SDM). If not, see this free online course for a comprehensive introduction to the concepts and principles of SDM. For a more in-depth discussion, see the paper by Elith et al. [1].\nYou are also assumed to be familiar with GRASS GIS. If not, see Appendix A for a quick introduction and check out the list of introductory and advanced tutorials and courses.\n\n\n\n\n1. Elith J, Leathwick JR (2009) Species Distribution Models: Ecological Explanation and Prediction Across Space and Time. Annual Review of Ecology, Evolution, and Systematics 40(1):677–697. https://doi.org/10.1146/annurev.ecolsys.110308.120159\n\n\n2. Phillips SJ, Anderson RP, Schapire RE (2006) Maximum entropy modeling of species geographic distributions. Ecological Modelling 190(3):231–259. https://doi.org/10.1016/j.ecolmodel.2005.03.026\n\n\n3. Phillips SJ, Anderson RP, Dudík M, Schapire RE, Blair ME (2017) Opening the black box: An open-source release of Maxent. Ecography 40(7):887–893. https://doi.org/10.1111/ecog.03049\n\n\n4. Phillips SJ, Dudík M, Schapire R (2024) Maxent software for modeling species niches and distributions (version 3.4.4)\n\n\n5. Phillips S (2021) Maxnet: Fitting ’maxent’ species distribution models with ’glmnet’\n\n\n6. van Swaay C, Wynhoff I, Verovnik R, et al (2010) Erebia albergana. The IUCN Red List of Threatened Species 2010. IUCN Red List of Threatened Species. https://doi.org/10.2305/IUCN.UK.2010-1.RLTS.T173278A6984115.en\n\n\n7. GRASS Development Team (2024) GRASS GIS",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "1_introduction.html",
    "href": "1_introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 About the tutorial\nThis tutorial introduces a GRASS GIS toolset for species distribution modelling. You will learn how to use it in combination with other core modules and add-ons to acquire and import data (section 2), prepare data (section 3), train, validate and evaluate distribution models (section 4), and use these models to predict the future potential distribution of the almond-eyed ringlet butterfly (section 5). In subsequent sections, which will be available in the coming weeks, we’ll dive deeper into the options, choices, and parameter settings to validate and fine-tune the model.\nThe examples in this tutorial show you how to run functions from the command line, using Python and, and using the GUI dialogs. There will not (always) be an explanation of the different settings and parameters. Instead, each time a function is used, a link is provided to the corresponding help page. These help pages are quite good, and you are encouraged to read them to fully understand the choices made in the tutorial.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "1_introduction.html#about-the-tutorial",
    "href": "1_introduction.html#about-the-tutorial",
    "title": "1  Introduction",
    "section": "",
    "text": "Figure 1.1: The four main steps in species distribution modeling and the corresponding GRASS GIS addon. The rationale for having separate addons for each step is to encourage and help the user to carefully consider the assumptions and considerations at each stage of the modelling process. The tutorial follows the same structure.\n\n\n\n\n\n\n\n\n\n\nTips and conventions used in this tutorial.\n\n\n\n\n\nThe following typographical conventions are used in this tutorial:\n\nNames of databases, storage locations, file names\nNames of GRASS data layers\nMenu and toolbar items\nName of programs, code libraries, addons\nFunction names, parameters, variable names\nParameter settings, user-supplied values\n\nThe examples in this tutorial show you how to run modules / functions using:\n\n     the command line\n     Python code\n     GUI dialogs\n\nThroughout the text, you’ll find cross-references with hyperlinks to e.g., figures, literature references and footnotes. Instead of clicking the hyperlink, you can also hoover over the link with your mouse. This will open the corresponding figure, reference or footnote in a floating Window. Try for example ?fig-erebiaalbergana.\nSome code is enriched with explanatory notes. You can read these notes by clicking on the number next to the code. See for example:\n1print(\"this is an example\")\n\n1\n\nYou got it\n\n\nClicking on the circled number in the code block will open a pop-up window. If you hover over a code block, you will also see a small ‘clipboard’ icon . Click it to copy the code.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "1_introduction.html#the-software",
    "href": "1_introduction.html#the-software",
    "title": "1  Introduction",
    "section": "1.2 The software",
    "text": "1.2 The software\n\n1.2.1 GRASS GIS\nGo to the GRASS GIS website and open the download page for your operating system from the menu. Use the stable release (at the time of writing, this is version 8.4.0).\nWindow users will find that there are two main flavors on the download page; the standalone winGRASS installer and the OSGeo4W installer. I recommend the latter, as it allows you to install a broad range of open source geospatial software packages, such as QGIS and GDAL/OGR. And it offers an easier way to update the software. Note that on Windows, if you have QGIS installed, there is a good change you already have GRASS GIS. In that case, just check if you have the latest version.\n\n\n1.2.2 GRASS addons\nThe three main modules used in this tutorial are v.maxent.swd, r.maxent.train and r.maxent.predict. Install these addons with the g.extension module.\n\nStep 1.1  \n\n\n\n\n\ng.extension extension=v.maxent.swd\ng.extension extension=r.maxent.train\ng.extension extension=r.maxent.predict\n\n\n1import grass.script as gs\ngs.run_command(\"g.extension\", extension=\"v.maxent.swd\")\ngs.run_command(\"g.extension\", extension=\"r.maxent.train\")\ngs.run_command(\"g.extension\", extension=\"r.maxent.predict\")\n\n1\n\nNote that you only need to import the grass.script once per session. So this will only be shown once here.\n\n\n\n\n\nRepeat the same for the other two addons.\n\n\n\nInstalled modules can be run by typing their name in the Console or terminal. They are also available from the Tools panel under Addons. If you don’t see them after installing them, you may have to restart GRASS first.\nYou will install and use a few other addons later on in the tutorial. See ?sec-functions for an overview of all addons used in this tutorial.\n\n\n1.2.3 Maxent\nThe addons r.maxent.train and r.maxent.predict both require the Maxent software. Download the software from the Maxent website and extract the Maxent.jar file. When running r.maxent.train and r.maxent.predict for the first time you’ll need to provide the path to this file. For now, just remember the location of the Maxent.jar file.\nThe Maxent software can be used on any computer running Java version 1.4 or later. For the Java runtime environment, the Maxent website refers to java.sun.com/javase/downloads, but OpenJDK should work as well. There is a good change you already have Java installed, but if not, you can download it from here. For Windows users, you can download an installer of an Azul Zulu build of OpenJDK here.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "1_introduction.html#sec-creatingadatabase",
    "href": "1_introduction.html#sec-creatingadatabase",
    "title": "1  Introduction",
    "section": "1.3 The database",
    "text": "1.3 The database\nOnce you have started GRASS GIS, the first thing to do for this tutorial is to create a new GRASS GIS database called GRASSdb. Next, create a new project called SDM with the coordinate reference system (CRS) WGS84 lat/lon (EPSG 4326). The easiest way to do this is using the GUI, but if you want to make it part of an automated process, you can do this in Python as well.\n\nStep 1.2  \n\n\n\n\n\n\n\n\n# Create the folder for the GRASS GIS database\n1os.mkdir(\"/media/paulo/HD2/\")\n\n# Create a new Project\ngs.create_project(\n    path=\"/media/paulo/HD2/GRASSdb\",\n    name=\"SDM\",\n    epsg=\"4326\",\n)\n\n# Change to the newly created Project / mapset\ngs.run_command(\"g.mapset\", mapset=\"PERMANENT\", project=\"SDM\")\n\n1\n\nReplace the path with the path to the directory where you want to create the database. Note, on Windows, paths are written using backslashes (\\) as the separator between folder names. On Unix based operating system such as macOS, Linux, and BSDs, the forward slash (/) is used as the path separator.\n\n\n\n\n\nNow that you have your database ready, you can start with the next step, which is getting and preparing the input data for the species distribution modeling.\n\n\n\n\n1. van Swaay C, Wynhoff I, Verovnik R, et al (2010) Erebia albergana. The IUCN Red List of Threatened Species 2010. IUCN Red List of Threatened Species. https://doi.org/10.2305/IUCN.UK.2010-1.RLTS.T173278A6984115.en",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "2_obtaining_data.html",
    "href": "2_obtaining_data.html",
    "title": "2  Acquire the data",
    "section": "",
    "text": "2.1 Working directory\nCertain modules require a file as input or output. You can use the full path to the file. However, if you also define a working directory and you put your files in that working directory. The working directory is a directory where GRASS would look for or output files to if the full path is not specified. That means that you only need to provide the path relative to the working directory. For this section, create your own working directory where you will store the downloaded data.\nNow, whenever you have to refer to a file, it is enough to specify the name of the file instead of the full path. Note that this applies to external files such as text files or GeoTiff files. This does not apply to raster maps, vector maps and other geospatial data stored in GRASS database which do not need any path to be specified.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Acquire the data</span>"
    ]
  },
  {
    "objectID": "2_obtaining_data.html#sec-workingdirectory",
    "href": "2_obtaining_data.html#sec-workingdirectory",
    "title": "2  Acquire the data",
    "section": "",
    "text": "Step 2.1  \n\n\n\n\n\nType cd followed by the path to the working directory in the wxGUI Command Console. If you prefer to work on the terminal, run the same on the terminal.\ncd replace-for-path-to-working-directory\n\n\nimport os\nos.chdir(\"replace-for-path-to-working-directory\")\n\n\nThe working directory can also be changed in wxGUI menu Settings → GRASS working environment → Change working directory\n\n\n\n\n\n\nFigure 2.1: Set the working directory via the menu or using the command line. After doing so, GRASS will look for files in that location if the full path is not provided.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Acquire the data</span>"
    ]
  },
  {
    "objectID": "2_obtaining_data.html#sec-baselayers",
    "href": "2_obtaining_data.html#sec-baselayers",
    "title": "2  Acquire the data",
    "section": "2.2 Base layers",
    "text": "2.2 Base layers\nWe’ll download and import a few base layers, including maps of the national boundaries, roads and rivers. For an effective workflow, it is a good idea to organize your data well. For this tutorial, we’ll keep the base layers in the PERMANENT mapset. For input data for our modeling and the modeling results, we’ll use separate mapsets.\n\n2.2.1 Boundaries\nWe’ll use the 1:10m scale data from Natural Earth, a public domain map dataset available at various scales. We download the Shapefile from Admin 0 - countries and unzip the data to the working directory. Next, we import the Shapefile in GRASS GIS as admin0 using r.in.ogr.\n\nStep 2.2  \n\n\n\n\n\nv.in.ogr \\\n  input=ne_10m_admin_0_countries.shp \\\n  output=admin0\n\n\ngs.run_command(\n    \"v.in.ogr\",\n    input=\"ne_10m_admin_0_countries.shp\",\n    output=\"admin0\",\n)\n\n\nOpen the v.in.ogr dialog and fill in:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nInput\nne_10m_admin_0_countries.shp\n\n\nOutput\nadmin0\n\n\n\n\n\n\nThe Almond-eyed Ringlet occurs in parts of Europe, so we’ll limit the areas for which we download the rest of the data to Europe, by setting the extent of the computational region (Section A.7). This can be done using g.region if we know the north, south, west and east limits, or interactively, using the Set computational region extent interactively option under the Various zoom options  button. For the latter, see the  tab below.\n\nStep 2.3  \n\n\n\n\n\ng.region n=55 s=35 w=-10 e=33\n\n\ngs.run_command(\"g.region\", n=55, s=35, w=-10, e=33)\n\n\n\n\n\n\nThe next step is to create a vector layer aoi with a polygon representing the boundaries of the region’s extent using v.in.region.\n\nStep 2.4  \n\n\n\n\n\nv.in.region output=aoi\n\n\ngs.run_command(\"v.in.region\", output=\"aoi\")\n\n\nOpen the v.in.region dialog. Fill in:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\noutput\naoi\n\n\n\n\n\n\nTo save some space, we can use the aoi vector layer to clip the admin0 layer to the spatial bounds of the computational region using v.clip, save the result as countries, and subsequently remove the admin0 layer using the g.remove module.\n\nStep 2.5  \n\n\n\n\n\nv.clip input=admin0 clip=aoi output=countries\ng.remove -f type=vector name=admin0\n\n\ngs.run_command(\"v.clip\", input=\"admin0\", clip=\"aoi\", output=\"countries\")\ngs.run_command(\"g.remove\", flags=\"f\", type=\"vector\", name=\"admin0\")\n\n\nOpen the v.clip dialog, and fill in:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nInput\ntmp\n\n\nClip\naoi\n\n\nOutput\ncountries\n\n\n\n\n\n\n\n\n\n\n2.2.2 Urban areas\nWe’ll get the vector layer with urban areas from Natural Earth as well. Download it from here and unzip it into your working directory. Similar to the previous step, we’ll import the data as tmp using v.in.ogr. To speed up the import, we set the -r flag to only import the vector features that fall completely or partially into the region’s spatial bounds.\n\nStep 2.6  \n\n\n\n\n\nv.in.ogr -r \\\n  input=ne_10m_urban_areas.shp \\\n  output=tmp\n\n\ngs.run_command(\n    \"v.in.ogr\",\n    flags=\"r\",\n    input=\"ne_10m_urban_areas.shp\",\n    output=\"tmp\",\n)\n\n\nOpen the v.in.ogr dialog, and fill in:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nInput\nne_10m_urban_areas.shp\n\n\nOutput\ntmp\n\n\nLimit the import to the current region (r)\n✅\n\n\n\n\n\n\nNext, we clip the layer to the boundaries of our region with v.clip and remove the temporary data.\n\nStep 2.7  \n\n\n\n\n\nv.clip input=tmp clip=aoi output=urban\ng.remove -f type=vector name=tmp\n\n\ngs.run_command(\"v.clip\", input=\"tmp\", clip=\"aoi\", output=\"urban\")\ngs.run_command(\"g.remove\", flags=\"f\", type=\"vector\", name=\"tmp\")\n\n\nOpen the v.clip dialog, and fill in:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nInput\ntmp\n\n\nClip\naoi\n\n\nOutput\nurban\n\n\n\n\n\n\n\n\n2.2.3 Roads\nWe’ll get the road data from Natural Earth as well. Download it from here and unzip it into your working directory. Similar to the previous step, we’ll import the data as tmp using v.in.ogr with the -r flag.\nThe vector data includes line features representing roads and ferry routes. Whether a line feature is a road or ferry route is defined in the featurecla column of the attribute table. We only want to import the roads. We can do this using the where.The where takes as SQL Where query as an argument to select the features that match a particular condition.\n\nStep 2.8  \n\n\n\n\n\nv.in.ogr -r \\\n  input=ne_10m_roads.shp \\\n  where=\"featurecla='Road'\" \\\n  output=tmp\n\n\ngs.run_command(\n    \"v.in.ogr\",\n    flags=\"r\",\n    input=\"ne_10m_roads.shp\",\n    where=\"featurecla='Road'\",\n    output=\"tmp\",\n)\n\n\nOpen the v.in.ogr dialog, and fill in:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nInput\nne_10m_roads.shp\n\n\nOutput\ntmp\n\n\nWhere\nfeaturecla='Road'\n\n\nLimit the import to the current region (r)\n✅\n\n\n\n\n\n\nNext, we clip the layer to the boundaries of our region with v.clip and remove the temporary data.\nClipping this particular data will result in a vector layer without an attribute table. This type of error is often caused by incorrect attribute data, such as hard returns or invalid column names. The latter is the case here. There is a column ‘add’, which GRASS does not accept. There are two ways to fix this. Rename the offending column with v.db.renamecolumn or remove it using v.db.dropcolumn. We’ll go for the latter solution.\n\nStep 2.9  \n\n\n\n\n\n# Remove problematic column\nv.db.dropcolumn map=tmp columns=add\n\n# Clip to aoi spatial bounds\nv.clip input=tmp clip=aoi output=roads\n\n# Remove temporary data\ng.remove -f type=vector name=tmp\n\n\n# Remove problematic column\ngs.run_command(\"v.db.dropcolumn\", map=\"tmp\", columns=\"add\")\n\n# Clip to aoi spatial bounds\ngs.run_command(\"v.clip\", input=\"tmp\", clip=\"aoi\", output=\"roads\")\n\n# Remove temporary data\ngs.run_command(\"g.remove\", flags=\"f\", type=\"vector\", name=\"tmp\")\n\n\nOpen the v.db.dropcolumn dialog.\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nmap\ntmp\n\n\ncolumn\nadd\n\n\n\nOpen the v.clip dialog, and fill in:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\ninput\ntmp\n\n\nclip\naoi\n\n\noutput\nroads\n\n\n\nOpen the g.remove dialog, and fill in:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\ntype\nvector\n\n\nname\ntmp\n\n\nForce removal (r)\n✅\n\n\n\n\n\n\n\n\n2.2.4 Rivers\nWe get the river data from Natural Earth as well (download link). Download both the rivers and lake centerlines and Europe supplement Shapefiles and unzip them into your working directory. We’ll use the same steps as above to import these layers.\n\nStep 2.10  \n\n\n\n\n\nv.in.ogr -r input=ne_10m_rivers_lake_centerlines.shp output=tmp1\nv.in.ogr -r input=ne_10m_rivers_europe.shp output=tmp2\n\n\ngs.run_command(\n    \"v.in.ogr\",\n    flags=\"r\",\n    input=\"ne_10m_rivers_lake_centerlines.shp\",\n    output=\"tmp1\",\n)\ngs.run_command(\n    \"v.in.ogr\",\n    flags=\"r\",\n    input=\"ne_10m_rivers_europe.shp\",\n    output=\"tmp2\",\n)\n\n\nTo import ne_10m_rivers_lake_centerlines.shp, fill in the following parameters in v.in.ogr:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\ninput\nne_10m_rivers_lake_centerlines.shp\n\n\noutput\ntmp1\n\n\nLimit the import to the current region (r)\n✅\n\n\n\nAnd, to import the ne_10m_rivers_europe.shp:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\ninput\nne_10m_rivers_europe.shp\n\n\noutput\ntmp2\n\n\nLimit the import to the current region (r)\n✅\n\n\n\n\n\n\nWe now clip the imported layers to the boundaries of the European mainland.\n\nStep 2.11  \n\n\n\n\n\nv.clip input=tmp1 clip=aoi output=tmp3\nv.clip input=tmp2 clip=aoi output=tmp4\n\n\ngs.run_command(\"v.clip\", input=\"tmp1\", clip=\"aoi\", output=\"tmp3\")\ngs.run_command(\"v.clip\", input=\"tmp2\", clip=\"aoi\", output=\"tmp4\")\n\n\nTo clip tmp1, open the v.clip dialog and fill in the following parameters:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\ninput\ntmp1\n\n\noutput\naoi\n\n\nclip\ntmp3\n\n\n\nAnd, to clip tmp2 to the area of interest (aoi):\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\ninput\ntmp2\n\n\noutput\naoi\n\n\nclip\ntmp4\n\n\n\n\n\n\nNext step would be to combine the two layers tmp3 and tmp4. However, the columns in the attribute tables of the two layers are different, both in their names and their order. To make these tables compatible, we need to rearrange and rename the columns so that both tables have the same structure. This requires some SQL code, which we can run using the db.execute module. We first rename the original tables.\n\nStep 2.12  \n\n\n\n\n\n1# Rename the attribute tables\ndb.execute sql=\"ALTER TABLE tmp3 RENAME TO tmp3old;\"\ndb.execute sql=\"ALTER TABLE tmp4 RENAME TO tmp4old;\"\n\n1\n\nWe use the SQLite ALTER TABLE command to renames the existing attribute table. This will decouple them from the vector layer (geometry).\n\n\n\n\n1# Rename the attribute tables\ngs.run_command(\n    \"db.execute\",\n    sql=\"ALTER TABLE tmp3 RENAME TO tmp3old;\",\n)\ngs.run_command(\n    \"db.execute\",\n    sql=\"ALTER TABLE tmp4 RENAME TO tmp4old;\",\n)\n\n1\n\nWe use the SQLite ALTER TABLE command to renames the existing attribute table. This will decouple them from the vector layer (geometry).\n\n\n\n\nTo rename the table tmp3 to tmp3old, open the db.execute dialog and, and in the sql field, we use the SQL ALTER TABLE command to rename the attribute tables of tmp31.\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nsql\nALTER TABLE tmp3 RENAME TO tmp3old\n\n\n\nLikewise, to rename the table tmp4:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nsql\nALTER TABLE tmp4 RENAME TO tmp4old\n\n\n\n\n\n\nWe then create new tables with a subset of columns from the original tables. We give these new tables the name of the original attribute tables. GRASS will automatically use these now as attribute tables.\n\nStep 2.13  \n\n\n\n\n\n1# Create new tables\ndb.execute \\\nsql=\"CREATE TABLE tmp3 AS SELECT cat,scalerank,featurecla,name,label,min_zoom,min_label FROM tmp3old\"\ndb.execute sql= \"CREATE TABLE tmp4 AS SELECT cat,scalerank,featurecla,name,label,min_zoom,min_label FROM tmp4old\"\n\n1\n\nWe use the SQLite CREATE TABLE command to create a new table with the original name, but with the correct structure, based on a selection of columns from the table we just renamed in the order we want. Note, this includes the cat column, which is used to link the table attributes to the vector features (see Section A.2.3). The new table gets the name of the original table.\n\n\n\n\n1# New attribute tables\ngs.run_command(\n    \"db.execute\",\n    sql=(\n        \"CREATE TABLE tmp3 AS \"\n        \"SELECT cat,scalerank,featurecla,name,label,min_zoom,min_label \"\n        \"FROM tmp3old\"\n    ),\n)\ngs.run_command(\n    \"db.execute\",\n    sql=(\n        \"CREATE TABLE tmp4 AS \"\n        \"SELECT cat,scalerank,featurecla,name,label,min_zoom,min_label \"\n        \"FROM tmp4old\"\n    ),\n)\n\n1\n\nWe use the SQLite CREATE TABLE command to create a new table with the original name, but with the correct structure, based on a selection of columns from the table we just renamed in the order we want. Note, this includes the cat column, which is used to link the table attributes to the vector features (see Section A.2.3). The new table gets the name of the original table.\n\n\n\n\nTo create a new attribute tables tmp3 with a subset of the columns from tmp3old, open the db.execute dialog, and run it with the following SQL code2:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nsql\nCREATE TABLE tmp3 AS SELECT cat,scalerank,featurecla,name,label,min_zoom,min_label FROM tmp3old\n\n\n\nAnd to create a new attribute tables tmp4 with a subset of the columns from tmp4old:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nsql\nCREATE TABLE tmp4 AS SELECT cat,scalerank,featurecla,name,label,min_zoom,min_label FROM tmp4old\n\n\n\n\n\n\nAfter we have assured the attribute tables of both layers have the same structure, we use v.patch to combine them into one layer called rivers.\n\nStep 2.14  \n\n\n\n\n\nv.patch input=tmp3,tmp4 output=rivers\n\n\ngs.run_command(\"v.patch\", input=[\"tmp3\", \"tmp4\"], output=\"rivers\")\n\n\nTo patch the two vector layers tmp3 and tmp4, open the v.patch dialog and run the function with the following parameters:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\ninput\ntmp3,tmp4\n\n\noutput\nrivers\n\n\n\n\n\n\nLast step is to remove the temporary layers. We use db.droptable to remove the ‘stand-alone’ tables, and g.remove to remove the intermediate vector layers. We use the pattern parameter and a wildcard to remove all vector layers with a name that start with tmp. See the g.remove manual page for more details.\n\nStep 2.15  \n\n\n\n\n\n1db.droptable -f table=tmp3old\ndb.droptable -f table=tmp4old\ng.remove -f type=vector pattern=tmp*\n\n1\n\nThe db.droptable module runs the SQLite DROP TABLE function in the background to remove the old table.\n\n\n\n\n1gs.run_command(\"db.droptable\", flags=\"f\", table=\"tmp3old\")\ngs.run_command(\"db.droptable\", flags=\"f\", table=\"tmp4old\")\ngs.run_command(\"g.remove\", flags=\"f\", type=\"vector\", pattern=\"tmp*\")\n\n1\n\nThe db.droptable module runs the SQLite DROP TABLE function in the background to remove the old table.\n\n\n\n\nOpen the db.droptable dialog, and use the following parameter settings:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\ntable\ntmp3old\n\n\n\nRepeat this, but replace tmp3old with tmp4old.\nRun g.remove with the following parameter settings:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\ntype\nvector\n\n\npattern\ntmp*\n\n\nForce removal (r)\n✅\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: The study area with the main roads, rivers and urban areas. Data sources: Natural Earth.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Acquire the data</span>"
    ]
  },
  {
    "objectID": "2_obtaining_data.html#sec-speciesdata",
    "href": "2_obtaining_data.html#sec-speciesdata",
    "title": "2  Acquire the data",
    "section": "2.3 Species data",
    "text": "2.3 Species data\n\n2.3.1 Range map\nLet’s see what the experts have to say about the distribution of our species. A good starting point is the range maps from the IUCN Red List website. You’ll need to log in first (create an account if you don’t already have one), then use the search bar to find the species (search by scientific name). Note, however, that on the RedList, the use a different scientific name, namely Erebia albergana (that is something to take into account, in some cases, there is no consensus on the scientific name of a species). Download the range data and unzip it into your working directory3.\nWe’ll create a new mapset species_data for our species distribution data and import the range map into this mapset.\n\nStep 2.16  \n\n\n\n\n\ng.mapset -c mapset=species_data\nv.in.ogr input=data_0.shp output=Erebia_alberganus_rangemap\n\n\ngs.run_command(\"g.mapset\", flags=\"c\", mapset=\"species_data\")\ngs.run_command(\n    \"v.in.ogr\",\n    input=\"data_0.shp\",\n    output=\"Erebia_alberganus_rangemap\",\n)\n\n\nType in g.mapset in the console to open the dialog, and use the following parameter settings.\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nmapset\nspecies_data\n\n\nCreate mapset if it doesn’t exist (c)\n✅\n\n\n\nRun v.in.ogr with the following parameter settings:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\ninput\ndata_0.shp\n\n\noutput\nErebia_alberganus_rangemap\n\n\n\n\n\n\n\n\n2.3.2 Occurrence data\nThe range map on the Red List website shows the boundaries of the area(s) where the species is thought to occur. The Global Biodiversity Information Facility (GBIF) website provides information on where the species has actually been observed. The GBIF data is the type of data typically used for species distribution modeling. GBIF is an international open data infrastructure that provides free and open access to a vast amount of biodiversity data collected over three centuries of natural history exploration, including current observations from citizen scientists, researchers, and automated monitoring programs.\nBefore downloading data, you must first register on the site. Once logged in, search for Erebia alberganus von Prunner, 1798. Select the result and open the page. Then click on the green button with the number of observations to open the table with observations.\n\n\n\n\n\n\nFigure 2.3: At the time of writing, GBIF had 16468 occurrences of Erebia alberganus.\n\n\n\nIn the new window, apply any filters that you think are important. Note which filters and what settings you used to select the records. Tip: open the map tab and check the map each time you change a filter to see how it affects the selection of observations.\n\n\n\n\n\n\nFigure 2.4: The table shows all occurrence records for our species. In the panel at the left, you can select specific records based on a number of criteria.\n\n\n\nClick on the download tab and select the Simple option. This will take you to the download page. When the download is complete, download the file and unzip into to your data folder 4.\nThe downloaded file is a tab delimited text file, which can be imported with the v.in.ascii module. A more convenient way is to use the v.in.gbif addon. We install the addon, and use this to import the data we just downloaded.\n\nStep 2.17  \n\n\n\n\n\ng.extension extension=v.in.gbif\nv.in.gbif input=0104058-240626123714530.csv output=Erebia_alberganus_obs\n\n\ngs.run_command(\"g.extension\", extension=\"v.in.gbif\")\ngs.run_command(\n    \"v.in.gbif\", input=\"0104058-240626123714530.csv\", output=\"Erebia_alberganus_obs\"\n)\n\n\nOpen the g.extension dialog, and run it with the following parameter settings:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nName of mapset (mapset)\nv.in.gbif\n\n\n\nNow, open the v.in.gbif dialog, and run it with the following parameter settings:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\ninput\n0104058-240626123714530.csv\n\n\noutput\nErebia_alberganus_obs\n\n\n\n\n\n\nThe GBIF website provides a number of filters to remove problematic records, such as fossil records or observations without location data. Even so, it is good practice to also check the downloaded data carefully for outliers and other possible problems. For example, we can compare the occurrence data with the RED list range map (Figure 2.5), or compare the country or lower-level administrative subdivisions of the site, as specified in the attribute table, with the location on the map. Besides the obvious errors, you may also encounter observations that are in unexpected, but not impossible locations. In these cases, you will need to look for other information to help you decide whether to include these points in your analysis.\n\n\n\n\n\n\nFigure 2.5: The Red List range map (green outline) and the GBIF occurrence data of Erebia alberganus. See here for the code used to create this map.\n\n\n\nAlthough there are some noticeable discrepancies between the range map and the occurrence data, for the sake of this tutorial, we’ll assume that the GBIF data is OK.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Acquire the data</span>"
    ]
  },
  {
    "objectID": "2_obtaining_data.html#sec-climatedata",
    "href": "2_obtaining_data.html#sec-climatedata",
    "title": "2  Acquire the data",
    "section": "2.4 Current climate",
    "text": "2.4 Current climate\nAs explanatory variables for the species distribution modeling, we’ll use the 19 bioclimatic variables from Worldclim[1]. Bioclimatic (or bioclim in short) variables are derived from the monthly temperature and rainfall values in order to generate more biologically meaningful variables5. They are available in raster format at different resolutions for historical6 and future conditions. Download the 30 arc seconds7 historical bioclim data (Figure 2.6) and unzip the GeoTiff files to your working directory.\n\n\n\n\n\n\nFigure 2.6: The Worldclim download page for the version 2.1 historical climate data, with the download link for the 30 arc-second bioclim variables.\n\n\n\nWe’ll create a new mapset climate to store the climate data, using the g.mapset module with the -c flag. Next, we use the g.region module to set the boundaries of the region for which we want to import the data. One option is to use the region extent we defined earlier. Or, now that we know more precisely where our species has been observed, we can defined new bounds using the g.region module with the n, s, e, and w parameters. We’ll do the latter, and set the extent of the computational region to 54N, 36S, -10w and 32E.\nNote that, if the region and the raster layer are not perfectly aligned, the area that is imported will be slightly larger than the computational region. How much larger depends on the resolution of the computational region, as explained in (Section A.7). We therefore use the res parameter to set the resolution to match the resolution of the layers we want to import.\n\nStep 2.18  \n\n\n\n\n\ng.mapset -c mapset=climate_current\ng.region n=54 s=36 w=-10 e=32 res=0.00833\n\n\ngs.run_command(\"g.mapset\", flags=\"c\", mapset=\"climate_current\")\ngs.run_command(\"g.region\", n=54, s=36, w=-10, e=32, res=0.0083)\n\n\nType in g.mapset in the console to open the dialog, and use the following parameter settings.\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nName of mapset (mapset)\nclimate_current\n\n\nCreate mapset if it doesn’t exist (c)\n✅\n\n\n\nNext, run the g.region module with the following settings below. Alternatively, you can define the region’s bounds manually in the Map display, as explained in Section A.7.\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nValue for the northern edge (n)\n54\n\n\nValue for the southern edge (s)\n36\n\n\nValue for the eastern edge (e)\n32\n\n\nValue for the western edge (w)\n-10\n\n\n2D grid resolution (res)\n0.00833\n\n\n\n\n\n\nTo import the bioclim data, we use r.in.gdal with the -r flag to tell GRASS to limit the import of the data to the area defined by the computational region that we defined in the previous step. To speed up the import, we use the memory parameter to set the maximum memory to be used (in MB) to be equal to the size of the input file (or otherwise to the maximum amount available on your system).\nTo avoid having to type in the same command 19 times, we can use a small script to automate the process. If you prefer to use the GUI, the r.import functions offers the possibility to batch import layers, as illustrated in the video under the  tab.\n\nStep 2.19  \n\n\n\n\n\n1base=\"wc2.1_30s_bio_\"\n2for n in {1..19}; do\n3    input=\"${base}${n}.tif\"\n4    output=\"bio_${n}\"\n5    r.in.gdal -r input=${input} output=${output} memory=1000\ndone\n\n1\n\nChange the base name of the GeoTiff files if you downloaded the lower resolution data. In the for loop, the base name, number and file extension will be combined to the name of the file to be imported.\n\n2\n\nThe ‘for’ loop will import the layer 1 to 19. Each round, the names of the input and output layers are created and used as input for r.in.gdal.\n\n3\n\nThe number and file extension are added to complete the path+name of the file to be imported.\n\n4\n\nThe name of the imported raster layer is created by combining the base name (bio_) and the number.\n\n5\n\nThe -r flag is set to limit the import to the regional bounds. To speed up the import, the memory parameter should be set to be equal to the size of the import file. If that is not possible, use the maximum value that is possible on your system.\n\n\n\n\n1base = \"wc2.1_30s_bio_\"\n2for n in range(1, 20):\n3    input = f\"{base}{n}.tif\"\n4    output = f\"bio_{n}\"\n    gs.run_command(\n        \"r.in.gdal\",\n5        flags=\"r\",\n        input=input,\n        output=output,\n6        memory=1000,\n    )\n\n1\n\nChange the base name of the GeoTiff files if you downloaded the lower resolution data. In the for loop, the base name, number and file extension will be combined to the name of the file to be imported. Note, the assumption is that you have downloaded the GeoTiff files to your working directory.\n\n2\n\nThe ‘for’ loop will import the layer 1 to 19. Each round, the names of the input and output layers are created and used as input for r.in.gdal.\n\n3\n\nBecause the file names only differ in the last number, we can simply create the name of the input layer by combining the base name (base) and the number (n).\n\n4\n\nSimilar, the name of the imported raster layer is created by combining the base name (bio_) and the number.\n\n5\n\nThe -r flag is set to limit the import to the regional bounds.\n\n6\n\nTo speed up the import, the memory value should be set to be equal to the size of the import file. If that is not possible, use the maximum value that is possible on your system.\n\n\n\n\n\n\n\n\nAs mentioned earlier, GRASS will import raster layers with the resolution and alignment of the original raster files. This means the computational region and the raster layers may not be perfectly aligned. In addition, the region’s resolution may not match that of the imported layers. To check this, you can run g.region with the -g and compare the outcomes with those of the r.info module.\n\nStep 2.20  \n\n\n\n\n\ng.region -p\nr.info map=bio_1\n\n\ngs.run_command(\"g.region\", flags=\"p\")\ngs.run_command(\"r.info\", map=\"bio_1\")\n\n\nType in g.region in the console to open the dialog, and use the following parameter settings.\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nPrint the current region (p)\n✅\n\n\n\nOpen the r.info dialog, and run it with the following parameter settings:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nmap\nbio_1\n\n\n\n\n\n\nTo ensure that the region’s settings (extent, resolution, orientation) match those of the imported bioclim layers, we use the g.region module again, but this time with the raster parameter8. We also use the save parameter to save the region settings in the named region file aoi.\n\nStep 2.21  \n\n\n\n\n\ng.region raster=bio_1 save=aoi\n\n\ngs.run_command(\"g.region raster=bio_1\", save=\"aoi\")\n\n\nOpen the dialog of the g.region module, and run it with the following parameter settings:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nraster\nbio_1\n\n\nSave current region settings in named region file (save)\naoi\n\n\n\n\n\n\nIt is important to note that the geographic region is defined per mapset. It is therefore the computational region of the mapset you are working in that defines the geographic area and resolution in which raster analyses are performed9. See the manual page for more information.\nIn Section 2.2.1 we created the vector layer aoi. To avoid confusion, we will replace it with an updated version that represents the boundaries of the region we just defined. To do this, we must first switch to the PERMANENT mapset, change the computational region using the settings we just saved, and then use v.in.region to create a vector polygon from the region’s extent. Since there is already a vector layer named aoi, we use the –overwrite parameter to overwrite it.\n\nStep 2.22  \n\n\n\n\n\ng.mapset mapset=PERMANENT\ng.region region=aoi@climate_current\nv.in.region output=aoi --overwrite\n\n\ngs.run_command(\"g.mapset\", mapset=\"PERMANENT\")\ngs.run_command(\"g.region \", region=\"aoi@climate_current\")\n1gs.run_command(\"v.in.region\", output=\"aoi\", overwrite=True)\n\n1\n\nOn the command line, --overwrite is used to tell GRASS to overwrite an existing layer. In Python, do do the same, you set the overwrite parameter to True or False\n\n\n\n\nRun the g.mapset module with the following parameter settings:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nmapset\nPERMANENT\n\n\n\nOpen the g.region dialog, and run it with the following settings:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nregion\naoi@climate_current\n\n\n\nRun the v.in.region module with the following parameter settings:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\noutput\naoi\n\n\nAllow output files to overwrite existing files (overwrite)\n✅\n\n\n\n\n\n\nFigure 2.7 shows the bioclimatic maps 1 to 19 that we have just imported for our study area. The maps also show the range map of the Almond-eyed Ringlet butterfly. In case you are interested, see here for the code used to create the maps.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.7: Bioclimatic variable 1 to 19 from Worldclim version 2.0. The red outlines show the distribution of the Almond-eyed Ringlet butterfly according to the IUCN Red List. Click on the image to enlarge and view the maps in detail.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Acquire the data</span>"
    ]
  },
  {
    "objectID": "2_obtaining_data.html#sec-obtainfutureclimatelayers",
    "href": "2_obtaining_data.html#sec-obtainfutureclimatelayers",
    "title": "2  Acquire the data",
    "section": "2.5 Future climates",
    "text": "2.5 Future climates\nTo model the potential distribution of our species under future climate conditions, we use downscaled future climate projections from Worldclim. There are bioclim data from 14 different general circulation models (GCMs) available for download. For each GCM, you can choose between data representing projected conditions in the periods (2021–2040, 241–2060, 2061–2080, and 2081-2100). You can furthermore choose between data based on the Shared Socio-economic Pathways (SSPs) 126, 245, 370 and 585. We’ll use the 30 arc-seconds bioclim variables for the period 2081-2100 based on the SSP585 from the GCM EC-Earth3-Veg. Download the data and save it to your working directory.\nWe’ll create a new mapset climate_EC_Earth3_Veg to store the climate data using the g.mapset module. We use the region parameter to define the computational region. As input, we use the region file aoi@climate_current that we created in step 2.21 10.\n\nStep 2.23  \n\n\n\n\n\ng.mapset -c mapset=climate_EC_Earth3_Veg\ng.region region=aoi@climate_current \n\n\ngs.run_command(\"g.mapset\", flags=\"c\", mapset=\"climate_EC_Earth3_Veg\")\ngs.run_command(\"g.region\", region=\"aoi@climate_current\")\n\n\nType in g.mapset in the console to open the dialog, and use the following parameter settings.\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nName of mapset (mapset)\nclimate_EC_Earth3_Veg\n\n\nCreate mapset if it doesn’t exist (c)\n✅\n\n\n\nNext, run the g.region module with the following settings:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nregion\naoi@climate_current\n\n\n\n\n\n\nUnlike the data for current climate conditions, the data consists of one geoTIF file. The file has 19 bands, representing the 19 bioclimatic variables. To verify this, you can use the gdalinfo function from the command line.\n\n\n\n\ngdalinfo wc2.1_30s_bioc_EC-Earth3-Veg_ssp585_2081-2100.tif \n\n\n\n\n\n\nFigure 2.8: gdalifo prints the names and some basic information about each of the bands in the GeoTiff file. The names of the first two bands are marked with a red outline.\n\n\n\n\n\n\nThe r.in.gdal module conveniently imports each band as a separate layer. The value of the output parameter will be used as prefix for the name of the imported raster layers, followed by the band number. If possible, use the memory parameter to increase the memory to be used by the module as this can speed up the import considerably.\n\nStep 2.24  \n\n\n\n\n\nr.in.gdal -r input=wc2.1_30s_bioc_EC-Earth3-Veg_ssp585_2081-2100.tif \\\noutput=bio memory=1000\n\n\ngs.run_command(\n    \"r.in.gdal\",\n    flags=\"r\",\n    input=\"wc2.1_30s_bioc_EC-Earth3-Veg_ssp585_2081-2100.tif\",\n    output=\"bio\",\n    memory=1000,\n)\n\n\nType in r.in.gdal in the console to open the dialog, and use the following parameter settings.\n\n\n\nParameter\nValue\n\n\n\n\ninput\nwc2.1_30s_bioc_EC-Earth3-Veg_ssp585_2081-2100.tif\n\n\noutput\nbio\n\n\nmemory\n1000\n\n\nLimit import to current region (r)\n✅\n\n\n\n\n\n\nFigure 2.9 shows the projected bioclimatic conditions for 2081-2100 (bioclim variables 1 to 19) that we have just imported for our study area. The maps also show the range map of the Almond-eyed Ringlet butterfly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.9: Projected bioclimatic conditions for the period 2081-2100 based on the SSP585 from the GCM EC-Earth3-Veg. Source: Worldclim version 2.0. The red outlines show the distribution of the Almond-eyed Ringlet butterfly according to the IUCN Red List. Click on the image to enlarge and view the maps in detail.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Acquire the data</span>"
    ]
  },
  {
    "objectID": "2_obtaining_data.html#code-summary",
    "href": "2_obtaining_data.html#code-summary",
    "title": "2  Acquire the data",
    "section": "2.6 Code summary",
    "text": "2.6 Code summary\nThe following code blocks consolidate all the Python code from this chapter. In the first block, the base layers are imported. The second block handles the import of species data, and the third block imports the climate layers. Input and output layer names are now defined as variables at the beginning, allowing users to easily customize them. This setup simplifies automating the workflow.\n\n Base layers Species data Climate data\n\n\n# Define input variables\nworking_directory = \"replace-with-path-to-working-directory\"\nadmin_boundary_file = \"ne_10m_admin_0_countries.shp\"\nurban_areas_file = \"ne_10m_urban_areas.shp\"\nroads_file = \"ne_10m_roads.shp\"\nrivers_centerlines_file = \"ne_10m_rivers_lake_centerlines.shp\"\nrivers_europe_file = \"ne_10m_rivers_europe.shp\"\n\n# Define output layer names\nadmin_layer = \"admin0\"\naoi_layer = \"aoi\"\nclipped_countries_layer = \"countries\"\nurban_layer = \"urban\"\nroads_layer = \"roads\"\nrivers_layer = \"rivers\"\n\n# Set working directory\nimport os\nimport grass.script as gs\n\nos.chdir(working_directory)\n\n# Import national boundaries\ngs.run_command(\n    \"v.in.ogr\",\n    input=admin_boundary_file,\n    output=admin_layer,\n)\n\n# Set the region and create a vector layer 'aoi_layer' for the region boundaries\ngs.run_command(\"g.region\", n=55, s=35, w=-10, e=33)\ngs.run_command(\"v.in.region\", output=aoi_layer)\n\n# Clip the national boundaries layer to the region boundaries\ngs.run_command(\n    \"v.clip\", input=admin_layer, clip=aoi_layer, output=clipped_countries_layer\n)\ngs.run_command(\"g.remove\", flags=\"f\", type=\"vector\", name=admin_layer)\n\n# Import and clip urban areas to the region\ngs.run_command(\n    \"v.in.ogr\",\n    flags=\"r\",\n    input=urban_areas_file,\n    output=\"tmp\",\n)\ngs.run_command(\"v.clip\", input=\"tmp\", clip=aoi_layer, output=urban_layer)\ngs.run_command(\"g.remove\", flags=\"f\", type=\"vector\", name=\"tmp\")\n\n# Import and clip roads to the region\ngs.run_command(\n    \"v.in.ogr\",\n    flags=\"r\",\n    input=roads_file,\n    where=\"featurecla='Road'\",\n    output=\"tmp\",\n)\ngs.run_command(\"v.db.dropcolumn\", map=\"tmp\", columns=\"add\")\ngs.run_command(\"v.clip\", input=\"tmp\", clip=aoi_layer, output=roads_layer)\ngs.run_command(\"g.remove\", flags=\"f\", type=\"vector\", name=\"tmp\")\n\n# Import and clip rivers layers to the region\ngs.run_command(\n    \"v.in.ogr\",\n    flags=\"r\",\n    input=rivers_centerlines_file,\n    output=\"tmp1\",\n)\ngs.run_command(\n    \"v.in.ogr\",\n    flags=\"r\",\n    input=rivers_europe_file,\n    output=\"tmp2\",\n)\ngs.run_command(\"v.clip\", input=\"tmp1\", clip=aoi_layer, output=\"tmp3\")\ngs.run_command(\"v.clip\", input=\"tmp2\", clip=aoi_layer, output=\"tmp4\")\n\n# Prepare and merge the rivers layers into one\ngs.run_command(\n    \"db.execute\",\n    sql=\"ALTER TABLE tmp3 RENAME TO tmp3old;\",\n)\ngs.run_command(\n    \"db.execute\",\n    sql=\"ALTER TABLE tmp4 RENAME TO tmp4old;\",\n)\ngs.run_command(\n    \"db.execute\",\n    sql=(\n        \"CREATE TABLE tmp3 AS \"\n        \"SELECT cat,scalerank,featurecla,name,label,min_zoom,min_label \"\n        \"FROM tmp3old\"\n    ),\n)\ngs.run_command(\n    \"db.execute\",\n    sql=(\n        \"CREATE TABLE tmp4 AS \"\n        \"SELECT cat,scalerank,featurecla,name,label,min_zoom,min_label \"\n        \"FROM tmp4old\"\n    ),\n)\ngs.run_command(\"v.patch\", input=[\"tmp3\", \"tmp4\"], output=rivers_layer)\ngs.run_command(\"db.droptable\", flags=\"f\", table=\"tmp3old\")\ngs.run_command(\"db.droptable\", flags=\"f\", table=\"tmp4old\")\ngs.run_command(\"g.remove\", flags=\"f\", type=\"vector\", pattern=\"tmp*\")\n\n\n# Define input variables\nworking_directory = \"replace-with-path-to-working-directory\"\nspecies_rangemap_file = \"data_0.shp\"\nspecies_occurrence_file = \"0104058-240626123714530.csv\"\n\n# Define output layer names\nspecies_rangemap_layer = \"Erebia_alberganus_rangemap\"\nspecies_obs_layer = \"Erebia_alberganus_obs\"\n\n# Set working directory\nimport os\nimport grass.script as gs\n\nos.chdir(working_directory)\n\n# Create new mapset for species data\ngs.run_command(\"g.mapset\", flags=\"c\", mapset=\"species_data\")\n\n# Import species range map and GBIF occurrence data\ngs.run_command(\n    \"v.in.ogr\",\n    input=species_rangemap_file,\n    output=species_rangemap_layer,\n)\ngs.run_command(\"g.extension\", extension=\"v.in.gbif\")\ngs.run_command(\n    \"v.in.gbif\",\n    input=species_occurrence_file,\n    output=species_obs_layer,\n)\n\n\n# Define input variables\nworking_directory = \"replace-with-path-to-working-directory\"\nfuture_bioclim_file = \"wc2.1_30s_bioc_EC-Earth3-Veg_ssp585_2081-2100.tif\"\nbioclim_base = \"wc2.1_30s_bio_\"\nbioclim_count = 19\n\n# Define output layer names\naoi_layer = \"aoi\"\nclipped_countries_layer = \"countries\"\nrivers_layer = \"rivers\"\nbioclim_layer_prefix = \"bio_\"\nfuture_bioclim_output = \"bio\"\n\n# Set working directory\nimport os\nimport grass.script as gs\n\nos.chdir(working_directory)\n\n# Create new mapset for current climate data and set the region\ngs.run_command(\"g.mapset\", flags=\"c\", mapset=\"climate_current\")\ngs.run_command(\"g.region\", n=54, s=36, w=-10, e=32, res=0.0083)\n\n# Import bioclim layers\nfor n in range(1, bioclim_count + 1):\n    input_file = f\"{bioclim_base}{n}.tif\"\n    output_file = f\"{bioclim_layer_prefix}{n}\"\n    gs.run_command(\n        \"r.in.gdal\",\n        flags=\"r\",\n        input=input_file,\n        output=output_file,\n        memory=1000,\n    )\n\n# Align region to bioclim layers\ngs.run_command(\"g.region raster=bio_1\", save=aoi_layer)\n\n# Update aoi vector layer in PERMANENT mapset\ngs.run_command(\"g.mapset\", mapset=\"PERMANENT\")\ngs.run_command(\"g.region\", region=f\"{aoi_layer}@climate_current\")\ngs.run_command(\"v.in.region\", output=aoi_layer, overwrite=True)\n\n# Create new mapset for future climate data and set region\ngs.run_command(\"g.mapset\", flags=\"c\", mapset=\"climate_EC_Earth3_Veg\")\ngs.run_command(\"g.region\", region=f\"{aoi_layer}@climate_current\")\n\n# Import future bioclim layers\ngs.run_command(\n    \"r.in.gdal\",\n    flags=\"r\",\n    input=future_bioclim_file,\n    output=future_bioclim_output,\n    memory=1000,\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Acquire the data</span>"
    ]
  },
  {
    "objectID": "2_obtaining_data.html#footnotes",
    "href": "2_obtaining_data.html#footnotes",
    "title": "2  Acquire the data",
    "section": "",
    "text": "Note, this will decouple them from the vector layer (geometry).↩︎\nWe use the SQLite CREATE TABLE command to create two new tables with the correct structure, based on a selection of columns from the table we just renamed. Note, this includes the cat column, which is used to link the table attributes to the vector features (see Section A.2.3). The new tables get the name of the original tables.↩︎\nNote that you are required to cite the data when using it in any publication. You can find the citation information on the webpage.↩︎\nIt is good practice to cite the data you use in your publications. GBIF conveniently provides you with the citation on the download page, including a Digital Object Identifier (DOI). This is a link to a web page that describes your search, including any filters you have set. Copy and save the citation, so you can use it in your report. If you are using a reference manager such as Zotero (highly recommended), you can use the provided BibTex or RIS file to import the reference.↩︎\nThe bioclimatic variables represent annual trends (e.g., mean annual temperature, annual precipitation) seasonality (e.g., annual range in temperature and precipitation) and extreme or limiting environmental factors (e.g., temperature of the coldest and warmest month, and precipitation of the wet and dry quarters).↩︎\nThe historical climate data represents climate conditions for the period 1970-2000. For the modeling exercise, they are considered to represent the recent / current climate conditions.↩︎\nYou can download the data at another resolution as well. If you do, make sure to to adapt the relevant code examples accordingly.↩︎\nThe different options of the g.region function provide a lot of flexibility in to how to define the region. You are encouraged to carefully consider the different options each time you use this module.↩︎\nRemember, the current region defines the geographic area in which all GRASS displays and raster analyses will be performed. Raster data will be resampled on-the-fly, if necessary, to match the resolution of the current geographic region setting. By default, GRASS uses the nearest neighbor method to resample the raster layer. This might not always be the best method. If not, you need to resample the data yourself, using the most appropriate method. See menu: Raster → Develop raster map for options.↩︎\nThe aoi region file is not stored in the current mapset but in the climate_current mapset. We can refer to objects (raster and vector maps and region files) stored in another mapset by appending a @ to the name of the object, followed by the name of the mapset. Strictly speaking, this is not always needed (see the manual page for details), but it is good practice as it avoids possible confusion if there are layers with the same name in multiple mapsets.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Acquire the data</span>"
    ]
  },
  {
    "objectID": "3_data_preparation.html",
    "href": "3_data_preparation.html",
    "title": "3  Prepare the data",
    "section": "",
    "text": "3.1 Species occurrences\nUndersampling, on the other hand, is more difficult to address, especially when significant portions of a species’ range are not adequately sampled. Although it does not resolve the issue, comparing GBIF occurrence data with other sources, such as Red List range maps (Figure 2.5), can provide useful insights into the extent of potential undersampling.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prepare the data</span>"
    ]
  },
  {
    "objectID": "3_data_preparation.html#sec-specoc",
    "href": "3_data_preparation.html#sec-specoc",
    "title": "3  Prepare the data",
    "section": "",
    "text": "One fundamental assumption of SDMs is that the entire area of interest has been systematically or randomly sampled [2, 3]. In practice, species occurrence datasets are often biased due to uneven sampling across the study area. For example, some areas may be more accessible (Figure 3.1) or more popular with visitors, resulting in more data from those locations.\nAnother source of bias comes from intensive monitoring in certain areas, resulting in repeated observations in the same space. If you check the attribute table, you’ll see that there are repeated observations by the same observers at certain locations within a few months or years.\nThere are several ways to deal with oversampling, focusing on the presence points (this paragraph) or the background points (next paragraph) [3, 4].\n\n\n\n\n\n\n\n\nFigure 3.1: Examples of different potential sources of bias in the species occurrence data for Erebia alberganus. On the right side of the map, the points are conspicuously clustered along a road, which may indicate sampling bias. On the left, the points form a grid-like pattern, suggesting that the coordinates were recorded with limited decimal precision.\n\n\n\n\n\n\n\n3.1.1 Point densities\nBefore proceeding, let’s check whether our GBIF occurrence dataset shows any signs of bias by analyzing how the density of occurrences varies across the study area. We can do this by counting the number of observations within each grid cell. To accomplish this, we will use the r.vect.stats module, which allows us to perform statistical analysis on raster-vector combinations. Since this is an addon, we will need to install it first.\n\nStep 3.1  \n\n\n\n\n\ng.extension extension=r.vect.stats\n\n\ngs.run_command(\"g.extension\", extension=\"r.vect.stats\")\n\n\nIn the menu, select Settings → Addon extensions → Install extension from addons [g.extension]. This opens a new window (see below).\n\n\n\n\n\n\nFigure 3.2: Use the search bar to look for r.vect.stats and hit Enter\n\n\n\n\n\n\nWe want to create the layer in the Erebia_alberganus mapset, so we need to switch to that mapset first.\n\nStep 3.2  \n\n\n\n\n\ng.mapset mapset=species_data\n\n\ngs.run_command(\"g.mapset\", mapset=\"species_data\")\n\n\n\n\n\n\n\n\nFigure 3.3: To change the mapset, right click on the name of the mapset in the Data panel. In the context menu, select Switch mapset.\n\n\n\n\n\n\nThe extent and resolution of the resulting raster is determined by the bounding box of the mapset. So we set the region’s extent to match the extent of the occurrence point layer. We also set the region’s resolution to match that of the bioclim layers, which is 0.008333 arc degrees.\n\nStep 3.3  \n\n\n\n\n\n1g.region -a res=0.008333 vector=Erebia_alberganus_obs\n\n1\n\nSetting the -a flag means that the function will adjust the boundaries of the extent if necessary to ensure that the region is perfectly aligned with the resolution.\n\n\n\n\ngs.run_command(\n    \"g.region\",\n1    flags=\"a\",\n    res=0.008333,\n    vector=\"Erebia_alberganus_obs\",\n)\n\n1\n\nSetting the -a flag means that the function will adjust the boundaries of the extent if necessary to ensure that the region is perfectly aligned with the resolution.\n\n\n\n\nTo change the region settings, open the g.region dialog, and fill in the following parameters:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nres\n0.008333\n\n\nvector\nErebia_alberganus_obs\n\n\nAlign region to resolution (a)\n✅\n\n\n\nNote that we use -a flag to tell g.region to ensure that the region is perfectly aligned with the resolution. To this end, the g.region function will adjust the region’s bound, if needed.\n\n\n\nWe can now run the r.vect.stats module. With the method parameter, we can tell the module what statistic to compute based on vector points over a raster grid. We use method=n to calculate the number of points per grid cell. The n stands for count.\n\nStep 3.4  \n\n\n\n\n\nr.vect.stats input=Erebia_alberganus_obs output=pointdensities method=n\n\n\ngs.run_command(\n    \"r.vect.stats\",\n    input=\"Erebia_alberganus_obs\",\n    output=\"pointdensities\",\n    method=\"n\",\n)\n\n\nTo calculate the point densities per grid cell, open the r.vect.stats dialog, and use the following parameter settings.\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\ninput\nErebia_alberganus_obs\n\n\noutput\npointdensities\n\n\nmethod\nn\n\n\n\n\n\n\nAfter completing these steps, we have a raster layer (pointdensities) with the number of occurrences per cell. Remember that the extent and resolution of the raster cell is based on the extent of the point data and the resolution of the bioclim layers (step 3.3). In the next step, we compute some statistics. To exclude cells with 0 (zero) occurrences, we convert them to NULL using the r.null function. This ensures that only the cells where the species was observed (non-zero values) will be considered in subsequent steps.\n\nStep 3.5  \n\n\n\n\n\nr.null map=pointdensities setnull=0\n\n\ngs.run_command(\"r.null\", map=\"pointdensities\", setnull=0)\n\n\nOpen the r.null dialog, and use the following parameter settings.\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nmap\npointdensities\n\n\nsetnull\n0\n\n\n\n\n\n\nWe use r.univar to compute the range and median of number of observations per grid cell and the r.boxplot addon to visualize the distribution of point densities and to identify possible outliers.\n\nStep 3.6  \n\n\n\n\n\n# Compute raster statistics\n1r.univar -e map=pointdensities\n\n# Install and run r.boxplot\ng.extension extension=r.boxplot\nr.boxplot -o -h input=pointdensities\n\n1\n\nThe -e flag tells r.univar to calculate extended statistics, like the median.\n\n\n\n\n# Compute raster statistics\n1gs.run_command(\"r.univar\", flags=\"e\", map=\"pointdensities\")\n\n# Install and run r.boxplot\ngs.run_command(\"g.extension\", extension=\"r.boxplot\")\ngs.run_command(\"r.boxplot\", flags=\"o\", input=\"pointdensities\")\n\n1\n\nThe -e flag tells r.univar to calculate extended statistics, like the median.\n\n\n\n\nOpen the r.univar dialog, and use the following parameter settings. The -e flag tells r.univar to calculate extended statistics, like the median.\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nmap\npointdensities\n\n\nCalculate extended statistics (e)\n✅\n\n\n\nInstall the r.boxplot and run it with the following parameter settings.\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nInput\npointdensities\n\n\nInclude outliers (o)\n✅\n\n\n\nNote that the r.boxplot offers various options to format the plot.\n\n\n\nThe results of r.univar show that the number of observations per raster cell range from 1 to 240, with a median of 1, and an average of 4.7. These results and the boxplot in Figure 3.4 show that even though within the majority of non-NULL raster cells there is only one observation, there are also raster cells with many more observations, up to 240 point observations.\n\n\n\n\n\n\nFigure 3.4: The boxplot shows the range, quartiles, outliers of the point densities.\n\n\n\nIf we look at the attribute table, we’ll see that where there are many observations, they often span several years and often involve the same observer. This suggests that certain areas have been surveyed more extensively than others, potentially leading to uneven sampling effort across the species’ range.\n\n\n3.1.2 Spatially thinning\nAs mentioned earlier, we wanted to check whether our GBIF occurrence dataset shows any signs of bias. The results suggest that bias may be present. A simple way to reduce the risk of spatial autocorrelation caused by oversampling is through spatial thinning or sub-sampling the data [5]. This method involves removing observation points from the training dataset to ensure a specified minimum distance between points or to limit the maximum density of points at each location.\nThere are different approaches to spatially thin a point layers. One way is to convert the vector point layer with occurrences to a raster layer using v.to.rast. Next, the raster layer is converted back to a vector point layer using r.to.vect. The result is a point layer with a point in the center of each raster cell with occurrences, as illustrated in Figure 3.5. The density and number of points in the output layer depend on the region’s resolution. This means that we can vary the point density by changing the region’s resolution first 1.\n\n\n\n\n\n\nFigure 3.5: Reduce number of sample points to a maximum of 1 point per raster cell.\n\n\n\nFor this tutorial, we’ll use the thinning option in the module v.maxent.swd. The module follows the same approach as described above. We’ll use this module later in this tutorial (Section 3.4) to prepare the input data for the Maxent analysis, so at this point, we don’t need to do anything.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prepare the data</span>"
    ]
  },
  {
    "objectID": "3_data_preparation.html#sec-studyarea",
    "href": "3_data_preparation.html#sec-studyarea",
    "title": "3  Prepare the data",
    "section": "3.2 Background points",
    "text": "3.2 Background points\nMaxent uses background points to characterize the environmental conditions of the study area. This is done by first selecting a large number of locations within the study area (green dots in Figure 3.6). The environmental conditions for these locations are then determined. These values represent the “environmental space” of the study area. Maxent compares these with the environmental conditions under which the species was observed (grey dots in Figure 3.6).\n\n\n\n\n\n\nFigure 3.6: Left: A study area with species observations marked by dark gray dots and randomly selected background points marked in green. Right: Scatter and density plots depict how rainfall and temperature are distributed across both the background points (green) and species observations (dark gray). The species prefers areas with higher rainfall, while there is no apparent preference for temperature.\n\n\n\n\n3.2.1 Study area\nOne of the decisions we need to make is about the boundaries of our study area. We can draw a bounding box or use more complex shapes representing specific boundaries.\n\n\n\n\n\n\nImportance of the size and shape of the study area\n\n\n\n\n\nAn important decision in species distribution modeling is the size and shape of the study area [6]. The environmental gradients will be larger in a larger study area. If the study area is very large, the background points may capture environmental conditions that are mostly far outside the possible range of suitable conditions. This can lead to an underestimation of the influence of variables operating at smaller spatial scales. It also artificially inflates performance metrics (such as AUC) by making the prediction of unsuitable areas appear easier than it is.\nOn the other hand, if the study area is largely limited to the areas where the species is observed, the model is likely to discriminate between presence and background points based on subtle differences that may not be ecologically meaningful. And if the study area does not include all areas where the species occurs, important environmental gradients may be missed, potentially underestimating the full range of suitable conditions for the species.\nThe size of the study area can also affect how well model evaluation metrics (e.g., AUC) reflect true model performance. In a large study area with many unsuitable environments, the model may appear to perform well because the contrast between suitable and unsuitable areas is greater. In smaller areas with less contrast, the predictive power of the model may be harder to detect.\nAn important consideration is the area that has been accessible to the species of interest over relevant time periods. This is the ideal area for model development, testing and comparison [7]. In general, this will not be easy to determine as it will depend on, for example, time, type of environment, and presence of barriers.\n\nHow to: Depending on how we want to select the background points, we can delimit our study area using the computational region, a MASK or using polygons. See below.\n\n\n\n\nWe’ll create background points within the boundaries of the area of interest (aoi) that we defined in Section 2.4. To store the vector layer with background points for this particular aoi in a separate mapset, we first use the g.mapset module to create a new mapset dataset01. Next, we use the g.region module to set the computational region, based on the region file we created earlier (step 2.21), and the r.mask module with the vector parameter to further restrict our study area to land areas. Finally, to avoid having to append the name of that mapset to a layers name, each time we need a layer from a different mapset, we give access to the other relevant mapsets using the g.mapsets command 2.\n\nStep 3.7  \n\n\n\n\n\n# Create a new mapset\ng.mapset -c mapset=dataset01\n\n# Define the region\ng.region region=aoi@climate_current\n\n# Create a MASK\nr.mask vector=countries@PERMANENT\n\n# Give access to layers in other mapsets\ng.mapsets mapset=climate_current,Erebia_alberganus operation=add\n\n\n# Create a new mapset and make it the active mapset\ngs.run_command(\"g.mapset\", flags=\"c\", mapset=\"dataset01\")\n\n# Set the region to match the region settings in the aoi region file\ngs.run_command(\"g.region\", region=\"aoi@climate_current\")\n\n# Create a MASK\ngs.run_command(\"r.mask\", vector=\"countries@PERMANENT\")\n\n# Give access to layers in other mapsets\ngs.run_command(\n    \"g.mapsets\", mapset=[\"climate_current\", \"Erebia_alberganus\"], operation=\"add\"\n)\n\n\n\nType in g.mapset in the console to open the dialog, and use the following parameter settings.\n\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nName of mapset (mapset)\ndataset01\n\n\nCreate mapset if it doesn’t exist (c)\n✅\n\n\n\n\n\nNext, run the g.region module with the following settings:\n\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nregion\naoi@climate_current\n\n\n\n\n\nCreate a MASK of the European land areas using r.mask:\n\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nvector\ncountries@PERMANENT\n\n\n\n\n\nGive access to layers in the other mapsets.\n\nIn the menu, go to Settings → GRASS working environment → Mapset access and select the mapsets climate_current and Erebia_alberganus.\n\n\n\nWe have now a new raster layer named MASK in the mapset dataset01. We can use r.info to check that it has the same extent and resolution as the computational region (Figure 3.7). The raster cells falling outside the MASK are not included in operations like raster algebra, statistics, or interpolation.\n\n\n\n\n\n\nFigure 3.7: Red outline: the bounds of the computational region. Green: the MASK, which covers the study area.\n\n\n\n\n\n3.2.2 Background points\nNow that we have defined the study area, we need to create a vector layer with background points. These will be used to characterize environmental conditions across the study area. The model will compare them with the conditions where the species is present.\n\n\n\n\n\n\nBackground vs pseudo-absence points\n\n\n\n\n\nBackground points are used to characterize the environmental conditions in the study area. They differ from absence points, which explicitly indicate locations where the species was surveyed but not found [2, e.g. 8]. Together with presence data, they can be used to estimate the conditions under which a species is more likely to occur than on average. Background points are commonly selected at random across the study area. However, alternative methods may yield better results [9–11].\nA closely related but different concept is that of pseudo-absences [12]. Pseudo-absence are locations where the species is assumed to be absent, even though no actual survey data confirms this. They are often generated randomly or based on specific criteria to contrast them with presence data (e.g., environmental dissimilarity to presence points). For example, one may select at random sample points throughout the region, excluding areas within a certain distance from presence points. Or sample points may be selected at places unlikely to be suitable for the species.\nThe advantage of background points over pseudo-absence points is that it requires fewer assumptions and therefore is less prone to bias. And methods such as Maxent can explicitly deal with the overlap between presence and background points [13]. We will therefore use background points.\n\n\n\nThere are different ways to create background points. To get a complete representation of the study area, we can convert the MASK layer to a vector point layer using the r.to.vect module. However, in our example, this will create a very large point layer with over 7 million locations, which could significantly slow down the modeling process and may well exceed Maxent’s capacity to handle such a large dataset. Instead, we create a point layer with randomly selected sample points to represent the environmental conditions in the selected study area.\n\n\n\n\n\n\nCreating sample background points\n\n\n\n\n\nThere are several ways to create background points. One option is to use the v.maxent.swd module, which we will use later in this tutorial to prepare the input data for the Maxent model. This module includes an option to generate random background points within the region’s bounds and MASK.\nFor more flexibility, we can create random points in a separate step using for example:\n\nThe r.random module creates a vector or raster point layer with randomly selected point locations within the current computational region bounds. If there is a MASK, points will only be generated within the masked area.\nThe v.random module randomly generates vector points within the current region. If an restrict vector map is specified with one or more areas (polygons), the location of random points is confined to those areas. By default, the requested number of points are distributed across all areas, but by using the -a flag, the requested number of points is generated for each individual area, thus allowing for stratified sampling (see examples in the manual page).\nIf you want the random points to be at least a certain distance apart, you can use the r.random.cells module. It generates a random set of raster cells that are at least a certain distance apart. Again, if a MASK is present, random cells will not be generated in masked areas.\nWith the r.random.weight module, you can vary the point density across the region, based on the values of a bias layer. This makes it possible to correct for the effect of sampling bias by creating a background point layer with the same bias as the presence locations [2, 5, 14].\n\n\n\n\nWe’ll use the r.random function to generate a point layer background_points with random points within the region’s bounds and MASK. The main choice we have to make is the number of sample points. The number of sample points often used is on the order of 10,000, which is what we will use. Note, however, that for larger, more heterogeneous areas, you may need more background points to ensure that all important environmental conditions are well represented [15].\n\nStep 3.8  \n\n\n\n\n\nr.random input=MASK npoints=10000 vector=background_points seed=5\n\n\ngs.run_command(\n    \"r.random\", input=\"MASK\", npoints=10000, vector=\"background_points\", seed=5\n)\n\n\nOpen the r.random, and use the following parameter settings.\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\ninput\nMASK\n\n\nnpoints\n10000\n\n\nvector\nbackground_points\n\n\nseed\n5",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prepare the data</span>"
    ]
  },
  {
    "objectID": "3_data_preparation.html#sec-multicollinearity",
    "href": "3_data_preparation.html#sec-multicollinearity",
    "title": "3  Prepare the data",
    "section": "3.3 Multicollinearity",
    "text": "3.3 Multicollinearity\nMulticollinearity occurs when two or more explanatory variables are highly correlated. This can make it difficult to determine the individual effect of each variable on the species distribution, and redundant variables may add noise to the model without improving its predictive power. Maxent, a commonly used SDM tool, is relatively insensitive to multicollinearity because it selects the most informative variables during model training. Yet, excluding highly redundant variables might still help to get a more parsimonious, robust and interpretable model. And on a practical note, it will reduce the time it takes to train the model.\n\n\n\n\n\n\nFigure 3.8: The bioclim 6 and bioclim 11 variables are highly correlated (r=0.98). This begs the question of how useful it is to use both as predictor variables in the model. You can explore the relation between any two variables yourself using the bivariate scatterplot tool. Click on the  in the toolbar of the map display.\n\n\n\nThe variance inflation factor (VIF) is a commonly used method to test for collinearity between predictor variables [16]. In GRASS GIS, the VIF can be calculated for a set of variables using the add-on r.vif. This addon also allows you to select a subset of variables using a stepwise variable selection procedure. In this procedure, the VIF is computed repeatedly. Each time, the variable with the highest VIF is removed until the highest VIF values are less than a user-defined threshold [17].\nWe’ll use the stepwise VIF procedure to select the minimum set of bioclimatic variables with a VIF &lt; 10. We use n=100,000 so that the VIF is performed based on the values of 100,000 random locations. This speeds up the calculation and avoids memory problems.\n\nStep 3.9  \n\n\n\n\n\n# Install the r.vif addon\n1g.extension extension=r.vif\n\n# Select the input variable\n2layers=`g.list -m type=raster pattern=bio* sep=\",\" mapset=climate_current`\n3r.vif maps=$layers seed=5 n=100000 maxvif=10\n\n1\n\nInstalls the r.vif add-on in GRASS GIS if it is not already installed.\n\n2\n\nThis line creates a comma separated list of raster layers in the mapset ‘climate_current’ whose names start with ‘bio’. The resulting list is stored in the variable ‘layers’. With the -m flag, the mapset is included in the layers names.\n\n3\n\nmaps=$layers: Uses the list of raster layers stored in the ‘layers’ variable as input for the VIF analysis. You can, alternatively, skip the previous line, and type in the names of each of the layers.\n\n\n\n\n# Install the r.vif addon\n1gs.run_command(\"g.extension\", extension=\"r.vif\")\n\n# Select the input variable\n2layers = gs.list_strings(pattern=\"bio*\", mapset=\"climate_current\", type=\"raster\")\n3gs.run_command(\"r.vif\", maps=layers, seed=5, n=100000, maxvif=10)\n\n1\n\nInstalls the r.vif add-on in GRASS GIS if it is not already installed.\n\n2\n\nThe list_strings module provides a convenient wrapper to the g.list. The parameters are the same as for the g.list.\n\n3\n\nThe seed parameter ensures that the random processes are reproducible. You can set any number.\n\n\n\n\nIn the menu, go to settings → addon extensions → install extensions from addons, search for r.vif and install it.\nNow, we run the r.vif. Make sure to fill in the names of all 19 bioclim variables under the maps parameter.\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nmaps\nbio_1,bio_2,bio_3 … bio_17,bio_18,bio_19\n\n\nn\n100000\n\n\nmaxvif\n10\n\n\n\n\n\n\nThe selected bioclimatic variables in my case are bio_1, bio_2, bio_4, bio_8, bio_9, bio_13, bio_14, bio_15 and bio_19. Note that there is a (small) change that your result will show a different combination of variables. This is because the VIF was calculated based on the values of 100,000 randomly selected point locations. Be sure to write down the names of the variables. We’ll need them later.\n\n\n\n\n\n\nThe VIF algorithm offers a data-driven method for selecting variables. However, it’s important to also consider ecological significance when making selections. For instance, if a species is sensitive to temperatures during the coldest quarter, this variable should be kept regardless of its VIF score. The r.vif module includes a retain parameter that allows you to specify one or more variables to retain during the stepwise selection process. If a retained variable has the highest VIF, the variable with the next highest VIF will be removed instead.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prepare the data</span>"
    ]
  },
  {
    "objectID": "3_data_preparation.html#sec-maxentswd",
    "href": "3_data_preparation.html#sec-maxentswd",
    "title": "3  Prepare the data",
    "section": "3.4 Export data",
    "text": "3.4 Export data\nThe last step before we, finally, can start with the actual modelling, is to prepare and export the input dataset for the Maxent model. We’ll use the module v.maxent.swd for this. The module exports the different layers in the right format for Maxent. The presence and background point layers are exported as so-called swd files. Raster layers are exported as ascii files.\nFirst step is to create a new folder in your working directory. Let’s call it dataset01. And in that folder, we create a sub-folder called envdat. We’ll export the swd files to the first folder, and the ascii raster files to the second folder using v.maxent.swd. Note, this make take some time, so be patient.\n\nStep 3.10  \n\n\n\n\n\n# Create a new folder in the working directory\nmkdir dataset01\nmkdir dataset01/envdat\n\n# Export the Maxent input data\n1v.maxent.swd -t species=Erebia_alberganus_obs \\\n2  bgp=background_points \\\n3  evp_maps=bio_1,bio_13,bio_14,bio_15,bio_19,bio_2,bio_4,bio_8,bio_9 \\\n4  species_output=dataset01/species.swd \\\n5  bgr_output=dataset01/background_points.swd \\\n6  export_rasters=dataset01/envdat\n\n1\n\nspecies: point layer with occurrences\n\n2\n\nbgp: point layer with background points. If you don’t have one, you can use the nbgp to generate a user-defined number of background points, respecting the computational region and MASK.\n\n3\n\nThe environmental raster layers that you want to include in your model as explanatory variables. Importantly, these should be continuous variables. Use the evp_cat for categorical variables (e.g., land use map).\n\n4\n\nThe location and file name of the species swd file. Note that if you save the file in a sub-folder of your working directory, you can use the relative path.\n\n5\n\nThe location and file name of the background swd file. Note that if you save the file in a sub-folder of your working directory, you can use the relative path.\n\n6\n\nThe location of the environmental raster layers. Note that if you save them to a sub-folder of your working directory, you can use the relative path.\n\n\n\n\n# Create a new folder in the working directory\n1os.makedirs(\"dataset01/envdat\", exist_ok=True)\n\n# Export the Maxent input data\ngs.run_command(\n    \"v.maxent.swd\",\n    flags=\"t\",\n2    species=\"Erebia_alberganus_obs\",\n3    bgp=\"background_points\",\n4    evp_maps=\"bio_1,bio_13,bio_14,bio_15,bio_19,bio_2,bio_4,bio_8,bio_9\",\n5    species_output=\"dataset01/species.swd\",\n6    bgr_output=\"dataset01/background_points.swd\",\n7    export_rasters=\"dataset01/envdat\",\n)\n\n1\n\nThe os.makedirs creates directories recursively.\n\n2\n\nspecies: point layer with occurrences\n\n3\n\nbgp: point layer with background points. If you don’t have one, you can use the nbgp to generate a user-defined number of background points, respecting the computational region and MASK.\n\n4\n\nThe environmental raster layers that you want to include in your model as explanatory variables. Importantly, these should be continuous variables. Use the evp_cat for categorical variables (e.g., land use map).\n\n5\n\nThe location and file name of the species swd file. Note that if you save the file in a sub-folder of your working directory, you can use the relative path.\n\n6\n\nThe location and file name of the backgroundpoints swd file. Note that if you save the file in a sub-folder of your working directory, you can use the relative path.\n\n7\n\nThe location of the environmental raster layers. Note that if you save them to a sub-folder of your working directory, you can use the relative path.\n\n\n\n\nCreate the folder dataset01 and sub-folder envdat using your favorite file browser. Next, open the v.maxent.swd dialog and run it with the following parameter settings:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nspecies 3\nErebia_alberganus_obs\n\n\nbgp 4\nbackground_points\n\n\nevp_maps 5\nbio_1,bio_2,bio_4,bio_8,bio_9,bio_13,bio_14,bio_15\n\n\nspecies_output 6\ndataset01/species.swd\n\n\nbgr_output 7\ndataset01/background_points.swd\n\n\nexport_rasters 8\ndataset01/envdat\n\n\nThin species and background points (t) 9\n✅",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prepare the data</span>"
    ]
  },
  {
    "objectID": "3_data_preparation.html#code-summary",
    "href": "3_data_preparation.html#code-summary",
    "title": "3  Prepare the data",
    "section": "3.5 Code summary",
    "text": "3.5 Code summary\nThe Python code blocks below combines all steps from this chapter. It mostly is a copy-paste of the code on this page, but note that step 3.9 and step 3.10 are combined, so the whole code can be ran without intermediate user input. And instead of creating a random background point layer in a separate step (step 3.8), we let the v.maxent.swd do this for us by setting the nbgp parameter. Input and output layer names are now defined as variables at the beginning, allowing users to easily customize them.\n\n Examine the data Create maxent input data\n\n\n# Define input variables\nworking_directory = \"replace-with-path-to-working-directory\"\nspecies_mapset = \"species_data\"\nspecies_vector = \"Erebia_alberganus_obs\"\naoi_mapset = \"dataset01\"\nresolution = 0.008333\n\n# Define output variables\ndensity_output = \"pointdensities\"\n\n# Import necessary libraries\nimport grass.script as gs\nimport os\n\n# Set the working directory\nos.chdir(working_directory)\n\n# Switch to the mapset containing species data\ngs.run_command(\"g.mapset\", mapset=species_mapset)\n\n# Set the region to match the extent and resolution of the species observations\ngs.run_command(\n    \"g.region\",\n    flags=\"a\",\n    res=resolution,\n    vector=species_vector,\n)\n\n# Generate a density map from the species vector data\ngs.run_command(\n    \"r.vect.stats\",\n    input=species_vector,\n    output=density_output,\n    method=\"n\",  # Count points in each cell\n)\ngs.run_command(\"r.null\", map=density_output, setnull=0)\n\n# Calculate statistics and create a boxplot\ngs.run_command(\"r.univar\", flags=\"e\", map=density_output) \ngs.run_command(\"r.boxplot\", flags=\"o\", input=density_output) \n\n\n# Define input variables\nworking_directory = \"replace-with-path-to-working-directory\"\nspecies_mapset = \"species_data\"\nspecies_vector = \"Erebia_alberganus_obs\"\naoi_mapset = \"dataset01\"\nclimate_mapset = \"climate_current\"\ncountry_vector = \"countries\"\nselected_layers_pattern = \"bio*\"\nresolution = 0.008333\nvif_seed = 5\nvif_n = 100000\nvif_max = 10\nnbgp = 10000\n\n# Define output variables\nswd_species_output = \"dataset01/Erebia_alberganus_obs.swd\"\nswd_background_output = \"dataset01/background_points.swd\"\nswd_export_rasters_dir = \"dataset01/envdat\"\n\n# Import necessary libraries\nimport grass.script as gs\nimport os\n\n# Set the working directory\nos.chdir(working_directory)\n\n# Create a directory for exporting environmental data\nos.makedirs(swd_export_rasters_dir, exist_ok=True)\n\n# Create a new mapset for further analysis\n# and add access to necessary mapsets\ngs.run_command(\"g.mapset\", flags=\"c\", mapset=aoi_mapset)\ngs.run_command(\"g.mapsets\", mapset=[climate_mapset, species_mapset], operation=\"add\")\n\n# Set the region to match the area of interest and apply a mask for analysis\ngs.run_command(\"g.region\", region=f\"aoi@{climate_mapset}\")\ngs.run_command(\"r.mask\", vector=country_vector)\n\n# Identify relevant climate layers using VIF (Variance Inflation Factor)\n# to reduce multicollinearity\nlayers = gs.list_strings(\n    pattern=selected_layers_pattern,\n    mapset=climate_mapset,\n    type=\"raster\",\n)\nselected_layers = gs.read_command(\n1    \"r.vif\", flags=\"v\", maps=layers, seed=vif_seed, n=vif_n, maxvif=vif_max\n2).strip()\n\n# Export data for Maxent input, including species presence and background data\ngs.run_command(\n    \"v.maxent.swd\",\n    flags=\"t\",\n    species=species_vector,\n3    nbgp=nbgp,\n    evp_maps=selected_layers,\n    species_output=swd_species_output,\n    bgr_output=swd_background_output,\n    export_rasters=swd_export_rasters_dir,\n)\n\n1\n\nThe -v results in an list of the finally selected layers.\n\n2\n\nThe output of the r.vif function call is a string that ends with a newline character. The .strip() function removes that new line character.\n\n3\n\nInstead of creating a random background point layer and using this as input, we use the nbgp, so the module will create the set of background points within the region’s bounds and MASK.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prepare the data</span>"
    ]
  },
  {
    "objectID": "3_data_preparation.html#footnotes",
    "href": "3_data_preparation.html#footnotes",
    "title": "3  Prepare the data",
    "section": "",
    "text": "For other options to spatially thin a point layer, check out the module v.decimate.↩︎\nTo state the obvious, be careful when using the same layer names across different mapsets.↩︎\nspecies: point layer with occurrences↩︎\nbgp: point layer with background points. If you don’t have one, you can use the nbgp to generate a user-defined number of background points, respecting the computational region and MASK.↩︎\nevp_maps: The environmental raster layers that you want to include in your model as explanatory variables. Importantly, these should be continuous variables. Use the evp_cat for categorical variables (e.g., land use map).↩︎\nspecies_output]{.style-parameter}: The location and file name of the species swd file. Note that if you save the file in a sub-folder of your working directory, you can use the relative path.↩︎\nbgr_output: The location and file name of the backgroundpoints swd file. Note that if you save the file in a sub-folder of your working directory, you can use the relative path.↩︎\nexport_rasters: The location of the environmental raster layers. Note that if you save them to a sub-folder of your working directory, you can use the relative path.↩︎\n-t: Select this flag if you want to limit the species and background points to maximum one point per raster cell.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prepare the data</span>"
    ]
  },
  {
    "objectID": "4_model_training.html",
    "href": "4_model_training.html",
    "title": "4  Model training",
    "section": "",
    "text": "4.1 Organize outputs\nIn this chapter, we’ll use the r.maxent.train module to create several species distribution models based on different input data and parameter settings1. Each time we train a model, the module creates a number of output files and GRASS output layers. To keep things organized, we’ll create a separate sub-folder in our working directory for each model we create. Similarly, we will create a new mapset in our GRASS database for each new model. You may have your own way of organizing your data. That is fine, of course, but organize your data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model training</span>"
    ]
  },
  {
    "objectID": "4_model_training.html#sec-4organizeoutputs",
    "href": "4_model_training.html#sec-4organizeoutputs",
    "title": "4  Model training",
    "section": "",
    "text": "Step 4.1  \n\n\n\n\n\n# Folders to store data\nmkdir model_01\n\n# Create a new mapset and switch to it\ng.mapset -c mapset=model_01\n\n# Define the region and set the MASK\n1g.region raster=bio_1@climate_current\n\n1\n\nTo make sure the output raster aligns with the input environmental variables, we use the raster parameter this time.\n\n\n\n\n# Set working directory and create a new folder in the working directory\nos.chdir(\"replace-for-path-to-working-directory\")\nos.makedirs(\"model_01\", exist_ok=True)\n\n# Create a new mapset and switch to it\ngs.run_command(\"g.mapset\", flags=\"c\", mapset=\"model_01\")\n\n# Set the region and create a MASK\n1gs.run_command(\"g.region\", raster=\"bio_1@climate_current\")\n\n1\n\nTo make sure the output raster aligns with the input environmental variables, we use the raster parameter this time.\n\n\n\n\nCreate the folder model_01 in your working directory using your favorite file browser. Next, create a new mapset and switch to this mapset using the Data panel. Alternatively, open the g.mapsets dialog and run it with the following parameter settings:\n\n\n\nParameter\nValue\n\n\n\n\nName of mapset (mapset)\ndataset01\n\n\nCreate mapset if it doesn’t exist (c)\n✅\n\n\n\nNext, use the g.region module to set the computational region style parameter, based on the region file we created earlier (step 2.21).\n\n\n\nParameter\nValue\n\n\n\n\nraster2\nbio_1@climate_current",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model training</span>"
    ]
  },
  {
    "objectID": "4_model_training.html#sec-4trainthemodel",
    "href": "4_model_training.html#sec-4trainthemodel",
    "title": "4  Model training",
    "section": "4.2 Train the model",
    "text": "4.2 Train the model\nMaxEnt offers a variety of training, validation, and output options. For now, we can rely on the default settings for most of these parameters. The minimum required inputs are the files containing species location data and background points. These are the SWD files created in step 3.10. And we need to specify the path to the folder where MaxEnt will save the results.\nWe’ll also use a few additional parameters or flags. Click on the numbers in the code block for a description of these parameters. We’ll go into more detail in the next section when we examine the results. For additional information, see the manual page of the module.\n\nStep 4.2  \n\n\n\n\n\nr.maxent.train \\\n1  samplesfile=dataset01/Erebia_alberganus_obs.swd \\\n2  environmentallayersfile=dataset01/background_points.swd \\\n3  projectionlayers=dataset01/envdat  \\\n4  outputdirectory=model_01 \\\n5  samplepredictions=E_alberganus_samplepred \\\n6  backgroundpredictions=E_alberganus_bgrdpred \\\n7  predictionlayer=E_alberganus_probability \\\n8  maxent=maxent.jar \\\n9  threads=4 memory=1000 \\\n10  -ybgi\n\n1\n\nThe (relative) path to the species swd file.\n\n2\n\nThe (relative) path to the background swd file.\n\n3\n\nThe (relative) path to the folder that holds the ascii raster layers of the input environmental variables.\n\n4\n\nThe (relative) path to the folder that where Maxent should write the results.\n\n5\n\nThe module will create a point layer with the locations of species occurrences used for the model. The attribute table contains the predicted probability scores. With this parameter, you can specify a custom name for this point layer. If left empty, a default name will be used, based on the species name, with as suffix samplePredictions\n\n6\n\nThe module will create a vector layer with background points used as input for this model. The attribute table contains the predicted probability scores. With this parameter, you can specify a custom name for the layer. If left empty, a default name will be used, based on the species name, with as suffix backgroundPredictions.\n\n7\n\nIf the projectionlayers is set, a raster prediction layer will be created that reflects the potential distribution based on the projection layers. With the parameter predictionlayer, you can set the name of this output layers. If left empty, a default name will be used, based on the species name, with as suffix the name of the folder with the environmental raster layers. In this case, that would have been Erebia_alberganus_obs_envdat.\n\n8\n\nSince this is the first time we are running this module, we use the maxent parameter to set the path to the Maxent.jar file, and the -i flag to copy the jar file to the add-on directory where GRASS will automatically find it on subsequent runs. If you don’t know where to find this file, go back to Section 1.2.3. Note, here, the maxent.jar file is assumed to be in the working directory. If not, provide the full path to the file.\n\n9\n\nTo improve the performance, you can increase the number of threads and allocate more memory. However, before making adjustments, check your system specifications and adjust the settings based on your system’s capabilities.\n\n10\n\nThe first three flags will create extra output that helps you to evaluate the model: y: Create a point feature layer with for each occurrence point the predicted probability score. This can be useful to identify discrepancies between the observed and predicted distribution. b: Create a vector point layer with predicted probability scores at the background point locations. This is useful to identify discrepancies between the observed and predicted distribution. g: Create response curves, which visualize the (marginal) effect of each explanatory variable on the predicted probability. With the i flag, you copy the maxent.jar file to the addon directory, so GRASS can find it next times without you having to provide the path. I.e., you only need to set this flag the first time you use this module.\n\n\n\n\ngs.run_command(\n    \"r.maxent.train\",\n1    samplesfile=\"dataset01/Erebia_alberganus_obs.swd\",\n2    environmentallayersfile=\"dataset01/background_points.swd\",\n3    projectionlayers=\"dataset01/envdat\",\n4    outputdirectory=\"model_01\",\n5    samplepredictions=\"E_alberganus_samplepred\",\n6    backgroundpredictions=\"E_alberganus_bgrdpred\",\n7    predictionlayer=\"E_alberganus_probability\",\n8    maxent=\"maxent.jar\",\n9    threads=4,\n10    memory=1000,\n11    flags=\"ybgi\",\n)  \n\n1\n\nThe (relative) path to the species swd file.\n\n2\n\nThe (relative) path to the background swd file.\n\n3\n\nThe (relative) path to the folder that holds the ascii raster layers of the input environmental variables.\n\n4\n\nThe (relative) path to the folder that where Maxent should write the results.\n\n5\n\nThe module will create a point layer with the locations of species occurrences used for the model. The attribute table contains the predicted probability scores. With this parameter, you can specify a custom name for this point layer. If left empty, a default name will be used, based on the species name, with as suffix samplePredictions\n\n6\n\nThe module will create a vector layer with background points used as input for this model. The attribute table contains the predicted probability scores. With this parameter, you can specify a custom name for the layer. If left empty, a default name will be used, based on the species name, with as suffix backgroundPredictions.\n\n7\n\nIf the projectionlayers is set, a raster prediction layer will be created that reflects the potential distribution based on the projection layers. With the parameter predictionlayer, you can set the name of this output layers. If left empty, a default name will be used, based on the species name, with as suffix the name of the folder with the environmental raster layers. In this case, that would have been Erebia_alberganus_obs_envdat.\n\n8\n\nSince this is the first time we are running this module, we use the maxent parameter to set the path to the Maxent.jar file, and the -i flag to copy the jar file to the add-on directory where GRASS will automatically find it on subsequent runs. If you don’t know where to find this file, go back to Section 1.2.3. Note, here, the maxent.jar file is assumed to be in the working directory. If not, provide the full path to the file.\n\n9\n\nTo improve the performance, you can increase the number of threads. However, before making adjustments, check your system specifications and adjust the settings based on your system’s capabilities.\n\n10\n\nTo improve the performance, you can allocate more memory. However, before making adjustments, check your system specifications and adjust the settings based on your system’s capabilities.\n\n11\n\nThe first three flags will create extra output that helps you to evaluate the model: y: Create a point feature layer with for each occurrence point the predicted probability score. This can be useful to identify discrepancies between the observed and predicted distribution. b: Create a vector point layer with predicted probability scores at the background point locations. This is useful to identify discrepancies between the observed and predicted distribution. g: Create response curves, which visualize the (marginal) effect of each explanatory variable on the predicted probability. With the i flag, you copy the maxent.jar file to the addon directory, so GRASS can find it next times without you having to provide the path. I.e., you only need to set this flag the first time you use this module.\n\n\n\n\nOpen the r.maxent.train dialog and run the module with the following parameter settings:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nsamplesfile 3\ndataset01/species.swd\n\n\nenvironmentallayersfile 4\ndataset01/background.swd\n\n\nprojectionlayers 5\ndataset01/envdat\n\n\noutputdirectory 6\nmodel_01\n\n\nsamplepredictions7\nE_alberganus_samplepred\n\n\nbackgroundpredictions8\nE_alberganus_bgrdpred\n\n\npredictionlayer9\nE_alberganus_probability\n\n\nmaxent 10\nmaxent.jar\n\n\nCopy maxent.jar to addon directory (i) 11\n✅\n\n\nthreads 12\n4\n\n\nmemory 13\n1000\n\n\nCreate a vector point layer from the sample predictions (y) 14\n✅\n\n\nCreate a vector point layer with predictions at backgr. points (b) 15\n✅\n\n\nCreate response curves (g) 16\n✅\n\n\n\nTip: if you are using the r.maxent.train dialog screen, keep it open after it finishes. That way, for our next run, you only need to adjust some parameter settings instead of typing in all again.\n\n\n\nDepending on how you run the module, it will generate some information in the terminal, console, Python console, or the function’s command output window (we will refer to any of these as the console). It also creates a number of files in the output directory you specify, and layers in the current mapset. We will look at these results in the next section.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model training</span>"
    ]
  },
  {
    "objectID": "4_model_training.html#sec-4examinetheresults",
    "href": "4_model_training.html#sec-4examinetheresults",
    "title": "4  Model training",
    "section": "4.3 Examine the results",
    "text": "4.3 Examine the results\nThe r.maxent.train modules shows a few messages on the console (Figure 4.1). The first message is that the maxent.jar file is copied to the GRASS addon script directory. This is good, it means that henceforward we don’t have to provide the path to the file anymore.\n\n\n\n\n\n\nFigure 4.1: Messages of the r.maxent.train module in the console, showing the number of training and background points, and the training AUC.\n\n\n\n\n\n\n\n\n\nKeep in mind that your evaluation statistics, including the AUC, may vary slightly from what is presented here. These statistics depend partly on the background points, which are selected randomly. As a result, your background points—and thus your evaluation statistics—may vary.\n\n\n\nThe second message is a warning indicating that the background.swd file contains -9999 values, which represent no data. This suggests that one or more background points fall outside the area covered by the Bioclim raster layers. A comparison of the background points and the Bioclim layers confirms that some points are indeed located in the sea.\n\n\n\n\n\nQ 4.1 Can you explain what went wrong here?\n\n\n\nThe background points were generated within the boundaries of a vector layer representing European countries. The bioclim layers contain values for land areas only. The boundaries of these land areas do not perfectly match those of the European countries layer (Figure 4.2).\n\n\n\n\n\n\nFigure 4.2: Examples of background points falling within the boundaries of the European countries, but outside the land area as defined by the bioclim layers.\n\n\n\nTo address this mismatch, we can use one of the bioclim layers to create the MASK, instead of using the vector layer of European countries.\n\n\n\nThe module prints a few basic statistics to the console. These are the number of training samples, the number of background points, and the the training AUC.\n\n\n\n\n\nQ 4.2 We created and used 10,000 background points as input. So where do these 13083 points come from?\n\n\n\nBy default, Maxent adds the presence points to the background points. You can disable this with the -n flag.\nStill, the presence + original background points do not add up to the number reported here. This is because Maxent ignores presence points if there is already a background point at that location.\n\n\n\nThe Area Under the Receiver Operator Curve (AUC) is a common metric for evaluating species distribution model (SDM) performance. Here, we are provided with the training AUC, calculated using the same presence and absence points that were used to train the model. The AUC represents the probability that a randomly selected presence location ranks higher than a randomly selected background point17. An AUC of 0.866 suggests that the model has reasonably good predictive ability for species distribution18. We’ll revisit this statistic in more detail below.\n\n4.3.1 Probability maps\nThe Erebia_alberganus_obs.html file in the output folder provides more evaluation statistics, including a short explanation of the results. Before reviewing these, let’s examine the sample prediction, background prediction, and raster prediction layers. In GRASS, go to the data panel, and double click on each of them to open them in the Map display panel.\n\nE_alberganus_probabilityE_alberganus_samplepredE_alberganus_bgrdpredClamping map\n\n\n\n\n\n\n\n\nFigure 4.3: The raster layer E_alberganus_probability with the predicted probability of occurrences of Erebia_alberganus, based on model_01.\n\n\n\nThe map in Figure 4.3 shows the predicted probability of presence of Erebia_alberganus within the bounds of the study area. As expected, probability scores are high in most areas where the species has been observed. However, there are additional areas where predicted probability scores are also high. We will not attempt to explain these differences here, as that analysis is beyond the scope of this tutorial, but it is clear that these results warrant further investigation.\n\n\n\n\n\n\n\n\nFigure 4.4: The E_alberganus_samplepred vector layer with the GBIF occurrences. The colors in this layer represent the predicted probability that the species occurs at these locations, based on model_01.\n\n\n\nThe map in Figure 4.4 shows the predicted probability that Erebia alberganus occurs at each observed location, allowing us to assess where the model accurately predicts suitable conditions and where it incorrectly suggests less or no suitable conditions. Notable examples of the latter include several occurrence points in Bulgaria. A follow up step would be to compare these predictions with maps of relevant environmental data layers or examining the GBIF source data for these observations more closely.\n\n\n\n\n\n\n\n\nFigure 4.5: The E_alberganus_bgrdpred vector layer with the locations of the background points. The colors in this layer represent the predicted probability of the occurrence of the species at these locations, based on model_01.\n\n\n\nThe map in Figure 4.5 shows the predicted probability of Erebia alberganus to occur at individual background point locations, providing similar information to Figure 4.3, but with a different presentation. Depending on your needs / preferences, you may want to have the module generate one of these maps.\n\n\n\n\n\n\n\n\nFigure 4.6: The values in the E_alberganus_probability_clamping raster layer give the absolute difference in predictions when using clamping vs not using clamping. The map is the second map shown in the Erebia_alberganus_obs.html webpage.\n\n\n\nClamping restricts environmental variables and features to the range of values found in the training data. For example, if the input raster layer for bio_1 (annual mean temperature) has a value of 35°C, but the highest temperature among the training points is 32°C, all bio_1 values above 32°C are clamped to 32°C. This prevents the model from extrapolating beyond the conditions observed during training and ensures that predictions remain within the range of known environmental conditions. Higher values on the map indicate areas where clamping is likely to have a greater effect on predicted fitness.\nThe clamping map is particularly useful for examining how well the background points cover the environmental conditions. In this case, the clamping is limited to a few small areas (outlined in red). These include some mountain tops and other small mountainous areas. This limited clamping is not surprising given the environmental heterogeneity typical of mountain areas. However, the extent of these regions is minimal, so no further adjustments are done at this stage.\n\n\n\nNote that Maxent supports four output formats for model value: raw, cumulative, logistic and cloglog. We used the default output, which is cloglog. It represents the probability of presence, and has a value between 0 and 1. Importantly, one should be aware that the scores strongly depends on details of the sampling design [1].\n\n\n4.3.2 Presence-absence\nWhile a probability distribution map shows the likelihood of species occurrence across a landscape, it can also be useful to convert this map into a binary presence-absence map. To do this, you need to convert the probability values, which range between 0 and 1, into binary values of 0 (absent) and 1 (present). This is done by selecting a threshold: all cells with a probability above this threshold are classified as 1 (presence), and all others as 0 (absence). We can for example use a threshold of 0.5 to convert the probability map to a presence-absence map. There are several ways to accomplish this conversion.\nThe r.mapcalc module is a versatile and powerful tool for this kind of raster calculation, while the r.recode module might be slightly faster for simple thresholding operations. Below, both methods are demonstrated for comparison.\n\nStep 4.3  \n\n\n\n\n\n# Use r.recode\nr.recode input=E_alberganus_probability output=E_alberganus_bin rules=- &lt;&lt; EOF\n0.0:0.5:0\n0.5:1:1\nEOF\n\n# Or use r.mapcalc\nr.mapcalc expression=\"E_alberganus_bin = if(E_alberganus_probability &lt;0.5,0,1)\"\n\n\n# Use r.recode\nrules = \"0.0:0.5:0\\n0.5:1:1\"\n1gs.write_command(\n    \"r.recode\",\n    input=\"E_alberganus_probability\",\n    output=\"E_alberganus_bin\",\n    rule=\"-\",\n    stdin=rules,\n)\n\n# Use r.mapcalc\ngs.run_command(\n    \"r.mapcalc\", expression=\"E_alberganus_bin = if(E_alberganus_probability &lt;0.5,0,1)\"\n)\n\n1\n\nNote that we need to use the gs.write_command because we are using stdin parameter.\n\n\n\n\nRun the r.recode dialog:\n\n\n\n\n\n\nFigure 4.7: Convert the probability map to a binary map using the r.recode module.\n\n\n\nOr use the r.mapcalc module. You can find this in the menu raster → raster map calculator → raster map calculator\n\n\n\n\n\n\nFigure 4.8: Convert the probability map to a binary map using the r.mapcalc module.\n\n\n\n\n\n\nThe challenge is to choose the best threshold. The answer is the perhaps unsatisfactory “it depends”. Setting a lower threshold will classify more cells as “present”, which means the model will correctly identify more actual presence locations (more true positives). However, this also increases the likelihood of predicting the species in areas where it may not actually occur (more false positives). Conversely, a higher threshold will make the model more conservative, classifying fewer cells as “present”. This reduces false positives, but also means that many areas where the species actually occurs may be missed (increasing false negatives).\nThe best threshold depends on your priorities: whether it’s more important to maximize sensitivity (identifying all areas where the species might occur) or specificity (ensuring that areas predicted to be suitable are actually suitable). Table 4.1, which comes from the file Erebia_alberganus_obs.html in the output folder, shows some common thresholds and the corresponding omission rates and fraction of the study area that would be classified as suitable based on that threshold. See the Maxent manual and references for an explanation of these thresholds.\n\nThreshold values0.2610.4120.600\n\n\n\n\n\nTable 4.1: Threshold values and corresponding fractional predicted area and omission rate values. The tabs on the right show the resulting maps for three of these thresholds.\n\n\n\n\n\n\n\n\n\n\n\n\nCumulative threshold\nCloglog threshold\nDescription\nFractional predicted area\nTraining omission rate\n\n\n\n\n1.000\n0.030\nFixed cumulative value 1\n0.489\n0.005\n\n\n5.000\n0.168\nFixed cumulative value 5\n0.303\n0.023\n\n\n10.000\n0.412\nFixed cumulative value 10\n0.252\n0.077\n\n\n0.005\n0.000\nMinimum training presence\n0.901\n0.000\n\n\n11.990\n0.476\n10 percentile training presence\n0.240\n0.100\n\n\n19.976\n0.600\nEqual training sensitivity and specificity\n0.206\n0.206\n\n\n6.664\n0.261\nMaximum training sensitivity plus specificity\n0.279\n0.036\n\n\n2.912\n0.080\nBalance training omission, predicted area and threshold value\n0.360\n0.010\n\n\n3.572\n0.105\nEquate entropy of thresholded and original distributions\n0.337\n0.014\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.9: Binary map based on a Maximum training sensitivity plus specificity threshold. The red oulines show the boundaries of the species rangemap.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.10: Binary map based on a Fixed cumulative value 10 threshold. The red oulines show the boundaries of the species range according to the IUCN Redlist rangemap.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.11: Binary map based on a Equal training sensitivity and specificity threshold. The red oulines show the boundaries of the species range according to the IUCN Redlist rangemap.\n\n\n\n\n\n\nComparing the predicted distribution with the IUCN Red List range map highlights several discrepancies. In some areas, the range map boundaries appear slightly shifted, while in others they seem overly optimistic. The question is whether this is due to unaccounted for variables or indicates a decline of the species’ presence in these areas. Notably, there are the extensive areas with similar climate conditions where the species has not been recorded according to GBIF data. This suggests that factors other than those used in the model may play a critical role in determining the distribution of the species.\n\n\n4.3.3 Evaluation statistics\nTo determine how good the model is at predicting the presence of a species, Maxent generates some model evaluation statistics. To do this, it converts the probability map to a binary map using a range of threshold values from 0 to 1. Each time, the number of presence points correctly classified, the number misclassified as absence, and the number of background points classified as either presence or absence are calculated. The resulting statistics serve as input for the calculation of standard validation metrics such as the area under the receiver operating characteristic curve (AUC-ROC). The results, which can be found in the file Erebia_alberganus_obs.html in the output folder, are briefly discussed below.\n\nAUC-ROCOmission graph\n\n\nThe ROC plot compares the proportion of correctly classified presence points, known as the sensitivity of the model, with the proportion of background points classified as presence. This comparison is made across a range of presence probability cutoff values between 0 and 1. The closer the ROC curve is to the upper left corner, the larger the area under the curve (AUC), and the better the model performance. In other words, the AUC provides a threshold-independent estimate of the predictive power of the model. As we have seen before, the AUC for our model is 0.866.\nA ROC curve below the black line indicates that the model performs worse than a random model would. It is important to reiterate that AUC values tend to be higher for species with narrow ranges, relative to the study area described by the environmental data.\n\n\n\n\n\n\nFigure 4.12: ROC curve and the area under the curve statistics. Because we are dealing with presence-only data, we do not have real absence points. Therefore, on the X-axis, Maxent uses the 1 - fraction of background points predicted to be present instead of the fraction of absence points that are predicted to be present.\n\n\n\n\n\nThe omission and predicted area graph (Figure 4.13) illustrates how the proportion of presence points incorrectly classified as absence (omission - blue line) and the fraction of background points predicted as suitable (red line) vary with the choice of cumulative threshold. Note that the x-axis shows the cumulative raw threshold values19. We see that the omission matches the expected omission rate (black line) closely.\n\n\n\n\n\n\nFigure 4.13: The omission/commission graph shows how omission rate and predicted area vary with the choice of cumulative threshold. The pink arrows show two of the threshold values and corresponding omission rate in Table 4.1.\n\n\n\n\n\nNote that Maxent uses random background points rather than true absence points. Since the background points don’t exclusively represent areas where the species is absent but rather a sample of the entire study area, there’s a degree of overlap between conditions at presence and background points. This overlap limits the model’s ability to perfectly separate the two classes, meaning that the AUC will typically be less than 1. In contrast, if true absences were available and clearly distinct from presences, a higher maximum AUC could theoretically be achieved.\n\n\n\n\n\n4.3.4 Variable importance\nA natural application of species distribution modeling is to answer the question, “Which variables are most important for the species being modeled? There is more than one way to answer this question.\nWhile the Maxent model is being trained, it keeps track of which environmental variables are contributing to fitting the model. This is at the end of the training process converted into an estimate of the relative contribution of each variable to the model. This is expressed as a percentage contribution. The higher the percentage, the more important that variable is for the model. This is somewhat equivalent to the coefficients of a regression model. Like the regression coefficients, when the environmental variables are highly correlated environmental variables, the percent contributions should be interpreted with caution.\n\n\n\nTable 4.2: Relative importance of the explanatory variables. The table is presented in the section ‘Analysis of variable contributions’ of the file Erebia_alberganus_obs.html\n\n\n\n\n\n\n\n\n\n\nVariable\nPercent contribution\nPermutation importance\n\n\n\n\nbio_1\n43.8\n32.5\n\n\nbio_8\n23.4\n0.8\n\n\nbio_4\n23.3\n39.2\n\n\nbio_13\n4.9\n15.2\n\n\nbio_2\n1.3\n0.6\n\n\nbio_19\n1.1\n4.4\n\n\nbio_9\n0.8\n1.3\n\n\nbio_14\n0.7\n0.4\n\n\nbio_15\n0.6\n5.5\n\n\n\n\n\n\nThe other metric is the Permutation importance. The contribution for each variable is determined by randomly permuting the values of that variable among the training points (both presence and background) and measuring the resulting decrease in training AUC. A large decrease indicates that the model depends heavily on that variable. Values are normalized to give percentages.\n\n\n4.3.5 Response curves\nThe HTML file Erebia_alberganus_obs.html also includes a section with two types of response curves. Both show how each environmental variable affects the prediction of the species probability distribution. Note that you can click on a plot for a larger and more detailed image20.\nThe first set of curves (Figure 4.14) illustrates how the predicted probability of presence changes when each environmental variable is varied individually, while keeping all other variables fixed at their average sample value. These are known as marginal response curves. They indicate, for example, that the species prefers cooler areas, with a mean annual temperature (bio_1) below 5°C, or a temperature seasonality around 600, corresponding to a standard deviation of monthly temperatures close to 6°C.\nThe curves in Figure 4.15 represent different models, where each model is built using only the corresponding environmental variable. These curves can be more straightforward to interpret when there are strong correlations between variables.\n\nMarginal response curvessingle-variable response curves\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.14: Response curves created by varying the specific variable, while keeping all other variables fixed at their average sample value\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.15: Response curves created by running a model based on only the specific variable as explanatory variable.\n\n\n\n\n\n\nThese two types of curves can sometimes yield conflicting insights. Click between the tabs to compare them. For instance, the marginal response curve for precipitation of the driest month (bio_14) suggests a negative relationship between this variable and the predicted probability of presence. In other words, when all other variables are held constant, an increase in precipitation during the driest month reduces the predicted probability of presence.\nIn contrast, the single-variable response curve for bio_14 shows the opposite trend: as precipitation increases, the predicted probability of presence also rises. This discrepancy likely arises because bio_14 is correlated with other precipitation-based variables21. Consequently, when environmental variables are correlated, marginal response curves can sometimes be misleading. Another example of this occurs when two closely correlated variables have nearly opposite response curves, producing a combined effect that remains minimal across most areas.\nTherefore, it’s advisable to examine and compare both sets of curves for a clearer understanding. For further details and examples, see the Maxent tutorial.\n\n\n4.3.6 Raw data\nAt the end of the HTML file Erebia_alberganus_obs.html, you’ll find a summary of the model input data and parameter settings. In addition, links are provided to various files in your output folder with model settings, predictions at presence and background point locations and summary statistics. This allow you to further analyse the model outcomes using other tools such as R. Some examples are provided in the Maxent tutorial.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model training</span>"
    ]
  },
  {
    "objectID": "4_model_training.html#footnotes",
    "href": "4_model_training.html#footnotes",
    "title": "4  Model training",
    "section": "",
    "text": "It is good to reiterate that r.maxent.train runs the Maxent application in the background. In this tutorial, we cover some of the results, but for a deeper understanding, refer to the tutorial and other essential resources available on the Maxent website.↩︎\nTo make sure the output raster aligns with the input environmental variables, we use the raster parameter this time.↩︎\nThe (relative) path to the species swd file.↩︎\nThe (relative) path to the background swd file.↩︎\nThe (relative) path to the folder that holds the ascii raster layers of the input environmental variables.↩︎\nThe (relative) path to the folder that where Maxent should write the results.↩︎\nThe module will create a point layer with the locations of species occurrences used for the model. The attribute table contains the predicted probability scores. With this parameter, you can specify a custom name for this point layer. If left empty, a default name will be used, based on the species name, with as suffix samplePredictions↩︎\nThe module will create a vector layer with background points used as input for this model. The attribute table contains the predicted probability scores. With this parameter, you can specify a custom name for the layer. If left empty, a default name will be used, based on the species name, with as suffix backgroundPredictions.↩︎\nIf the projectionlayers is set, a raster prediction layer will be created that reflects the potential distribution based on the projection layers. With the parameter predictionlayer, you can set the name of this output layers. If left empty, a default name will be used, based on the species name, with as suffix the name of the folder with the environmental raster layers. In this case, that would have been Erebia_alberganus_obs_envdat.↩︎\nSince this is the first time we are running this module, we use the maxent parameter to set the path to the Maxent.jar file, and the -i flag to copy the jar file to the add-on directory where GRASS will automatically find it on subsequent runs. If you don’t know where to find this file, go back to Section 1.2.3. Note, here, the maxent.jar file is assumed to be in the working directory. If not, provide the full path to the file.↩︎\nWith the -i flag, you copy the maxent.jar file to the addon directory, so GRASS can find it next times without you having to provide the path. I.e., you only need to set this flag the first time you use this module.↩︎\nTo improve the performance, you can increase the number of threads. However, before making adjustments, check your system specifications and adjust the settings based on your system’s capabilities.↩︎\nTo improve the performance, you can allocate more memory. However, before making adjustments, check your system specifications and adjust the settings based on your system’s capabilities.↩︎\nCreate a point feature layer with for each occurrence point the predicted probability score. This can be useful to identify discrepancies between the observed and predicted distribution.↩︎\nCreate a vector point layer with predicted probability scores at the background point locations. This is useful to identify discrepancies between the observed and predicted distribution.↩︎\nCreate response curves, which visualize the (marginal) effect of each explanatory variable on the predicted probability.↩︎\nThe AUC is normally used to determine how the model distinguishes between presences and absences. That is, it compares the portion of correctly classified known presence points, known as the sensitivity of the model, and the portion of absence points that were classified as presence. In presence-only models like Maxent, there are no absence points. So the ROC compares the portion of correctly classified presence points with the fraction of background points that is predicted to be present (1- specificity, or fractional predicted area).↩︎\nAUC values range from 0 to 1, with certain threshold interpretations commonly applied: values below 0.5 indicate the model is performing worse than random chance. Values between 0.5 and 0.7 typically suggest poor model performance, values from 0.7 to 0.9 suggest reasonable performance, and values above 0.9 are generally considered good. However, these thresholds should be interpreted cautiously and not used at face value.\n\n\nIt is important to realize that the AUC doesn’t account for the prevalence of presence points in your dataset. In fact, you can increase the AUC simply by adding more background points. Try this for yourself: create a new point layer with 50,000 background points and run the r.maxent.train module again with the same settings. See how this affects the AUC value. It means that AUC should only be used to compare models with a similar ratio of presence to background points.↩︎\nSo to be clear, it does not use the cloglog values. In the file that ends with _omission.csv (in our case, this is the file Erebia_alberganus_obs_omission.csv), you can find for each raw value the corresponding cloglog value. In [tab-tresholdvalues_model01?] some key treshold values are provided (both the raw and cloglog variants) with the corresponding ommission values.↩︎\nWe have attempted to reduce the issue of multi-collinearity using the stepwise VIF procedure (Section 3.3). But the results suggest that this did not solve all problems.↩︎\nWe have attempted to reduce the issue of multi-collinearity using the stepwise VIF procedure (Section 3.3). But the results suggest that this did not solve all problems.↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model training</span>"
    ]
  },
  {
    "objectID": "5_model_prediction.html",
    "href": "5_model_prediction.html",
    "title": "5  Model prediction",
    "section": "",
    "text": "5.1 Creating a future distribution map\nWe already downloaded the climate layers for the above-mentioned SSP and GCM in Section 2.5, and saved them in the mapset climate_EC_Earth3_Veg. Let’s first make sure we have access to this mapset from the current mapset. As you might remember, we can use the g.mapsets module for that.\nWe can now use the r.maxent.predict to apply the previously created Maxent model to this set of climate raster data. The minimum input requirements are the names of the input raster layers and a lambdas file and the name of the raster layers for the predictor variables described in the lambdas file. The lambdas file was one of the outputs from r.maxent.train and can be found in the output folder model_01. It has the extension .lambdas and describes the Maxent model.\nThe names of the raster layers need to be the same as the variable names used to create the Maxent model. If you are not sure, you can check the file maxent_explanatory_variable_names.csv in folder with the model out. This file is automatically created by the r.maxent.train module and can be found in the model_01 folder. If the raster names are not the same as the variables used to create the Maxent model, the variables parameter can be used to provide the right variable names1.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model prediction</span>"
    ]
  },
  {
    "objectID": "5_model_prediction.html#creating-a-future-distribution-map",
    "href": "5_model_prediction.html#creating-a-future-distribution-map",
    "title": "5  Model prediction",
    "section": "",
    "text": "Step 5.1  \n\n\n\n\n\ng.mapsets mapset=climate_EC_Earth3_Veg operation=add\n\n\ngs.run_command(\n    \"g.mapsets\", mapset=[\"climate_EC_Earth3_Veg\"], operation=\"add\"\n)\n\n\nIn the menu, go to Settings → GRASS working environment → Mapset access and select the mapsets climate_EC_Earth3_Veg.\n\n\n\n\n\n\nStep 5.2  \n\n\n\n\n\n# Make sure you are working in the working directory\ncd path_to_working_directory\n\n# Create the future potential distribution map\nr.maxent.predict \\\n  lambda=model_01/Erebia_alberganus_obs.lambdas \\\n  rasters=bio.1,bio.13,bio.14,bio.15,bio.19,bio.2,bio.4,bio.8,bio.9 \\\n1  variables=bio_1,bio_13,bio_14,bio_15,bio_19,bio_2,bio_4,bio_8,bio_9 \\\n  output=E_alberganus_futprob\n\n1\n\nThe names of the future bioclim layers differ from the variables, so we need to provided the variable names. Be careful, they need to be given in the same order as the corresponding raster layers.\n\n\n\n\n# Make sure you are working in the working directory\nos.chdir(\"replace-for-path-to-working-directory\")\n\n# Create the future potential distribution map\ngs.run_command(\n    \"r.maxent.predict\",\n    lambdafile=\"model_01/Erebia_alberganus_obs.lambdas\",\n    rasters=\"bio.1,bio.13,bio.14,bio.15,bio.19,bio.2,bio.4,bio.8,bio.9\",\n1    variables=\"bio_1,bio_13,bio_14,bio_15,bio_19,bio_2,bio_4,bio_8,bio_9\",\n    output=\"E_alberganus_futprob\",\n)\n\n1\n\nThe names of the future bioclim layers differ from the variables, so we need to provided the variable names. Be careful, they need to be given in the same order as the corresponding raster layers.\n\n\n\n\nOpen the r.maxent.predict dialog and run it with the following parameter settings:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nlambdafile\nmodel_01/Erebia_alberganus_obs.lambdas\n\n\nrasters\nbio.1,bio.13,bio.14,bio.15,bio.19,bio.2,bio.4,bio.8,bio.9\n\n\nvariables 2\nbio_1,bio_13,bio_14,bio_15,bio_19,bio_2,bio_4,bio_8,bio_9\n\n\noutput\nE_alberganus_futprob",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model prediction</span>"
    ]
  },
  {
    "objectID": "5_model_prediction.html#current-vs-future-distribution",
    "href": "5_model_prediction.html#current-vs-future-distribution",
    "title": "5  Model prediction",
    "section": "5.2 Current vs future distribution",
    "text": "5.2 Current vs future distribution\n\n5.2.1 Comparing probability maps\nThere are several ways to compare the predicted current and future distributions of our species. For a visual comparison of the probability maps, it’s essential that both maps use the same color table. To achieve this, we can use the r.colors module to match the color table of the future probability distribution map with that of the current climate distribution layer (E_alberganus_probability). This ensures that any differences we see are due to changes in distribution, not color scale variations.\n\nStep 5.3  \n\n\n\n\n\nr.colors map=E_alberganus_futprob raster=E_alberganus_probability\n\n\ngs.run_command(\n    \"r.colors\", map=\"E_alberganus_futprob\", raster=\"E_alberganus_probability\"\n)\n\n\nOpen the r.colors dialog and run it with the following parameter settings:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nmap\nE_alberganus_futprob\n\n\nraster\nE_alberganus_probability\n\n\n\n\n\n\nExamining the resulting map reveals that by 2081, the potential distribution area of the Almond-eyed Ringlet is projected to decrease significantly under SSP585 (Figure 5.1). In the Alps, its range is expected to shift to higher altitudes, while in other areas, the species may dissapear entirely.\n\nFuture climate conditionsCurrent climate conditions\n\n\n\n\n\n\n\n\nFigure 5.1: The raster layer E_alberganus_futprob with the predicted probability of occurrences of Erebia_alberganus in 2081-2100 under the SSP585 climate scenario, based on model_01.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.2: The raster layer E_alberganus_probability with the predicted probability of occurrences of Erebia_alberganus, based on model_01.\n\n\n\n\n\n\nTo highlight areas where the changes are expected to happen, we can create a change map by calculating the differences in habitat suitability scores between current and future distributions.\n\nStep 5.4  \n\n\n\n\n\n# Calculate the change map\nr.mapcalc \\\n  expression=\"E_alb_diff = E_alberganus_futprob - E_alberganus_probability\"\n\n# Assign a color table that emphasize differences\nr.colors map=E_alb_diff color=differences\n\n\n# Calculate the change map\n# Calculate the change map\ngs.run_command(\n    \"r.mapcalc\",\n    expression=\"E_alb_diff = E_alberganus_futprob - E_alberganus_probability\",\n)\n\n# Assign a color table that emphasize differences\ngs.run_command(\"r.colors\", map=\"E_alb_diff\", color=\"differences\")\n\n\nOpen the raster map calculator from the raster and run it with the following expression.\n\n\n\n\n\n\nFigure 5.3: Use the raster map calculator to calculate the differences in habitat suitability scores between current and future distributions.\n\n\n\nNext, open the r.colors module, and run it with the following parameters.\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nmap\nE_alb_diff\n\n\ncolor\ndifferences\n\n\n\n\n\n\nOpen the change map tab above to view the resulting map (Figure 5.4). This map reinforces our earlier observations: in most areas, climate conditions are predicted to become less suitable, while at higher elevations in the Alps, climate conditions may become more favorable3\n\nChange map\n\n\n\n\n\n\n\n\nFigure 5.4: The change map with negative values (blue) representing where climate conditions are predicted to become (less) suitable for the species, and positive values representing areas where climate conditions are predicted to become more suitable (red). Te inset map shows the region where the species may shift towards higher elevations.\n\n\n\n\n\n\n\n\n5.2.2 Presence-absence maps\nWhile the change map is useful for highlighting areas of change, it doesn’t indicate where conditions are, and will remain, unsuitable - or conversely, where they are and will remain suitable. Another way to approach this is to convert the probability maps to presence-absence maps, and subsequently compare these.\nFor the first step we need to select a threshold value. Raster cells with a probably &gt; threshold value will be converted to 1 (presence), and the other raster cells will be converted to 0 (absence). We’ll use the threshold value that balances the sensitivity4 and specificity5. In Table 4.2, we can see that for our model, this is 0.6. After creating the presence-absence map, we assign it a color table using the r.colors module.\n\nStep 5.5  \n\n\n\n\n\n# Convert the probability under current conditions\nr.mapcalc expression=\"E_alberganus_bin = if(E_alberganus_probability &lt;0.6,0,1)\"\n\n# Convert the probability under future conditions\nr.mapcalc expression=\"E_alberganus_futbin = if(E_alberganus_futprob &lt;0.6,0,1)\"\n\n# Assign white to absence (0) and green to presence (1)\n1echo -e \"0 white\\n1 green\" | r.colors map=E_alberganus_bin rules=-\necho -e \"0 white\\n1 green\" | r.colors map=E_alberganus_futprob rules=-\n\n1\n\nWith echo -e you normally print a text to the screen. Here, these are the color instructions. The ‘|’ (pipe) symbol, takes what would have been printed on the screen and sends it directly into the next command, r.colors, as input. The rules=- part tells r.colors to get its color instructions from the previous command (instead of a file). The dash (-) means “read from the pipeline,” which is the output of echo.\n\n\n\n\n# Convert the probability to presence-absence under current conditions\ngs.run_command(\n    \"r.mapcalc\",\n    expression=\"E_alberganus_presabs = if(E_alberganus_probability &lt;0.6,0,1)\",\n)\n\n# Convert the probability to presence-absence under future conditions\ngs.run_command(\n    \"r.mapcalc\",\n    expression=\"E_alberganus_presabsfut = if(E_alberganus_futprob &lt;0.6,0,1)\",\n)\n\n# Assign white to absence (0) and green to presence (1)\nCOLORS = \"0 white\\n1 green\"\ngs.write_command(\n1    \"r.colors\",\n    map=\"E_alberganus_presabs\",\n    rules=\"-\",\n    stdin=COLORS,\n) \ngs.write_command(\n2    \"r.colors\",\n    map=\"E_alberganus_presabsfut\",\n    rules=\"-\",\n    stdin=COLORS,\n)\n\n1\n\nThis sets the color table for the raster map E_alberganus_presabs, using the color rules defined in the COLORS variable (0 as white, 1 as green). The color rules are provided directly in the code and passed to the r.colors command via stdin parameter, eliminating the need for an external file.\n\n2\n\nThis sets the color table for the raster map E_alberganus_presabsfut, using the color rules defined in the COLORS variable (0 as white, 1 as green). The color rules are provided directly in the code and passed to the r.colors command via stdin parameter, eliminating the need for an external file.\n\n\n\n\nOpen the raster map calculator in the menu raster → raster map calculator → raster map calculator\n\n\n\n\n\n\n\n\n\n\n\n(a) Current conditions\n\n\n\n\n\n\n\n\n\n\n\n(b) Future conditions\n\n\n\n\n\n\n\nFigure 5.5: Convert current and future probability maps to binary maps using the r.mapcalc module.\n\n\n\nNow, assign colors to the two categories absence (0 = white) and presence (1 = green):\n\n\n\n\nNow we can compare the two maps to identify areas where the species is present both now and in the future, present now but will be absent in the future, absent now but will be present in the future, and absent both now and in the future. We can do this with the r.cross function, which creates a cross product of the category values from multiple raster map layers.\n\nStep 5.6  \n\n\n\n\n\nr.cross -z \\\n  input=E_alberganus_presabs,E_alberganus_presabsfut \\\n1  output=E_alberganus_presabschange\n\n1\n\nThe -z flag tells r.cross to ignore raster cells with no data.\n\n\n\n\ngs.run_command(\n    \"r.cross\",\n1    flags=\"n\",\n    input=\"E_alberganus_presabs,E_alberganus_presabsfut\",\n    output=\"E_alberganus_presabschange\",\n)\n\n1\n\nThe -z flag tells r.cross to ignore raster cells with no data.\n\n\n\n\nOpen the r.cross module and run it with the following parameters.\n\n\n\nParameter\nValue\n\n\n\n\ninput\nE_alberganus_presabs,E_alberganus_presabsfut\n\n\noutput\nE_alberganus_presabschange\n\n\nNon-NULL data only (z) 6\n✅\n\n\n\n\n\n\nTo gain a more quantitative perspective on the predicted changes, we can use the r.report module, which provides area statistics for each category (Figure 5.9).\n\nStep 5.7  \n\n\n\n\n\n1r.report -n map=E_alberganus_presabschange \\\n2  units=kilometers,percent\n\n1\n\nThe -n flag tells r.report to not report on no data values.\n\n2\n\nYou can choose in what unit(s) r.report should report the area statistics.\n\n\n\n\ngs.run_command(\n    \"r.report\",\n1    flags=\"n\",\n2    units=\"kilometers,percent\",\n    map=\"E_alberganus_presabschange\",  \n)\n\n1\n\nThe -n flag tells r.report to not report on no data values.\n\n2\n\nYou can choose in what unit(s) r.report should report the area statistics.\n\n\n\n\nOpen the r.report module and run it with the following parameters.\n\n\n\nParameter\nValue\n\n\n\n\nmap\nE_alberganus_presabschange\n\n\nunits 7\nkilometers,percent\n\n\nDo not report no data value (n) 8\n✅\n\n\n\n\n\n\nThe map we created in step 5.6 shows the predicted presence and absence of Erebia_alberganus under current and future conditions (Figure 5.6). According to the statistics we calculated in step 5.7, about 98% of the study area is and will remain unsuitable for our species. Currently, 2.15% of the area is classified as suitable for the species. However, projections suggest that this suitability will significantly decrease, with only 0.1% remaining suitable by 2081-2100. Additionally, 0.07% of the area may become newly suitable for the species over this period (Figure 5.9).\n\nThe mapStatistics\n\n\n\n\n\n\n\n\nFigure 5.6: The raster layer E_alberganus_presabschange with four categories: Category 0 shows areas where the species is predicted to be absent under both current and future conditions; category 1 represents areas where the species is absent now but predicted to be present in the future; category 2 highlights areas where the species is present now but predicted to be absent in the future; and category 3 indicates areas where the species is predicted to be present under both current and future conditions\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.7: The area statistics for the categorical layer E_alberganus_binchange.\n\n\n\n\n\n\nThe categories aren’t easy to read, and the colors of the map aren’t necessarily the best to bring out the different categories. We can therefore assign a different color table to the map using r.colors and rename the categories using r.category.\n\nStep 5.8  \n\n\n\n\n\n# Give the categories more meaningfull names\nr.category E_alberganus_presabschange separator=\":\" rules=- &lt;&lt; EOF\n0:Absent  - Absent\n1:Absent  - Present\n2:Present - Absent\n3:Present - Present\nEOF\n\nr.colors map=E_alberganus_futprob rules=- &lt;&lt; EOF\n0 white\n1 77:175:74\n2 251:154:153\n3 55:126:184\nEOF\n\n\n# Give the categories more meaningfull names\nCATS = (\n    \"0: Absent  - Absent\\n\"\n    \"1: Absent  - Present\\n\"\n    \"2: Present - Absent\\n\"\n    \"3: Present - Present\\n\"\n)\ngs.write_command(\n    \"r.category\",\n    map=\"E_alberganus_presabschange\",\n    separator=\":\",\n    rules=\"-\",\n    stdin=CATS,\n)\n\n# Define colors per category\nCOLORS = \"0 white\\n1 77:175:74\\n2 251:154:153\\n3 55:126:184\"\ngs.write_command(\n    \"r.colors\",\n    map=\"E_alberganus_presabschange\",\n    rules=\"-\",\n    stdin=COLORS,\n)\n\n\nYou can find the r.category in the raster menu.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.8: Using r.category to rename the categories of the raster layer E_alberganus_presabschange (click on the images to enlarge).\n\n\n\nTo change the colors of the categories, see step 5.5.\n\n\n\nNow we can create the map and report with area statistics again by repeating step 5.7 The results are, obviously, the same, but hopefully easier to read/interpret.\n\nThe mapStatistics\n\n\n\n\n\nThe raster layer E_alberganus_presabschange with four categories: Category 0 shows areas where the species is predicted to be absent under both current and future conditions; category 1 represents areas where the species is absent now but predicted to be present in the future; category 2 highlights areas where the species is present now but predicted to be absent in the future; and category 3 indicates areas where the species is predicted to be present under both current and future conditions\n\n\n\n\n\n\n\n\n\n\nFigure 5.9: The area statistics for the categorical layer E_alberganus_binchange.\n\n\n\n\n\n\nGRASS GIS includes various other modules that compute and visualize different (change) statistics. A good starting point is to explore the toolbox and the list of addons. You have now completed the basic steps to create and use a species distribution model using Maxent in GRASS GIS. In subsequent sections, which will be available in the coming weeks, we’ll dive deeper into the options, choices, and parameter settings to validate and fine-tune the model.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model prediction</span>"
    ]
  },
  {
    "objectID": "5_model_prediction.html#footnotes",
    "href": "5_model_prediction.html#footnotes",
    "title": "5  Model prediction",
    "section": "",
    "text": "Alternatively, the alias_file parameter can be used to provide a CSV file with the names of the explanatory variables (first column) and the names of the corresponding raster layers (second column).↩︎\nThe names of the future bioclim layers differ from the variables, so we need to provided the variable names. Be careful, they need to be given in the same order as the corresponding raster layers.↩︎\nThese maps should be interpreted with caution, as they reflect only projected changes in climate conditions and do not account for other potentially important factors.↩︎\nThe ability of the model to correctly identify presence locations, or true positives.↩︎\nNormally, this is the ability of the model to correctly identify absence locations (true negatives). However, for presence-only models this can be defined as the fraction of the background points predicted to be absent.↩︎\nThe -z flag tells r.cross to ignore raster cells with no data.↩︎\nYou can choose in what unit(s) r.report should report the area statistics.↩︎\nThe -n flag tells r.report to not report on no data values.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model prediction</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "1. Elith J, Leathwick JR (2009) Species\nDistribution Models: Ecological Explanation\nand Prediction Across Space and Time. Annual\nReview of Ecology, Evolution, and Systematics 40(1):677–697. https://doi.org/10.1146/annurev.ecolsys.110308.120159\n\n\n2. Phillips SJ, Anderson RP, Schapire RE (2006)\nMaximum entropy modeling of species geographic distributions. Ecological\nModelling 190(3):231–259. https://doi.org/10.1016/j.ecolmodel.2005.03.026\n\n\n3. Phillips SJ, Anderson RP, Dudík M, Schapire RE,\nBlair ME (2017) Opening the black box: An open-source release of\nMaxent. Ecography 40(7):887–893. https://doi.org/10.1111/ecog.03049\n\n\n4. Phillips SJ, Dudík M, Schapire R (2024) Maxent\nsoftware for modeling species niches and distributions (version\n3.4.4)\n\n\n5. Phillips S (2021) Maxnet: Fitting\n’maxent’ species distribution models with ’glmnet’\n\n\n6. van\nSwaay C, Wynhoff I, Verovnik R, et al (2010) Erebia albergana. The\nIUCN Red List of Threatened Species 2010. IUCN Red\nList of Threatened Species. https://doi.org/10.2305/IUCN.UK.2010-1.RLTS.T173278A6984115.en\n\n\n7. GRASS Development Team (2024) GRASS\nGIS\n\n\n8. Fick\nSE, Hijmans RJ (2017) WorldClim 2: New 1-km spatial\nresolution climate surfaces for global land areas. International Journal\nof Climatology 37(12):4302–4315. https://doi.org/10.1002/joc.5086\n\n\n9. Jarnevich CS, Stohlgren TJ, Kumar S, Morisette\nJT, Holcombe TR (2015) Caveats for correlative species distribution\nmodeling. Ecological Informatics 29:6–15. https://doi.org/10.1016/j.ecoinf.2015.06.007\n\n\n10. Phillips SJ, Dudík M, Elith J, et al (2009)\nSample selection bias and presence-only distribution models:\nImplications for background and pseudo-absence data. Ecological\nApplications 19(1):181–197. https://doi.org/10.1890/07-2153.1\n\n\n11. Kramer-Schadt S, Niedballa J, Pilgrim JD, et al\n(2013) The importance of correcting for sampling bias in\nMaxEnt species distribution models. Diversity and\nDistributions 19(11):1366–1379. https://doi.org/10.1111/ddi.12096\n\n\n12. Fourcade Y, Engler JO, Rödder D, Secondi J\n(2014) Mapping Species Distributions with MAXENT\nUsing a Geographically Biased Sample of\nPresence Data: A Performance Assessment of\nMethods for Correcting Sampling Bias. PLOS ONE\n9(5):e97122. https://doi.org/10.1371/journal.pone.0097122\n\n\n13. Beck J, Böller M, Erhardt A, Schwanghart W\n(2014) Spatial bias in the GBIF database and its effect on\nmodeling species’ geographic distributions. Ecological Informatics\n19:10–15. https://doi.org/10.1016/j.ecoinf.2013.11.002\n\n\n14. Acevedo P, Jiménez-Valverde A, Lobo JM, Real R\n(2012) Delimiting the geographical background in species distribution\nmodelling. Journal of Biogeography 39(8):1383–1390. https://doi.org/10.1111/j.1365-2699.2012.02713.x\n\n\n15. Barve N, Barve V, Jiménez-Valverde A, et al\n(2011) The crucial role of the accessible area in ecological niche\nmodeling and species distribution modeling. Ecological Modelling\n222(11):1810–1819. https://doi.org/10.1016/j.ecolmodel.2011.02.011\n\n\n16. Phillips SJ, Dudík M, Elith J, et al (2009)\nSample selection bias and presence-only distribution models:\nImplications for background and pseudo-absence data. Ecological\nApplications 19(1):181–197. https://doi.org/10.1890/07-2153.1\n\n\n17. Ward G, Hastie T, Barry S, Elith J, Leathwick\nJR (2009) Presence-Only Data and the EM\nAlgorithm. Biometrics 65(2):554–563. https://doi.org/10.1111/j.1541-0420.2008.01116.x\n\n\n18. Phillips SJ, Dudik M (2008) Modeling of species\ndistributions with Maxent: New extensions and a\ncomprehensive evaluation. Ecography 31(2):161–175\n\n\n19. Barbet-Massin M, Jiguet F, Albert CH, Thuiller\nW (2012) Selecting pseudo-absences for species distribution models: How,\nwhere and how many?: How to use pseudo-absences in niche\nmodelling? Methods in Ecology and Evolution 3(2):327–338. https://doi.org/10.1111/j.2041-210X.2011.00172.x\n\n\n20. Moua Y, Roux E, Seyler F, Briolant S (2020)\nCorrecting the effect of sampling bias in species distribution modeling\n– A new method in the case of a low number of presence\ndata. Ecological Informatics 57:101086. https://doi.org/10.1016/j.ecoinf.2020.101086\n\n\n21. Whitford AM, Shipley BR, McGuire JL (2024) The\ninfluence of the number and distribution of background points in\npresence-background species distribution models. Ecological Modelling\n488:110604. https://doi.org/10.1016/j.ecolmodel.2023.110604\n\n\n22. Graham MH (2003) Confronting multicollinearity\nin ecological multiple regression. Ecology 84(11):2809–2815. https://doi.org/10.1890/02-3114\n\n\n23. Craney TA, Surles JG (2002)\nModel-Dependent Variance Inflation Factor Cutoff Values.\nQuality Engineering 14(3):391–403. https://doi.org/10.1081/QEN-120001878",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "A1_grassbasics.html",
    "href": "A1_grassbasics.html",
    "title": "Appendix A. Working in GRASS GIS",
    "section": "",
    "text": "A.1 First time use\nTo run GRASS, click on the GRASS link on the desktop or from the system menu. Alternatively, open the command line window and type grass. When launching GRASS for the first time, you will open a default project called world_latlog_wgs84 where you can find a map layer called country_boundaries showing a world map in the WGS84 coordinate system.\nYou can also download other sample datasets. For example, the North Carolina dataset is often used in examples. You can simply reach them through Download sample project to current database management icon  in the Data panel. The sample data will be downloaded as a new project. in the current GRASS database.\nThe start up message offers you the option to create a new project (it still uses the older term Location for project here). Selecting this will start a project wizard which will guide you through a series of steps to browse and select the coordinate reference system (CRS) suitable for your study area or one that matches your data’s CRS. By default, it will do this in the current GRASS database, but you have the option to create a new one in the process. But before you start creating your own database, let’s see how GRASS handles and stores data.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Working in GRASS GIS</span>"
    ]
  },
  {
    "objectID": "A1_grassbasics.html#first-time-use",
    "href": "A1_grassbasics.html#first-time-use",
    "title": "Appendix A. Working in GRASS GIS",
    "section": "",
    "text": "Figure A.1: Start-up screen for first time users",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Working in GRASS GIS</span>"
    ]
  },
  {
    "objectID": "A1_grassbasics.html#sec-thedatabase",
    "href": "A1_grassbasics.html#sec-thedatabase",
    "title": "Appendix A. Working in GRASS GIS",
    "section": "A.2 The database",
    "text": "A.2 The database\n\nA.2.1 DB structure\nRaster (incl. aerial imagery and satellite data) and vector data are stored in GRASS internal formats. Hence, all external data files (GeoTIFF, SHAPE, GeoPackage, …) need to be imported into the GRASS GIS database. This is a set of directories and files with certain structure (Figure A.2). Directories within a GRASS database are called projects1. All data in a project are in the same projection/coordinate reference system (CRS)2. Typically, a project contains all data related to one project or a geographic area. Alternatively, a project can simply contain all data in a given CRS.\n\n\n\n\n\n\nFigure A.2: A sample GRASS dataset names grassdata with three projects, nc_spm, usa_albers and world_latlon. All data in one project have the same coordinate reference system (projection, datum). The structure of the nc_spm project includes the obligatory PERMANENT mapset, a mapset with data for the Wake county and another mapset with data set for user 1.\n\n\n\nEvery project includes a obligatory PERMANENT mapset, which typically contain commonly used data within one GRASS project such as base maps. It can furthermore contain other mapsets (subprojects) that can be used to group data sets together according to e.g., area, task, theme or user.\nWhen you’re working within the PERMANENT mapset, you automatically have read access to all the other mapsets within that project This allows you to use data from any other mapset in the current location without needing to change settings or append the mapset name.\nIf you are working within any other mapset (besides PERMANENT), you do not have automatic access to the data in other mapsets. To use data from a different mapset, you have two options:\n\nAppend the name of the mapset to the layer name using the format layername@mapsetname. This lets GRASS know to look for the layer in a different mapset.\nUse the g.mapsets command to add access to multiple mapsets. Alternatively, use the Data catalog to grant access.\n\nRegardless of which mapset you are working in, you can only write (create or modify) layers in the current mapset. Even if you access data from other mapsets, you cannot modify or add data to those mapsets. This ensures that mapsets maintain some level of separation and data integrity, preventing accidental changes to data outside your working environment.\n\n\nA.2.2 Mapset access\nIn GRASS GIS, the concept of access rights is closely tied to the structure of the mapsets within a Project Each location can have multiple mapsets, with a designated “PERMANENT” mapset that typically contains core data like region settings, projection information, and often serves as a repository for foundational layers (e.g., basemaps, DEMs). Here’s how access rights work across mapsets:\nFor GIS users not used to this setup, it may take a bit to get used to. But it has some clear advantages. Read this overview for more information about the GRASS database.\n\n\nA.2.3 Attribute management\nIn GRASS, the geometry (spatial data) of vector layers is stored and managed separately from their associated attribute data. The geometry (spatial data) of vector features, such as points, lines, and polygons, is stored in a special format within the GRASS GIS database. This geometry data includes the coordinates that define the shape and location of each feature on the map. The attribute data (non-spatial data)3 stored it in a separate attribute database. GRASS supports various database management systems (DBMS), such as SQLite (default), PostgreSQL, MySQL etc.). All these database backends provide full SQL support4.\nEach vector feature in the geometry data has a unique ID. This ID is used to link the feature to its corresponding row in the attribute table based on the matching ID (usually stored in the “cat” integer column). When performing analysis or creating maps, GRASS automatically combines the geometry with the relevant attribute information, allowing you to to work seamlessly with both the geometry and attribute data, even though they are stored separately.\nThe way the data is stored is not something the casual user should be concerned with, but good to know for more advanced uses. It gives more flexibility as users can update or modify attribute data without affecting the geometry, and vice versa. For an example of how this can be used, see Section 2.2.4.\n\n\nA.2.4 Create a database\nCreating a new database is rather straightforwards using the database database wizard . It guides the user through a number of simple dialogs. In the example below a new GRASS database called GRASSdb is created with a project called SDM with the coordinate reference system (CRS) WGS84 lat/lon (EPSG 4326). Note that if you want to create a new project in an existing GRASS database, use the project Wizard  instead, and go straight to step 3.\n\n 1 2 3 4 5/6 7\n\n\nIn the Data catalog, click the  icon to create a new database. Next, select the folder in which you want to create the database. Create a new sub-folder (the GRASS data directory) with the name of your GRASS GIS database and select it. After you have created the database, it will appear in the list of databases in the Data panel.\n\n\n\nAfter selecting the folder for your database, you are asked if you want to create a new project. Select Yes\n\n\n\nProvide the name of the new project, for example SDM, optionally add a description, and hit Next.\n\n\n\nIf you know the CRS of your data or study area, you can fill EPSG code or description. In this example, use keep the default choice to select the CRS from a list by EPSG in the next step.\n\nIf you do not know CRS of you data, you can read it from your georeferenced data file (e.g. GeoPackage or GeoTiff file with the related metadata properly included). If you use this option, you are asked whether the data itself should be imported into the new project. The default region is then set to match imported map.\n\n\nType in 4326 in the search box of the ‘Select coordinate reference system (CRS) window’, hit Next and in the subsequent ‘Data transformation’ window keep the default and hit Next again.\n\n\n\nReview the information in the ‘Summary’ window, and if everything is OK, hiet Enter.\n\n\n\n\nThe GRASS database and project should now be visible in the Data panel, and include a PERMANENT mapset. Note that you can also create a new database, project or mapset using Python code, as explained in Section A.9.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Working in GRASS GIS</span>"
    ]
  },
  {
    "objectID": "A1_grassbasics.html#data-import",
    "href": "A1_grassbasics.html#data-import",
    "title": "Appendix A. Working in GRASS GIS",
    "section": "A.3 Data import",
    "text": "A.3 Data import\nWhen processing spatial data in GRASS GIS, the first step is to import the datasets of interest into GRASS (they can also just be registered to avoid data duplication at the expense of sometimes lower data quality). To import your own data, use the raster data import  or vector data import  tools. If the coordinate reference system (CRS) of your data does not match your project’s CRS, data will be automatically reprojected. Alternatively, you can import data using the command line. For the import of raster data, use r.import or r.in.gdal. The first offers on-the-fly reprojection5, while the latter offers more control about how your data is imported. Similarly, you can import vector data using v.import and v.in.ogr.\n\n Import raster data Import vector data Import other data\n\n\n\n\n\n\n\n\nFigure A.3: Import raster data using the GUI or command line/console. Make sure to check the options under import settings tab.\n\n\n\n\n\n\n\n\n\n\n\nFigure A.4: Import vector data using the GUI or command line/console. Make sure to check the options under import settings tab.\n\n\n\n\n\n\n\n\n\n\n\nFigure A.5: GRASS can handle many different data types. Most types of raster can be imported using r.in.gdal, but you’ll find more options in the Raster import menu. Similarly, you’ll find more options for the import of vector, 3D and tabular data under the File menu.\n\n\n\n\n\n\nGRASS can handle many different data types, and offers various tools to import e.g., temporal data sets and satellite images as well. This page provides a list of core import modules. And for more specialized data import tools, check out the list of addons (Section A.6).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Working in GRASS GIS</span>"
    ]
  },
  {
    "objectID": "A1_grassbasics.html#data-management",
    "href": "A1_grassbasics.html#data-management",
    "title": "Appendix A. Working in GRASS GIS",
    "section": "A.4 Data management",
    "text": "A.4 Data management\nThe GUI interface allows you to find, explore, manage and display raster and vector data. More advanced exploration and visualization is also possible using, e.g., queries and adding legend. The screenshots below depicts some of the ways to display and explore data. You are encouraged to familiarize yourself with the different ways to interact with the data using the menu, context menu6 and command line. For a more detailed overview of the GRASS GUI, see here.\n\n Data catalog Adding layers Layer properties Attribute data Metadata\n\n\n\n\n\n\n\n\nFigure A.6: To have a better overview of our raster and vector data, we can use the Data catalog. You can search for data by name and when you right click at the item. In the context menu, you’ll find options to e.g., easily copy or remove data, add them to display, or switch between mapsets. Note that by for safety reasons you can modify data only in current location and mapset. However, you can unlock the other mapsets for editing by clicking on the pencil.\n\n\n\n\n\n\n\n\n\n\n\nFigure A.7: Different ways to add raster data to the map display. The same goes for vector and 3D data.\n\n\n\n\n\n\n\n\n\nFigure A.8: Add overlay elements to the map and zoom, pan and query maps.\n\n\n\n\n\n\n\n\n\n\n\nFigure A.9: To change layer properties, right click on the layer name in the Layers panel. This will give you a context menu with various options, like the ‘Set color interactively’ option.\n\n\n\n\n\n\n\n\n\n\n\nFigure A.10: View the attribute table of a vector layer and highlight selected records on the map.\n\n\n\n \n\n\n\n\n\n\n\n\nFigure A.11: Get the metadata using the context menu by right clicking on the layer in the Layer manager or Data catalog. Alternatively, use the r.info or v.info to get the metadata of raster and vector layer respectively.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Working in GRASS GIS</span>"
    ]
  },
  {
    "objectID": "A1_grassbasics.html#sec-runningfunctions",
    "href": "A1_grassbasics.html#sec-runningfunctions",
    "title": "Appendix A. Working in GRASS GIS",
    "section": "A.5 GRASS modules",
    "text": "A.5 GRASS modules\nGRASS GIS functionality is organized into modules, which are standalone programs with defined interface. They can be executed either through a graphical user interface (GUI) , command line interface (CLI)  or using Python . The GUI offers a user-friendly approach to executing modules where the user can navigate to data layers that they would like to analyze and modify processing options with simple check boxes.\nThe CLI allows users to execute a module using command prompts specific to that module. This is handy when you are running similar analyses with minor modification or are familiar with the module commands for quick efficient processing. And you can easily combined both; this manual page describes in more detail how GUI and command line interface represent the same tool.\nFrom the three options, Python arguably offers the most flexibility and power, as one can easily combine GRASS-specific functions and generic Python functions. In fact, many of the GRASS modules are written in Python. See Section A.9 for a short introduction of using Python in GRASS.\n\n\n\n\n\n\n\n\n\n\nFigure A.12: If you already know the name of the module, you can just use it in the command line or terminal. The GUI offers a Console tab with command line specifically build for running GRASS modules. If you type module name there, you will get suggestions for automatic completion of the name.\n\n\n\n\n\n\n\n\n\n\n\nFigure A.13: The python console offers a convenient interface to quickly run Python commands. The python editor allows you to write and run Python scripts. See also Section A.9.\n\n\n\n\n\n\n\n\n\n\n\nFigure A.14: Find functions through the menu or use the search box of the toolbox. Note that that after installation, addons are available in the toolbox under ‘addons’.\n\n\n\nTheir graphical user interface (GUI) is a dialog with several tabs which organize module parameters into groups. They all follow the same structure. The figure below shows the GUI dialog for the r.neighbors module. Each parameter can have different type of input fields, for example text entry or drop-down list. Flags are represented as checkboxes. The parameter (or flag) name is visible on the right side of each input field so that it is simple to understand how the module dialog relates to the command representation which is used in the manuals and tutorials. The commands can be used to call the module in the command line, Shell scripts or, with a slight modification, in a Python script. For a more detailed description, see [here]{https://grass.osgeo.org/grass84/manuals/wxGUI.modules.html}\n\n\n\n\n\n\nFigure A.15: The GUI dialog of the module r.neighbors.\n\n\n\n\n\n\nGRASS contains over 500 programs and tools to import, manipulate, analyse and visualize data. To find the modules or function you need, it is good to know that all raster functions start with r.* Likewise, vector functions start with a v.*, temporal functions with a t.*, 3D raster functions with r3.*, image functions with a i.*, d.* and database functions with a db.*. For an overview of all core modules in GRASS, see this page.\nTo run a function from the command line or console, simply type in the name of the function, followed by the required arguments. For a quick overview of the function’s argument, type the function’s name followed by --help. Alteratively, you can type in the name of the function in the Console tab or in the terminal and hit enter (Figure A.16).\n\n\n\n\n\n\nFigure A.16: Type in the name of the function in the console (or terminal) and hit Enter. This will open the function’s window. Now you can explore and set the different parameters and flags. The corresponding code is shown at the bottom of the window and can be copied. Or you can check out the Manual page for more information about the function and its parameters. Note that the same help pages are available online as well, just search for “GRASS + function name”.\n\n\n\nKeeping record of the functions you use in your analysis is good practice. GRASS offers some convenient ways to track and repeat commands you have executed before. In the terminal, you can use the up or down arrow to go through the commands you have run on the command line during the session. For commands carried out using the menu or console, you can check the history browser pane. The history is stored in your database, so they are available across sessions, and can be easily shared along with your data.\n\n\n\n\n\n\nFigure A.17: In the history browser panel, you can track the history of executed commands, and execute them again.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Working in GRASS GIS</span>"
    ]
  },
  {
    "objectID": "A1_grassbasics.html#sec-usingaddons",
    "href": "A1_grassbasics.html#sec-usingaddons",
    "title": "Appendix A. Working in GRASS GIS",
    "section": "A.6 GRASS addons",
    "text": "A.6 GRASS addons\nThere is a large list of GRASS addons available. These offer functions that are not (yet) part of the core software package. They can be easily installed using the wxGUI Extension Manager to install Addons or using the command line.\n\n\n\n\n\n\n\n\n\n\nFigure A.18: Install and manage addons\n\n\n\n\n\nYou can also install addons from the command line using the g.extension function. For example, the code below installs the r.maxent.train addon.\ng.extension extension=r.maxent.train\n\n\n\nInstalled addons can be found in the Tools panel under Addons (see Figure A.14) and can be used in the same way as core functions.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Working in GRASS GIS</span>"
    ]
  },
  {
    "objectID": "A1_grassbasics.html#sec-computationalregion",
    "href": "A1_grassbasics.html#sec-computationalregion",
    "title": "Appendix A. Working in GRASS GIS",
    "section": "A.7 Computational region",
    "text": "A.7 Computational region\nThe region is one of the fundamental concepts of GRASS. Any computation and raster analysis adhere to the so-called computational region7. This is the currently active bounding box and a defined spatial resolution. This means that if a raster map has an original resolution that is different from the resolution of the current computational region, or when the current computational region’s cells are shifted in space compared to the original raster map, the values are resampled on-the-fly by GRASS GIS, using the nearest neighbor method8, to meet the cell resolution and extent of the current computational region setting.\nImportantly, the geographic region is defined per mapset. It is therefore the current region in the mapset you are working in that defines the geographic area and resolution in which raster analyses will be done. See this GRASS wiki page for a more detailed explanation of this concept.\n\n Set the region Using the region\n\n\n\n\n\n\n\n\nFigure A.19: Different ways to set the computational region, and to view the current settings. The red outline in the Map display shows the extent of the current computation region.\n\n\n\n\n\nBy changing the computational region, the user can run raster analyses on a subset of a larger extent data for quicker testing or analysis of specific regions, without having to create a new dataset first (avoid clipping). The video below demonstrates this by repeating the same raster calculation using two different region settings.\n\n\n\n\nNote that by default, the region settings do not affect how raster layers are imported. GRASS GIS import tools by default always import the entire map data, maintaining the map’s original resolution and alignment.\nThe region’s settings are important if the r.in.gdal module is used with the -r flag. This flag tells GRASS to limit the import of the raster data to the area defined by the computational region. If the region and the raster layer are not perfectly aligned, the area that is imported will be slightly larger than the computational region. How much larger depends on the resolution of the computational region, as illustrated in Figure A.20.\n\n\n\n\n\n\nFigure A.20: The green grid represents the import raster. The red solid line shows the bounds and resolution of the computational region of the current mapset. The red dashed outline is a buffer around the region of 1 x the region’s resolution (so imagine the region is extended to all sides with one cell). When using r.in.gdal with the -r flag, all raster cells of the input raster that are within or touching the red dashed outline will be imported. These are the blue cells in the illustration.\n\n\n\nImportantly, the region’s settings only affect the precise area that will be imported when using the -r flag. The resolution and alignment of the imported raster layer will be the same as that of the original input raster file.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Working in GRASS GIS</span>"
    ]
  },
  {
    "objectID": "A1_grassbasics.html#sec-mask",
    "href": "A1_grassbasics.html#sec-mask",
    "title": "Appendix A. Working in GRASS GIS",
    "section": "A.8 Using a MASK",
    "text": "A.8 Using a MASK\nIn GRASS GIS, a MASK is a raster map that defines areas of interest for raster operations by masking out unwanted areas. It allows users to focus their analysis on specific regions while ignoring others. If a raster map named “MASK” exists, most GRASS raster modules will operate only on data falling inside the masked area, and treat any data falling outside of the mask as if its value were NULL. This feature is especially useful when working with large datasets or when analyzing a particular region of interest.\nA MASK is typically created from an existing raster map by setting certain cell values as null (masked) and others as valid (unmasked). Commands such as r.mask or r.mapcalc can be used to create a MASK based on specific criteria or thresholds. Read the manual page for more details concerning creating MASK layers using different methods.\nOnce a MASK is set, it is automatically applied to all raster operations, so only the unmasked (valid) areas are considered. I.e., operations like raster algebra, statistics, or interpolation only occur where the MASK allows it. MASKs are not permanent. Once a MASK is created, it remains active until explicitly removed using the r.mask -r command. This temporary behavior allows users to switch between different MASKs for various operations without permanently altering their data.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Working in GRASS GIS</span>"
    ]
  },
  {
    "objectID": "A1_grassbasics.html#sec-workinginpython",
    "href": "A1_grassbasics.html#sec-workinginpython",
    "title": "Appendix A. Working in GRASS GIS",
    "section": "A.9 Working in Python",
    "text": "A.9 Working in Python\nAll GRASS modules can be used in Python scripts, using the GRASS Python scripting library or the Python API, as described here. Examples in this tutorial will use the Python interface provided by the grass.script package. To use the this package, you first need to import the grass.script library. In the code below, it is imported as gs (this only needs to be done once at the beginning of a session or script). Next, you can run GRASS functions, using for example the run_command, read_command, write_command or parse_command. Read more about when to use what functions in the manual.\nFor simply executing a function, you use run_command. The first parameter for functions from this group is the name of the GRASS module as string. Other parameters are options of the module. Flags can be passed in a parameter flags where value of the parameter is a string containing all the flags we want to set. The general syntax is the following:\n\n\n\n\nimport grass.script as gs\ngs.run_command(\n    \"module.name\",\n    option1=\"...\",\n    option2=\"...\",\n    flags=\"flagletters\",\n)\n\n\n\nThe function parameters are the same as module options, so you can just use the manual page of the module to learn about the interface. Compare, for example, the command line code () and Python code () to import the raster landuse.tif as landuse raster layer in GRASS using the r.in.gdal module. As you can see, the Python run_command function let’s you ‘wrap’ the command line code in a Python command.\n\n\n\n\nr.in.gdal input=landuse.tif output=landuse\n\n\nimport grass.script as gs\ngs.run_command('r.in.gdal', input=\"landuse.tif\", output=\"landuse\")\n\n\n\nOne of the advantages of using Python is that you can combine GRASS specific functions and generic Python code. The following example illustrates this. In Section A.2 you saw how to create a new database. The code below does the same. First, a new folder GRASSdb is created using regular Python code9. Next, we create a new project in that folder. This will automatically promote that folder to a GRASS database (After all, a GRASS database is nothing else than a folder with one or more GRASS projects). The last step is to change to that new project/mapset. This will ensure that all subsequent analysis are carried out in that new mapset.\n\n\n\n\n# Import the grass.script and os libraries\nimport os\nimport grass.script as gs\n\n# Create a new folder (this will be the location of the GRASS GIS database)\nos.mkdir(\"/media/paulo/HD2/\")\n\n# Create a new project\ngs.create_project(path=\"/media/paulo/HD2/GRASSdb\", name=\"SDM\", epsg=\"4326\")\n\n# Change to the newly created project / mapset\ngs.run_command(\"g.mapset\", mapset=\"PERMANENT\", project=\"SDM\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Working in GRASS GIS</span>"
    ]
  },
  {
    "objectID": "A1_grassbasics.html#footnotes",
    "href": "A1_grassbasics.html#footnotes",
    "title": "Appendix A. Working in GRASS GIS",
    "section": "",
    "text": "In GRASS versions prior to 8.4 these were called locations.↩︎\nFor quality reasons, GRASS GIS handles one CRS per location. Read more about the reasons in this Wiki page. To learn more about coordinate references systems, go here. For information about different ways to store and share CRS information, see here.↩︎\nThe attribute data (non-spatial data) refers to descriptive information associated with each vector feature. For example, if you have a vector layer representing lakes, the attribute data might include information like the lake’s name, area, and depth.↩︎\nSee this manual page for more information.↩︎\nRead this page about map reprojection in GRASS.↩︎\nA context menu (also know as a contextual menu, shortcut menu or pop-up menu) is the menu that appears when you right-click on an item in the menu. It offers a set of choices that are available for, or in context of, whatever it was you clicked.↩︎\nNote that this concept does not apply to vector maps. Vector maps are always processed entirely.↩︎\nGRASS will use the nearest neighbor method to resample the raster layer. This might not always be the best method. If not, you need to resample the data yourself, using the most suitable method. See menu: Raster → Develop raster map for options. See this wiki page for more insights.↩︎\nNote, on Windows, paths are written using backslashes (\\) as the separator between folder names. On Unix based operating system such as macOS, Linux, and BSDs, the forward slash (/) is used as the path separator.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Working in GRASS GIS</span>"
    ]
  },
  {
    "objectID": "A3_sampledata.html",
    "href": "A3_sampledata.html",
    "title": "Appendix B. Sample data",
    "section": "",
    "text": "This tutorial guides you through all the steps of species distribution modeling, starting from creating a GRASS GIS database, downloading and preparing the input data, building the models, and finally using these models to predict the potential distribution of the Almond-eyed Ringlet (Erebia albergana) butterfly.\nIf you are mainly interested in learning about the Maxent modules in GRASS GIS, you can skip Section 1.3 - 2.3 and jump directly to ?sec-creatinginputdata. Download the sample data set, which includes a project named SDM and several mapset with data layers. These are the same you would otherwise have created in aforementioned sections.\nThere are two versions of the sample data. The first version includes raster layers with a resolution of 2.5 arc minutes (approximately 4 km at the equator). The second version has a higher resolution of 30 arc seconds (approximately 900 meters at the equator). Choose the version you prefer, download it, and unzip it into an existing GRASS database or an empty directory. If you unzip it into an empty directory, that directory will automatically be converted into a GRASS database, which is simply a directory containing one or more projects.\nOther climate data:\n\nhttps://sites.ualberta.ca/~ahamann/data/climateeu.html\nhttps://gis.stackexchange.com/questions/82186/are-there-better-climate-data-than-worldclim-for-europe\nhttps://www.ccafs-climate.org/statistical_downscaling_climgen/\nhttps://rmets.onlinelibrary.wiley.com/doi/full/10.1002/gdj3.110\n\nModels:\n\nhttps://besjournals-onlinelibrary-wiley-com.has.idm.oclc.org/doi/full/10.1111/2041-210X.13360\nhttps://www-sciencedirect-com.has.idm.oclc.org/science/article/pii/S0048969717336458\nhttps://esd.copernicus.org/articles/14/457/2023/\nhttps://cds.climate.copernicus.eu/cdsapp#!/dataset/projections-cmip6?tab=overview\n\n\n\n\n\n1. van Swaay C, Wynhoff I, Verovnik R, et al (2010) Erebia albergana. The IUCN Red List of Threatened Species 2010. IUCN Red List of Threatened Species. https://doi.org/10.2305/IUCN.UK.2010-1.RLTS.T173278A6984115.en",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Sample data</span>"
    ]
  },
  {
    "objectID": "A4_scriptmap.html",
    "href": "A4_scriptmap.html",
    "title": "Appendix C. Scripting a map",
    "section": "",
    "text": "Code for Figure 2.5",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Scripting a map</span>"
    ]
  },
  {
    "objectID": "A4_scriptmap.html#sec-rangemapoccurrences",
    "href": "A4_scriptmap.html#sec-rangemapoccurrences",
    "title": "Appendix C. Scripting a map",
    "section": "",
    "text": "# Import libraries\nimport grass.script as gs\nimport os\n\n# Set region\ngs.run_command(\"g.region\", vector=\"aoi\", res=100)\n\n# Output settings\noutputfile = \"rangemap.png\"\nwidth_image = 3600\ntitle = \"Erebia albergana range map\"\nos.remove(outputfile)\n\n# Get width/height ratio of image\nregion_settings = gs.region()\nheight = float(region_settings[\"n\"]) - float(region_settings[\"s\"])\nwidth = float(region_settings[\"e\"]) - float(region_settings[\"w\"])\nheight_image = width_image / (width / height)\n\n# Set environmental variables\nos.environ[\"GRASS_RENDER_IMMEDIATE\"] = \"cairo\"\nos.environ[\"GRASS_RENDER_FILE\"] = outputfile\nos.environ[\"GRASS_RENDER_HEIGHT\"] = str(height_image)\nos.environ[\"GRASS_RENDER_WIDTH\"] = str(width_image)\nos.environ[\"GRASS_RENDER_FILE_READ\"] = \"TRUE\"\nos.environ[\"GRASS_FONT\"] = \"DejaVuSansCondensed\"\n\n# Render image\ngs.run_command(\n    \"d.vect\",\n    map=\"aoi\",\n    type=\"area\",\n    color=\"199:226:234\",\n    fill_color=\"199:226:234\",\n    width=1,\n)\ngs.run_command(\n    \"d.vect\",\n    map=\"countries\",\n    type=\"area\",\n    color=\"195:195:195\",\n    fill_color=\"236:236:236\",\n    width=1,\n)\ngs.run_command(\n    \"d.vect\",\n    map=\"occurrences\",\n    type=\"point\",\n    color=\"211:24:24:255\",\n    fill_color=\"211:24:24:100\",\n    width=10,\n)\ngs.run_command(\n    \"d.vect\",\n    map=\"rangemap\",\n    type=\"area\",\n    color=\"255:255:255\",\n    fill_color=\"none\",\n    width=6,\n)\ngs.run_command(\n    \"d.vect\",\n    map=\"rangemap\",\n    type=\"area\",\n    color=\"43:151:37\",\n    fill_color=\"none\",\n    width=3,\n)\ngs.run_command(\n    \"d.grid\",\n    size=5,\n    color=\"190:190:190\",\n    text_color=\"100:100:100\",\n    border_color=\"160:160:160\",\n    fontsize=24,\n    width=0.5,\n)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Scripting a map</span>"
    ]
  },
  {
    "objectID": "A4_scriptmap.html#sec-bioclimvars",
    "href": "A4_scriptmap.html#sec-bioclimvars",
    "title": "Appendix C. Scripting a map",
    "section": "Code for Figure 2.7",
    "text": "Code for Figure 2.7\n# Import libraries\nimport grass.script as gs\nimport os\n\n# Set region\ngs.run_command(\"g.region\", region=\"aoi\", res=0.00833)\n\n# Variable\nvariables = [f\"bio_{num}\" for num in list(range(1, 20))]\nnames = [\n    \"BIO1 - Annual Mean Temperature\",\n    \"BIO2 - Mean Diurnal Range\",\n    \"BIO3 - Isothermality\",\n    \"BIO4 - Temperature Seasonality\",\n    \"BIO5 - Max Temperature of Warmest Month\",\n    \"BIO6 - Min Temperature of Coldest Month\",\n    \"BIO7 - Temperature Annual Range\",\n    \"BIO8 - Mean Temperature of Wettest Quarter\",\n    \"BIO9 - Mean Temperature of Driest Quarter\",\n    \"BIO10 - Mean Temperature of Warmest Quarter\",\n    \"BIO11 - Mean Temperature of Coldest Quarter\",\n    \"BIO12 - Annual Precipitation\",\n    \"BIO13 - Precipitation of Wettest Month\",\n    \"BIO14 - Precipitation of Driest Month\",\n    \"BIO15 - Precipitation Seasonality\",\n    \"BIO16 - Precipitation of Wettest Quarter\",\n    \"BIO17 - Precipitation of Driest Quarter\",\n    \"BIO18 - Precipitation of Warmest Quarter\",\n    \"BIO19 - Precipitation of Coldest Quarter\",\n]\n\n# Output settings\nwidth_image = 1200\ntitle = \"-\"\n\n# Get width/height ratio of image\nregion_settings = gs.region()\nheight = float(region_settings[\"n\"]) - float(region_settings[\"s\"])\nwidth = float(region_settings[\"e\"]) - float(region_settings[\"w\"])\nheight_image = width_image / (width / height)\n\n# Set environmental variables\nos.environ[\"GRASS_RENDER_IMMEDIATE\"] = \"cairo\"\nos.environ[\"GRASS_RENDER_HEIGHT\"] = str(height_image)\nos.environ[\"GRASS_RENDER_BACKGROUNDCOLOR\"] = \"#e7f7fe\"\nos.environ[\"GRASS_RENDER_FILE_READ\"] = \"TRUE\"\nos.environ[\"GRASS_FONT\"] = \"DejaVuSansCondensed\"\n\nfor i, variable in enumerate(variables):\n\n    outputfile = f\"{variable}.png\"\n    os.environ[\"GRASS_RENDER_FILE\"] = outputfile\n\n    # Render image\n    gs.run_command(\n        \"d.rast\",\n        map=variable,\n    )\n    gs.run_command(\n        \"d.vect\",\n        map=\"countries\",\n        type=\"area\",\n        color=\"white\",\n        fill_color=\"none\",\n        width=0.75,\n    )\n    gs.run_command(\n        \"d.legend\",\n        flags=\"bd\",\n        raster=variable,\n        font=\"Arial:Regular\",\n        fontsize=12,\n        at=[7, 9, 75, 97],\n        title=names[i],\n    )\n    gs.run_command(\n        \"d.vect\",\n        map=\"rangemap@Erebia_albergana\",\n        type=\"area\",\n        color=\"255:0:0\",\n        fill_color=\"none\",\n        width=2,\n    )",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Scripting a map</span>"
    ]
  },
  {
    "objectID": "A4_scriptmap.html#sec-futbioclimvars",
    "href": "A4_scriptmap.html#sec-futbioclimvars",
    "title": "Appendix C. Scripting a map",
    "section": "Code for Figure 2.9",
    "text": "Code for Figure 2.9\n# Import libraries\nimport grass.script as gs\nimport os\n\n# Set region\ngs.run_command(\"g.region\", region=\"aoi\", res=0.00833)\n\n# Variable\nvariables = [f\"bio.{num}\" for num in list(range(1, 20))]\n\nnames = [\n    \"BIO1 - Annual Mean Temperature\",\n    \"BIO2 - Mean Diurnal Range\",\n    \"BIO3 - Isothermality\",\n    \"BIO4 - Temperature Seasonality\",\n    \"BIO5 - Max Temperature of Warmest Month\",\n    \"BIO6 - Min Temperature of Coldest Month\",\n    \"BIO7 - Temperature Annual Range\",\n    \"BIO8 - Mean Temperature of Wettest Quarter\",\n    \"BIO9 - Mean Temperature of Driest Quarter\",\n    \"BIO10 - Mean Temperature of Warmest Quarter\",\n    \"BIO11 - Mean Temperature of Coldest Quarter\",\n    \"BIO12 - Annual Precipitation\",\n    \"BIO13 - Precipitation of Wettest Month\",\n    \"BIO14 - Precipitation of Driest Month\",\n    \"BIO15 - Precipitation Seasonality\",\n    \"BIO16 - Precipitation of Wettest Quarter\",\n    \"BIO17 - Precipitation of Driest Quarter\",\n    \"BIO18 - Precipitation of Warmest Quarter\",\n    \"BIO19 - Precipitation of Coldest Quarter\",\n]\n\n# Output settings\n\nwidth_image = 1200\ntitle = \"-\"\n# os.remove(outputfile)\n\n# Get width/height ratio of image\nregion_settings = gs.region()\nheight = float(region_settings[\"n\"]) - float(region_settings[\"s\"])\nwidth = float(region_settings[\"e\"]) - float(region_settings[\"w\"])\nheight_image = width_image / (width / height)\n\n# Set environmental variables\nos.environ[\"GRASS_RENDER_IMMEDIATE\"] = \"cairo\"\nos.environ[\"GRASS_RENDER_HEIGHT\"] = str(height_image)\nos.environ[\"GRASS_RENDER_BACKGROUNDCOLOR\"] = \"#e7f7fe\"\nos.environ[\"GRASS_RENDER_FILE_READ\"] = \"TRUE\"\nos.environ[\"GRASS_FONT\"] = \"DejaVuSansCondensed\"\n\nfor i, variable in enumerate(variables):\n\n    j = i + 1\n\n    outputfile = f\"futbio_{j}.png\"\n    os.environ[\"GRASS_RENDER_FILE\"] = outputfile\n\n    # Render image\n    gs.run_command(\n        \"d.rast\",\n        map=f\"{variable}@climate_EC_Earth3_Veg\",\n    )\n    gs.run_command(\n        \"d.vect\",\n        map=\"countries\",\n        type=\"area\",\n        color=\"white\",\n        fill_color=\"none\",\n        width=0.75,\n    )\n    gs.run_command(\n        \"d.vect\",\n        map=\"rangemap@Erebia_albergana\",\n        type=\"area\",\n        color=\"255:0:0\",\n        fill_color=\"none\",\n        width=2,\n    )\n    gs.run_command(\n        \"d.legend\",\n        flags=\"bd\",\n        raster=variable,\n        font=\"Arial:Regular\",\n        fontsize=12,\n        at=[7, 9, 75, 97],\n        title=names[i],\n    )",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Scripting a map</span>"
    ]
  },
  {
    "objectID": "A4_scriptmap.html#sec-probmapmodel1",
    "href": "A4_scriptmap.html#sec-probmapmodel1",
    "title": "Appendix C. Scripting a map",
    "section": "Code for Figure 4.3",
    "text": "Code for Figure 4.3\n# Import libraries\nimport grass.script as gs\nimport os\n\nos.chdir(\"/home/paulo/Desktop\")\n\n# Variable\nvariable = \"Erebia_albergana_obs_envdat@model_01\"\nname = \"Erebia_albergana_probability_distribution_model_01\"\n\n# Set region\ngs.run_command(\"g.region\", raster=variable)\n\n# Output settings\nwidth_image = 1200\ntitle = \"-\"\n\n# Get width/height ratio of image\nregion_settings = gs.region()\nheight = float(region_settings[\"n\"]) - float(region_settings[\"s\"])\nwidth = float(region_settings[\"e\"]) - float(region_settings[\"w\"])\nheight_image = width_image / (width / height)\n\n# Set environmental variables\nos.environ[\"GRASS_RENDER_IMMEDIATE\"] = \"cairo\"\nos.environ[\"GRASS_RENDER_HEIGHT\"] = str(height_image)\nos.environ[\"GRASS_RENDER_WIDTH\"] = str(width_image)\nos.environ[\"GRASS_RENDER_BACKGROUNDCOLOR\"] = \"#e7f7fe\"\nos.environ[\"GRASS_RENDER_FILE_READ\"] = \"TRUE\"\nos.environ[\"GRASS_FONT\"] = \"DejaVuSansCondensed\"\n\noutputfile = f\"{name}.png\"\nos.remove(outputfile)\nos.environ[\"GRASS_RENDER_FILE\"] = outputfile\n\n# Render image\ngs.run_command(\n    \"d.rast\",\n    map=variable,\n)\ngs.run_command(\n    \"d.vect\",\n    map=\"countries\",\n    type=\"area\",\n    color=\"white\",\n    fill_color=\"none\",\n    width=0.5,\n)\ngs.run_command(\n    \"d.legend\",\n    flags=\"bt\",\n    raster=variable,\n    font=\"Arial:Regular\",\n    fontsize=16,\n    at=[6,94,92,94],\n    digits=1,\n    label_step=0.1\n)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Scripting a map</span>"
    ]
  },
  {
    "objectID": "A4_scriptmap.html#sec-samlepred_model_01",
    "href": "A4_scriptmap.html#sec-samlepred_model_01",
    "title": "Appendix C. Scripting a map",
    "section": "Code for Figure 4.4",
    "text": "Code for Figure 4.4\n# Import libraries\nimport grass.script as gs\nimport os\n\nos.chdir(\"/home/paulo/Desktop\")\n\n# Variable\nvariable = \"E_albergana_samplepred\"\nnames = \"Erebia albergana probability distribution\"\nlegend = \"E_albergana_probability\"\n\n# Set region\ngs.run_command(\"g.region\", region=\"aoi\")\n\n# Output settings\nwidth_image = 1200\ntitle = \"-\"\n\n# Get width/height ratio of image\nregion_settings = gs.region()\nheight = float(region_settings[\"n\"]) - float(region_settings[\"s\"])\nwidth = float(region_settings[\"e\"]) - float(region_settings[\"w\"])\nheight_image = width_image / (width / height)\n\n# Set environmental variables\nos.environ[\"GRASS_RENDER_IMMEDIATE\"] = \"cairo\"\nos.environ[\"GRASS_RENDER_HEIGHT\"] = str(height_image)\nos.environ[\"GRASS_RENDER_WIDTH\"] = str(width_image)\nos.environ[\"GRASS_RENDER_BACKGROUNDCOLOR\"] = \"#e7f7fe\"\nos.environ[\"GRASS_RENDER_FILE_READ\"] = \"TRUE\"\nos.environ[\"GRASS_FONT\"] = \"DejaVuSansCondensed\"\n\noutputfile = f\"{variable}.png\"\nos.environ[\"GRASS_RENDER_FILE\"] = outputfile\n\n# Render image\ngs.run_command(\n    \"d.rast\",\n    map=legend,\n)\n\ngs.run_command(\n    \"d.vect\",\n    map=\"countries\",\n    type=\"area\",\n    color=\"white\",\n    fill_color=\"206:206:206\",\n    width=0.5,\n)\n\n# Render image\ngs.run_command(\n    \"d.vect\",\n    map=variable,\n    icon=\"basic/point\",\n    color=\"none\",\n    width=0,\n    size=7,\n)\n\ngs.run_command(\n    \"d.legend\",\n    flags=\"bt\",\n    raster=legend,\n    font=\"Arial:Regular\",\n    fontsize=18,\n    at=[90, 94, 72, 96],\n    digits=1,\n    label_values=\"0.1,0.3,0.5,0.7,0.9\",\n)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Scripting a map</span>"
    ]
  },
  {
    "objectID": "A4_scriptmap.html#sec-bgrpred_model_011",
    "href": "A4_scriptmap.html#sec-bgrpred_model_011",
    "title": "Appendix C. Scripting a map",
    "section": "Code for Figure 4.5",
    "text": "Code for Figure 4.5\n# Import libraries\nimport grass.script as gs\nimport os\n\nos.chdir(\"/home/paulo/Desktop\")\n\n# Variable\nvariable = \"E_albergana_bgrdpred@model_01\"\nnames = \"Erebia albergana probability distribution\"\nlegend = \"E_albergana_probability\"\n\n# Set region\ngs.run_command(\"g.region\", region=\"aoi\")\n\n# Output settings\nwidth_image = 1200\ntitle = \"-\"\n\n# Get width/height ratio of image\nregion_settings = gs.region()\nheight = float(region_settings[\"n\"]) - float(region_settings[\"s\"])\nwidth = float(region_settings[\"e\"]) - float(region_settings[\"w\"])\nheight_image = width_image / (width / height)\n\n# Set environmental variables\nos.environ[\"GRASS_RENDER_IMMEDIATE\"] = \"cairo\"\nos.environ[\"GRASS_RENDER_HEIGHT\"] = str(height_image)\nos.environ[\"GRASS_RENDER_WIDTH\"] = str(width_image)\nos.environ[\"GRASS_RENDER_BACKGROUNDCOLOR\"] = \"#e7f7fe\"\nos.environ[\"GRASS_RENDER_FILE_READ\"] = \"TRUE\"\nos.environ[\"GRASS_FONT\"] = \"DejaVuSansCondensed\"\n\noutputfile = f\"{variable}.png\"\nos.environ[\"GRASS_RENDER_FILE\"] = outputfile\n\n# Render image\ngs.run_command(\n    \"d.rast\",\n    map=legend,\n)\n\ngs.run_command(\n    \"d.vect\",\n    map=\"countries\",\n    type=\"area\",\n    color=\"white\",\n    fill_color=\"206:206:206\",\n    width=0.5,\n)\n\n# Render image\ngs.run_command(\n    \"d.vect\",\n    map=variable,\n    icon=\"basic/point\",\n    color=\"none\",\n    width=0,\n    size=7,\n)\n\ngs.run_command(\n    \"d.legend\",\n    flags=\"bt\",\n    raster=legend,\n    font=\"Arial:Regular\",\n    fontsize=18,\n    at=[90, 94, 72, 96],\n    digits=1,\n    label_values=\"0.1,0.3,0.5,0.7,0.9\",\n)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Scripting a map</span>"
    ]
  },
  {
    "objectID": "A4_scriptmap.html#sec-futdistr_01",
    "href": "A4_scriptmap.html#sec-futdistr_01",
    "title": "Appendix C. Scripting a map",
    "section": "Code for Figure 5.1",
    "text": "Code for Figure 5.1\n# Import libraries\nimport grass.script as gs\nimport os\n\nos.chdir(\"/home/paulo/Desktop\")\n\n# Variable\nvariable = \"E_albergana_futprob\"\nname = \"Erebia_albergana_probability_futdist_model_01\"\n\n# Set region\ngs.run_command(\"g.region\", raster=variable)\n\n# Output settings\nwidth_image = 1200\ntitle = \"-\"\n\n# Get width/height ratio of image\nregion_settings = gs.region()\nheight = float(region_settings[\"n\"]) - float(region_settings[\"s\"])\nwidth = float(region_settings[\"e\"]) - float(region_settings[\"w\"])\nheight_image = width_image / (width / height)\n\n# Set environmental variables\nos.environ[\"GRASS_RENDER_IMMEDIATE\"] = \"cairo\"\nos.environ[\"GRASS_RENDER_HEIGHT\"] = str(height_image)\nos.environ[\"GRASS_RENDER_WIDTH\"] = str(width_image)\nos.environ[\"GRASS_RENDER_BACKGROUNDCOLOR\"] = \"#e7f7fe\"\nos.environ[\"GRASS_RENDER_FILE_READ\"] = \"TRUE\"\nos.environ[\"GRASS_FONT\"] = \"DejaVuSansCondensed\"\n\noutputfile = f\"{name}.png\"\nos.remove(outputfile)\nos.environ[\"GRASS_RENDER_FILE\"] = outputfile\n\n# Render image\ngs.run_command(\n    \"d.rast\",\n    map=variable,\n)\ngs.run_command(\n    \"d.vect\",\n    map=\"countries\",\n    type=\"area\",\n    color=\"white\",\n    fill_color=\"none\",\n    width=0.5,\n)\ngs.run_command(\n    \"d.legend\",\n    flags=\"bt\",\n    raster=variable,\n    font=\"Arial:Regular\",\n    fontsize=16,\n    at=[6,94,92,94],\n    digits=1,\n    label_step=0.1\n)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Scripting a map</span>"
    ]
  },
  {
    "objectID": "A4_scriptmap.html#sec-changemap01",
    "href": "A4_scriptmap.html#sec-changemap01",
    "title": "Appendix C. Scripting a map",
    "section": "Code for Figure 5.4",
    "text": "Code for Figure 5.4\n# Import libraries\nimport grass.script as gs\nimport os\n\nos.chdir(\"/home/paulo/Desktop\")\n\n# Variable\nvariable = \"E_alb_diff\"\nname = \"E_alb_diff\"\n\n# Set region\ngs.run_command(\"g.region\", raster=variable)\n\n# Output settings\nwidth_image = 1200\ntitle = \"-\"\n\n# Get width/height ratio of image\nregion_settings = gs.region()\nheight = float(region_settings[\"n\"]) - float(region_settings[\"s\"])\nwidth = float(region_settings[\"e\"]) - float(region_settings[\"w\"])\nheight_image = width_image / (width / height)\n\n# Set environmental variables\nos.environ[\"GRASS_RENDER_IMMEDIATE\"] = \"cairo\"\nos.environ[\"GRASS_RENDER_HEIGHT\"] = str(height_image)\nos.environ[\"GRASS_RENDER_WIDTH\"] = str(width_image)\nos.environ[\"GRASS_RENDER_BACKGROUNDCOLOR\"] = \"#e7f7fe\"\nos.environ[\"GRASS_RENDER_FILE_READ\"] = \"TRUE\"\nos.environ[\"GRASS_FONT\"] = \"DejaVuSansCondensed\"\n\noutputfile = f\"{name}.png\"\nos.remove(outputfile)\nos.environ[\"GRASS_RENDER_FILE\"] = outputfile\n\n# Render image\ngs.run_command(\n    \"d.rast\",\n    map=variable,\n)\ngs.run_command(\n    \"d.vect\",\n    map=\"countries\",\n    type=\"area\",\n    color=\"white\",\n    fill_color=\"none\",\n    width=0.5,\n)\ngs.run_command(\n    \"d.vect\",\n    map=\"insetmap\",\n    type=\"area\",\n    color=\"red\",\n    fill_color=\"none\",\n    width=0.5,\n)\ngs.run_command(\n    \"d.legend\",\n    flags=\"bt\",\n    raster=variable,\n    font=\"Arial:Regular\",\n    fontsize=16,\n    at=[6, 94, 92, 94],\n    digits=1,\n    label_step=0.1,\n)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Scripting a map</span>"
    ]
  },
  {
    "objectID": "A4_scriptmap.html#sec-r_cross_report_01",
    "href": "A4_scriptmap.html#sec-r_cross_report_01",
    "title": "Appendix C. Scripting a map",
    "section": "Code for Figure 5.9",
    "text": "Code for Figure 5.9\n# Import libraries\nimport grass.script as gs\nimport os\n\nos.chdir(\"/home/paulo/Desktop\")\n\n# Variable\nvariable = \"E_alberganus_presabschange\"\nname = \"E_alberganus_presabschange\"\n\n# Set region\ngs.run_command(\"g.region\", raster=variable)\n\n# Output settings\nwidth_image = 1200\ntitle = \"-\"\n\n# Get width/height ratio of image\nregion_settings = gs.region()\nheight = float(region_settings[\"n\"]) - float(region_settings[\"s\"])\nwidth = float(region_settings[\"e\"]) - float(region_settings[\"w\"])\nheight_image = width_image / (width / height)\n\n# Set environmental variables\nos.environ[\"GRASS_RENDER_IMMEDIATE\"] = \"cairo\"\nos.environ[\"GRASS_RENDER_HEIGHT\"] = str(height_image)\nos.environ[\"GRASS_RENDER_WIDTH\"] = str(width_image)\nos.environ[\"GRASS_RENDER_BACKGROUNDCOLOR\"] = \"#e7f7fe\"\nos.environ[\"GRASS_RENDER_FILE_READ\"] = \"TRUE\"\nos.environ[\"GRASS_FONT\"] = \"DejaVuSansCondensed\"\n\noutputfile = f\"{name}.png\"\nos.remove(outputfile)\nos.environ[\"GRASS_RENDER_FILE\"] = outputfile\n\n# Render image\ngs.run_command(\n    \"d.rast\",\n    map=variable,\n)\n\ngs.run_command(\n    \"d.legend\",\n    flags=\"bt\",\n    raster=variable,\n    font=\"Arial:Regular\",\n    fontsize=16,\n    at=[75, 94, 80, 84],\n)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Scripting a map</span>"
    ]
  },
  {
    "objectID": "A4_scriptmap.html#sec-r_cross_report_02",
    "href": "A4_scriptmap.html#sec-r_cross_report_02",
    "title": "Appendix C. Scripting a map",
    "section": "Code for ?fig-r_cross_report_02",
    "text": "Code for ?fig-r_cross_report_02\n# Import libraries\nimport grass.script as gs\nimport os\n\nos.chdir(\"/home/paulo/Desktop\")\n\n# Variable\nvariable = \"E_alberganus_presabschange\"\nname = \"E_alberganus_presabschange\"\n\n# Set region\ngs.run_command(\"g.region\", raster=variable)\n\n# Output settings\nwidth_image = 1200\ntitle = \"-\"\n\n# Get width/height ratio of image\nregion_settings = gs.region()\nheight = float(region_settings[\"n\"]) - float(region_settings[\"s\"])\nwidth = float(region_settings[\"e\"]) - float(region_settings[\"w\"])\nheight_image = width_image / (width / height)\n\n# Set environmental variables\nos.environ[\"GRASS_RENDER_IMMEDIATE\"] = \"cairo\"\nos.environ[\"GRASS_RENDER_HEIGHT\"] = str(height_image)\nos.environ[\"GRASS_RENDER_WIDTH\"] = str(width_image)\nos.environ[\"GRASS_RENDER_BACKGROUNDCOLOR\"] = \"#e7f7fe\"\nos.environ[\"GRASS_RENDER_FILE_READ\"] = \"TRUE\"\nos.environ[\"GRASS_FONT\"] = \"DejaVuSansCondensed\"\n\noutputfile = f\"{name}.png\"\nos.remove(outputfile)\nos.environ[\"GRASS_RENDER_FILE\"] = outputfile\n\n# Render image\ngs.run_command(\n    \"d.rast\",\n    map=variable,\n)\n\ngs.run_command(\n    \"d.legend\",\n    flags=\"bt\",\n    raster=variable,\n    font=\"Arial:Regular\",\n    fontsize=16,\n    at=[75, 94, 80, 84],\n)\n\n\n\n\n1. van Swaay C, Wynhoff I, Verovnik R, et al (2010) Erebia albergana. The IUCN Red List of Threatened Species 2010. IUCN Red List of Threatened Species. https://doi.org/10.2305/IUCN.UK.2010-1.RLTS.T173278A6984115.en",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Scripting a map</span>"
    ]
  }
]